
Основная суть работы нейронной сети заключается в её способности аппроксимировать (приближать) 
неизвестную сложную функцию, которая связывает входные данные с выходными. 
Цель этого процесса — дать сети возможность предсказывать соответствующие
выходные значения для новых, ранее не встречавшихся входных данных. Предсказание это полезный
 результат распространения значении по нейронной сети, при котором данные трансформируются. В
нейронной сети распространение происходит по слоям, при этом для каждого из нейронов следующего
 слоя считается сумма значении с нейронов прошлого слоя что связанны с текущим и эти значения перед
суммой умножаются на веса связей и после к сумме в нейроне котором произошла сумма добавляется 
смещение и после для каждого нейрона с новой суммой применяется функция активации. Математически 
это очень удобно описать произведением вектора значении из прошлого слоя на матрицу весов а после 
прибавляем вектор смещении и функцию активации.

Функция активации f - функция в нейроне, 
которая часто нелинейно преобразует 
сумму (часто используют σ, ReLu, tanh) 

В общем случае предсказание можно описать так (y' - результат NN):
y' = fₙ( ... f₃(f₂(f₁(XW₁ + B₁)W₂ + B₂)W₃ + B₃) ... )Wₙ + Bₙ

Обучение нейронной сети сводится к поиску оптимальных значений весов (weights) и смещений (biases), 
которые минимизируют функцию ошибки (loss function) на всех доступных обучающих примерах.
Функция ошибки (функция потерь ее так же могут помечать буквой L) измеряет расхождение между предсказанными значениями сети (y′) и реальными значениями (y). 
Типичное представление ошибки для одного примера: e=y′−y. 
Для обучения используется средняя функция ошибки по всему обучающему набору или его подмножеству (батчу):
E = (Σ(y' - y)²) / n    

Тогда нам нужно изменять веса таким образом чтобы эта функция была минимальна то есть найти все минимумы. 
Это можно сделать аналитически , приравняв все производные к нулю и решив системы уравнении, но в большинстве 
случаев это невозможно из-за огромного количества параметров и нам нужно алгоритмическое решение, тогда 
используется градиентный спуск - это численный, итеративный метод оптимизации. 
Он не ищет точное решение сразу, а постепенно, шаг за шагом, приближается к минимуму функции потерь.
(η — скорость обучения, Wk веса на k слое): 

Wₖ = Wₖ - η * (dE/dWₖ)  


Поиск минимума ошибки которая зависит от параметров - весов на рисунках
показано пространство весов (для двух параметров - веса и смещения) 
 еще красно-синим функция ошибки (белым ее вектор) . С увеличением продолжительности обучения нейронной сети, её аппроксимирующая функция демонстрирует повышенную конвергенцию к истинной базовой функции, скрытой в распределении обучающих данных. Данный процесс обусловлен последовательной адаптацией параметров предсказательной модели сети, направленной на достижение максимальной точности аппроксимации предоставленных входных данных.

Сеть учится сжимать данные в информативные признаки , кодировать закономерности игнорируя шум. Это позволяет ей работать с новыми данными, а не просто запоминать примеры. Иначе был бы overfitting.


Каждый слой нейронной сети преобразует 
пространство признаков предыдущего слоя в 
новое пространство.
- Матрица весов слоя размером n×m трансформирует
 n-мерное пространство в m-мерное.
- Нелинейная функция активации
 (например, sigmoid или ReLU) преобразует линейное 
разделение, создаваемое гиперплоскостью, в нелинейное.
Эта сложная гиперповерхность способна аппроксимировать 
практически 
любую непрерывную функцию 
(Теорема универсальной аппроксимации).

Неопределённость — это ситуация, когда вероятности разных событий близки друг к другу.
Наблюдение приносит информацию уменьшая неопределенность. Информация — мера уменьшения
неопределенности. 
Энтропия - мера неопределенности. Энтропия максимальна при равных вероятностях.
Условная энтропия - неопределённость Y при известном X те энтропия которая характеризует степень неопределенности значений 
случайной величины Y, остающуюся после того, как стало известно, что случайная величина X приняла значение.
Взаимная информация - информация, которую Y даёт о X количество информации одной случайной величины, содержащейся в другой.
Больше информации получает наблюдатель если событие неожиданно, мера того сколько информации дает наблюдение события , собственная информация I(x) которая получена 
при наблюдении.

Представьте, что у вас есть система (или вы сами как субъект), которая пытается определить, на каком из четырех языков написан короткий фрагмент текста. У системы есть некоторая начальная (априорная) вероятностная модель, основанная на предыдущем опыте или общих знаниях о распространенности языков в данном контексте.
Если цель найти один объект то стратегия та же — максимизировать информационный выигрыш. Вопрос должен делить оставшееся пространство поиска (с учётом вероятностей) на две максимально равновероятные части. Это быстро сужает круг поиска (бинарный поиск) и при этом в случае неудачного угадывания вы пойдете в другую половину а не большую часть.

Оптимальная стратегия задавания вопросов для быстрого нахождения информации о всех объектах
Минимизация неопределённости (энтропии) системы H(X) через последовательное получение информации. На каждом шаге выбирается вопрос, максимизирующий информационный выигрыш (IG).

Метод информационного бутылочного горлышка (IBM)
- Цель IBM - извлечь полезную информацию I(X;Y) и минимизировать нерелевантные данные в представлении X^ .
- Ожидаемое IB-искажение (DIB) - I(X;Y∣X) показывает, сколько информации упущено в представлении.
- Преимущество IBM - Анализирует внутренние процессы сети, а не только итоговые потери (например, MSE).

Архитектура глубокой нейронной сети прямого действия с m скрытыми слоями, 
X - входные данные, Y - выходные данные и Y^ - прогнозируемые выходные данные.
Скрытые слои образуют марковскую цепочку, I(Y;Y^)/I(X;Y)
 определяет количество релевантной информации, получаемой сетью. 

Компрессия данных вместо подсчёта параметров
- Обобщение зависит не от числа параметров, а от способности сети сжимать данные выделяя значимые паттерны.
- Квантование данных - сеть разбивает входные данные на группы (кластеры) где внутри группы данные однородны по метке
- Сложность сети - определяется не размером модели d, а числом таких групп (T_ε), что резко снижает оценку сложности.
 
Роль информации и энтропии
- Взаимная информация - обобщение зависит от I(T_ε; X) — сколько информации о данных сохраняет сеть после сжатия.
- Граница обобщения - ошибка ε² < [2^{I(T_ε; X)} + log(1/δ)] / m
    Чем сильнее сеть сжимает данные (↓I(T_ε; X)), тем лучше обобщение даже при d > m.

Почему это важно для нейросетей?
- Объяснение успеха DNN - их сила — в эффективном сжатии данных (например, через слои свёрток или внимания), а не в малом числе параметров.
- Качество данных важнее размера модели - Сети обобщают, если могут выделить информативные паттерны (шумовые данные не сожмутся).
- Регуляризация через сжатие - Методы вроде dropout или batchnorm косвенно помогают «квантовать» данные, улучшая обобщение.
- Интерпретация моделей - Анализ скрытых представлений (embeddings) показывает, как сеть группирует данные.

Кластер — это группа похожих данных (X), которые нейросеть объединяет по каким-то признакам. Например, в наборе картинок кошек и собак кластеры — это группы, где все картинки в одном кластере — кошки, а в другом — собаки. Прототипы категорий являются центрами кластеров во входном пространстве. Граничные условия категорий определяются весами и биасами (смещениями).
Энтропия — это так же мера "разнообразия" данных. Чем больше H(X), тем больше разных вариантов в данных. Примерно 2^H(X) — это число типичных комбинаций. Например, если H(X) = 10 бит, то данных много и они разные.
Условная энтропия — это разнообразие данных внутри кластеров (T_ε). Если сеть разделила картинки на кластеры "кошки" и "собаки", и внутри каждого кластера все похоже, то H(X|T_ε) маленькая.
Взаимная информация показывает, насколько кластеры помогают понять данные.  
Формула: I(T_ε; X) = H(X) - H(X|T_ε).
- Если кластеры идеальны (все кошки в одном, все собаки в другом), то H(X|T_ε) ≈ 0, и I(T_ε; X) ≈ H(X).
- Если кластеры случайные, то I(T_ε; X) ≈ 0.
Число кластеров примерно равно: |T_ε| ≈ 2^(I(T_ε; X)) = 2^(H(X) - H(X|T_ε)).  
Пример:
- H(X) = 10 бит, H(X|T_ε) = 5 бит.
- Тогда I(T_ε; X) = 10 - 5 = 5 бит, и |T_ε| ≈ 2^5 = 32 кластера.

Нейросети учатся делить данные на кластеры, где каждый кластер — это один класс (например, "кошки").
- Чем лучше сеть сжимает данные (меньше H(X|T_ε) и больше I(T_ε; X)), тем меньше кластеров нужно, и тем лучше она работает на новых данных.
- Пример: в сверточных сетях (CNN) первые слои находят простые признаки (края), а последние собирают их в классы (объекты).

Вся "долговременная память" сети — это матрицы весов и вектора bias.

Качественная информационная плоскость с теоретическими и практическими пределами IB. 
     Ось X (R): Сколько полезной информации сохранено.
    Ось Y (DIB​): Сколько бесполезной информации (шума) осталось.
    Черная линия: Идеальный теоретический предел сжатия.
Синие линии - это неоптимальные бифуркации IB, полученные путем 
увеличения мощности X или сохранения в том же представлении.
Красная линия соответствует верхней границе искажения IB вне выборки (взаимной информации о Y) при обучении на конечной выборке. Показывает, что при обучении на реальных данных (конечная выборка) слишком сильное сохранение информации может привести к переобучению (искажение на новых данных начинает расти).
Зеленые точки (слои DNN): Движение информации от входа (X) к выходу (Y), где сеть сжимает шум и движется к оптимальному компромиссу между сжатием и сохранением полезной информации.

- Входные данные X часто содержат много лишней информации, а релевантные признаки для Y сложно выделить. Большая часть их энтропии нерелевантна для выходной метки Y и высокоразмерна.
- Нейроны в DNN работают только с линейно разделимыми данными, требуя трансформаций на скрытых слоях.
- DNN трансформируют данные на скрытых слоях для разделимости, часто до линейного преобразования.
- Скрытые слои DNN сжимают входные данные о Y, улучшая обобщение.
- Если данные о Y теряются на одном слое, их нельзя восстановить позже. Количество релевантной информации, которую каждый скрытый слой hi​ содержит о Y, убывает по мере углубления в сеть.  Это выражается неравенством: I(X;Y)≥I(hj​;Y)≥I(hi​;Y)≥I(Y^;Y), где i≥j.
Информация о входных данных тоже уменьшается:  
  H(X) >= I(X;T1) >= I(X;T2)>=...>=I(X;Tm)>=I(X;Y^)        
  где H(X) — энтропия входа.  
  Сеть постепенно "забывает" детали X, фокусируясь на ключевой информации для предсказания.

Процесс обучения DNN с использованием стохастического градиентного спуска (SGD) проходит через две фазы:
Фаза дрейфа (drift):
На начальной стадии (случайная инициализация весов) представления слоев на информационной плоскости разбросаны хаотично.
Фаза диффузии (diffusion):
По мере обучения (например, после 400 эпох) точки начинают сходиться. К концу (например, 9000 эпох) они концентрируются, показывая сжатие информации.
Последний скрытый слой:
Обычно обозначен оранжевым и находится в левом нижнем углу плоскости (Низкие I(T:Y) I(X:T)). Это указывает на сильное сжатие данных и фокусировку только на самой важной информации для предсказания.


Двухфазная структура обучения Обучение глубоких сетей происходит в две фазы: фаза подгонки (fitting) и фаза сжатия/забывания (compression/forgetting).

Фаза подгонки В первой фазе сеть быстро минимизирует функцию потерь и увеличивает взаимную информацию как с входными данными, так и с метками. Эта фаза относительно короткая.
Первая фаза объясняется минимизацией потерь перекрестной энтропии, в ходе которой накапливает информацию из данных. Происходит быстрое запоминание паттернов, а также количество входных данных при
 сохранении неравномерности обработки данных (нижние уровни содержат больше информации)

Фаза сжатия (забывания) Вторая фаза начинается когда шум в градиенте начинает доминировать над средним значением градиента. В этой фазе сеть сжимает представления, отбрасывая нерелевантную информацию о входных данных. Эта фаза намного медленнее по сравнению с первой фазой, и слои теряют ненужную информацию во время этой фазы до тех пор, пока не произойдет конвергенция. Сеть отбрасывает нерелевантную информацию, сохраняя только значимые признаки для обобщения. Эта фаза критична для способности к генерализации. (Если остаются только важные закономерности, то увеличивается генерализация так как модель будет использовать закономерности а не множество каких то признаков которых так много просто создают соответствие между входом и выходом и модель получается запоминает соответствие а не закономерности)

Роль мини-батчей Флуктуации в мини-батчах критически важны для процесса сжатия. Размер мини-батча линейно связан с точкой начала компрессии. Даже при полном батче наблюдается некоторое сжатие.


Хотя вся подгонка данных (снижение ошибки) происходит в первой фазе (дрейфа), основной прирост информации о метке (т.е. улучшение способности к обобщению) происходит именно во второй фазе (диффузии/забывания).
    В этой фазе отбрасывается большая часть нерелевантной информации о входных паттернах, что приводит к общей компрессии представления. Серым начало диффузии.

Снимок слоев сети на информационном уровне во время оптимизации SGD 
(слева - начальные веса, посередине - после 400 эпох, справа - после 9000 эпох). 
Разные точки соответствуют 50 различным инициализациям веса.

Меньшее количество обучающих примеров приводит к сходимости, очень далекой от сходимости 
при большем количестве примеров в данных. Информация о метке уменьшается в случае меньшего
 количества примеров на этапе сжатия. В данном случае мы выполняем чрезмерное сжатие до 
такой степени, что представление становится слишком простым для получения информации о метке.
 С другой стороны, в случае с большим количеством образцов информация о метке явно
 увеличивается на этапе сжатия. 

Средняя траектория движения скрытых слоев в процессе оптимизации на информационном плане.
На рисунке показана траектория, по которой движутся скрытые слои (различные случайные 
инициализации усредняются для получения траектории каждого скрытого слоя). Траектория от 
A до C соответствует данным, а от C до E является обобщением. Обратите внимание, что все слои 
достигают точки перегиба С более или менее одновременно.

Шум как двигатель обучения: Флуктуации в мини-батчах SGD не побочный эффект, а необходимый механизм. Шум позволяет сети "исследовать" пространство признаков и отбрасывать нерелевантную информацию через случайное блуждание в нерелевантных измерениях.
Информационное бутылочное горлышко: Сеть автоматически находит минимальное представление данных, сохраняющее максимум информации о целевых метках

Фундаментальная проблема: 
    Каждый бит компрессии в фазе диффузии требует экспоненциального времени. Время релаксации слоя Δt_m ∼ e^(ΔS_m), где ΔS_m - компрессия слоя.
Решение через множественные слои:
Слои выполняют параллельные процессы Марковской диффузии
Каждый слой сжимает информацию только с точки, где остановился предыдущий
Слои "подталкивают" друг друга, ускоряя процесс забывания

Математическое ускорение:
    Без параллелизации: общее время = exp(∑ΔS_m)
С множественными слоями: общее время = ∑exp(ΔS_m)
Поскольку exp(∑ΔS_m) ≫ ∑exp(ΔS_m), это дает экспоненциальное сокращение времени обучения
Причины эффективности
    Структурное разделение: По мере углубления в сеть данные становятся линейно разделимыми - сложные нелинейные границы преобразуются в простые линейные.
    Ковариационная матрица шума: Шум мал в направлениях релевантных признаков, но велик в нерелевантных направлениях, что обеспечивает направленное забывание.
    Взаимопомощь представлений: Слои помогают друг другу в компрессии, создавая каскадный эффект очистки от нерелевантной информации.
Фундаментальный парадокс
    Хотя точность достигается в фазе подгонки, способность к обобщению формируется в фазе забывания. Основной прирост информации о метках происходит именно тогда, когда сеть "забывает" детали входных данных.

Нейронные сети автоматически выделяют иерархию признаков:  
  - Первые слои — простые паттерны (края, текстуры в изображениях).  
  - Глубокие слои — сложные и абстрактные паттерны (морды животных, объекты). 
 Нейроны не всегда соответствуют конкретным 
признакам (особенно в больших сетях). Чаще это распределённое представление.
В НН веса и смещения - это основной способ хранения информации. Веса,
 числовые значения связей между нейронами, определяют 
силу влияния и регулируют данные. 
- Cмещения(bias) — это константы, добавляемые к взвешенной сумме, позволяющие
 нейронам активироваться, даже если входных данных недостаточно.
- Вся "долговременная память" сети — это матрицы весов и вектора bias.  
- Функция активации и архитектура влияют на то, какие паттерны сеть может выучить.  
- "Знание" — это не просто веса, а структура их взаимодействия.
 Например, в Transformer'ах важны внимание (attention) и residual connections.  
- Знания сети хранятся в весах (сила связей) и биасах (смещения). 
- Отдельный нейрон редко означает отдельный признак. Чаще признаки
 кодируются ансамблями нейронов (распределённое представление).  
- Знания в ИНС распределены по всему набору весов и смещений, а не 
хранятся в одном месте, в виде нейронной подсети. Каждая сила соединения 
участвует в кодировании множества паттернов, делая память о любом 
элементе нелокальной. Это динамический процесс, где внутренняя модель данных
 постоянно уточняется. Такое распределенное кодирование обеспечивает 
устойчивость к отказам и способствует обобщению.
а "забывание" является переформированием этого ландшафта весов и смещений.
 "Воспоминание" происходит через активацию определенной конфигурации
 нейронов с соответствующими весами
Близкие объекты для NN имеют перекрывающиеся паттерны весов, 
обеспечивая генерализацию.

Скрытые представления способны захватывать семантические отношения между понятиями 

Рекуррентные нейронные сети (РНС) — это нейронные сети для обработки последовательных данных.
 Они анализируют данные шаг за шагом, сохраняя контекст через скрытое состояние (h_t).
- Скрытое состояние обновляется на каждом шаге по этому РНС "помнят" предыдущий контекст:  
        - h_t = f(W_{hh} · h_{t-1} + W_{xh} · x_t + b_h),  
          где:
          - x_t — текущий элемент,
          - h_{t-1} — память о прошлых шагах,
          - W_{hh}, W_{xh} — веса, b_h — смещение.
- Обучение РНС:
        - Метод: обратное распространение ошибки во времени (BPTT).  
        - Этапы:  
          - Разворачивание сети.  
          - Вычисление градиентов.  
          - Обновление весов.  
        - Проблемы:  
          - Высокие вычислительные затраты.  
          - Затухающие или взрывные градиенты.  
- Ограничения РНС:
        - Забывание информации на длинных последовательностях.  
        - Сложность настройки параметров.  
        - Медленная работа из-за последовательной обработки.


Механизм внимания в нейронных сетях
- Механизм внимания в нейронных сетях помогает модели динамически выделять ключевые
 элементы данных при их обработке. Например, при переводе предложения «Кот сидит на ковре»
 на английский, модель может сосредоточиться на словах «кот» и «ковер», 
чтобы правильно перевести слово «сидит».
- Как он работает:
        1. Вычисление весов внимания: Каждому элементу входных данных (например, слову)
 присваивается вес, показывающий его важность для текущей задачи.
        2. Взвешенное суммирование: Данные комбинируются с учетом этих весов, создавая
 контекстный вектор с наиболее релевантной информацией.
        3. Использование результата: Этот вектор помогает модели принимать решения, например, 
предсказывать следующее слово.
- Как работает внимание в человеческом мозге?
        - В человеческом мозге внимание — это способность фокусироваться на определенных стимулах, 
отфильтровывая лишнее. Например, мы можем слушать собеседника в шумном кафе 
или мгновенно реагировать на внезапный звук.
        - Особенности внимания в мозге:
                - Селективность: Мы выбираем, на чем сосредоточиться, игнорируя отвлекающие факторы.
                - Гибкость: Внимание переключается или распределяется между задачами (например, при вождении и разговоре).
                - Нейронная основа: Оно регулируется сложной сетью мозга, включая префронтальную и теменную кору.
- Сходства между механизмами
        Механизм внимания в нейронных сетях вдохновлен работой мозга, и у них есть общие черты:
        1. Фокус на важном: Оба механизма выделяют релевантную информацию. В нейронных сетях 
это веса внимания, в мозге — активация нужных нейронных путей.
        2. Динамичность: И там, и там внимание адаптируется к контексту. Например, мозг переключается
 между словами при чтении, а сеть — между частями данных.
        3. Фильтрация шума: Оба процесса отсекают несущественное, улучшая эффективность.
- Различия между механизмами
- В нейронных сетях внимание — это математический процесс (например, скалярное произведение для вычисления весов). 
В мозге — это сложное взаимодействие нейронов, памяти и восприятия.  - Нейронные сети обычно оптимизированы
 под конкретную задачу (например, перевод).
 Мозг более универсален, справляясь с множеством ситуаций, включая творчество и планирование.
- В сетях внимание настраивается через оптимизацию на данных. В мозге оно развивается через опыт, но также зависит 
от врожденных механизмов.
- Сети чаще решают одну задачу за раз. Мозг может переключаться или распределять внимание между несколькими задачами.


Гипотеза предиктивного кодирования
- Гипотеза предиктивного кодирования — это теория, которая рассматривает мозг как машину предсказаний, постоянно формирующую ожидания о том, что произойдет дальше, на основе прошлого опыта.
- Она предполагает, что мозг активно прогнозирует сенсорные входные данные и корректирует свои модели, когда эти прогнозы оказываются ошибочными. 
- Прямое распространение аналогично предсказанию
        - Нейроны каждого слоя в мозге (или в искусственной нейронной сети) формируют «ожидания» о том, что должно произойти на следующем уровне обработки. Эти ожидания основаны на текущих связях между нейронами, которые называются весами.
        - В контексте мозга это означает, что каждый слой нейронов предсказывает активность следующего слоя, опираясь на усвоенные ранее шаблоны. Например, если вы слышите начало мелодии, ваш мозг может предсказать, какие ноты последуют дальше.
- Ошибка предсказания
        - Ошибка предсказания возникает, когда фактический результат отличается от ожидаемого. Она рассчитывается как разница между реальным значением (y) и предсказанным значением (ŷ), то есть y - ŷ.
        - В мозге это можно представить как момент, когда реальность не соответствует ожиданиям — например, если вместо ожидаемой ноты вы услышали другую. Эта разница сигнализирует о необходимости корректировки.
- Обратное распространение
        - Ошибка не просто фиксируется — она распределяется обратно через сеть. Этот процесс называется обратным распространением. В искусственных нейронных сетях градиенты ошибки вычисляются для каждого веса, чтобы понять, насколько он повлиял на итоговую ошибку.
        - В рамках гипотезы предиктивного кодирования мозг делает нечто похожее: сигналы ошибки передаются назад, чтобы скорректировать внутреннюю модель мира. Это помогает нейронам лучше подготовиться к будущим предсказаниям.
- Корректировка модели
        - Модель обновляется, чтобы уменьшить ошибку в будущем. Веса корректируются пропорционально их вкладу в ошибку, согласно формуле: Δw = -η ∂L/∂w, где:
          - Δw — изменение веса,
         - η — скорость обучения (параметр, определяющий, насколько сильно веса будут изменены),
     - ∂L/∂w — градиент функции потерь относительно веса, показывающий, как вес влияет на ошибку.
        - В мозге этот процесс можно сравнить с пластичностью синапсов: связи между нейронами усиливаются или ослабляются в зависимости от того, насколько точно они предсказали входные данные.
- Гипотеза предиктивного кодирования описывает замкнутый цикл:
        1. Мозг формирует предсказание (прямое распространение).
        2. Сравнивает его с реальностью и вычисляет ошибку (ошибка предсказания).
        3. Распределяет эту ошибку обратно через сеть (обратное распространение).
        4. Корректирует свои связи, чтобы улучшить будущие предсказания (корректировка модели).


Иерархическая организация коры головного мозга
- Первые слои CNN: Распознают простые визуальные элементы (линии под разными углами, текстуры)
- Нейрон активируется максимально при соответствии паттерна на рецептивном поле его весам
- Средние слои: Комбинируют активации предыдущих слоев через веса
- Формируют детекторы более сложных структур (например, комбинация нейронов горизонтальных и вертикальных линий создает детектор прямоугольников)
- Глубокие слои: Формируют инвариантные к позиции и масштабу детекторы высокоуровневых объектов
- Распознают целостные объекты (лица, текст и др.) через свертку весов предыдущих слоев

Мозг одновременно обрабатывает разнообразные стимулы благодаря параллельной обработке.
- Последовательная обработка используется для задач, требующих упорядоченных шагов и внимания к деталям.
- Иерархическая обработка организует информацию от низкоуровневых признаков (например, края, углы) к сложным структурам (например, формы, объекты), особенно в сенсорных системах, таких как зрение.
- Зрительная иерархия включает до 10 уровней кортикальной обработки и до 14 уровней с подкорковыми структурами (сетчатка, латеральное коленчатое тело).
- Зона Бродмана 44 (BA44) участвует в обработке сложных иерархических структур в зрительно-пространственной области, играя супрамодальную роль для структур с длинными зависимостями.
- Мозг — гибридная система, объединяющая параллельную, последовательную и иерархическую обработку.
- Параллельная обработка обеспечивает быстрое обнаружение признаков и широкий сенсорный ввод.
- Последовательная обработка важна для сложных пошаговых когнитивных задач.
- Иерархическая обработка интегрирует параллельные и последовательные аспекты, выстраивая сложность от низкого к высокому уровню.
- Несмотря на силу параллельной обработки, мозг имеет ограничения по пропускной способности и использует внимание, создавая «последовательные узкие места».
- Мозг использует многоступенчатую систему памяти, начиная с сенсорного регистра, который кратковременно удерживает информацию.
- Затем информация перемещается в кратковременную память для временного хранения и повторения.
- Рабочая память активно хранит и манипулирует информацией для текущих когнитивных задач.
- Рабочая память включает активацию лобно-теменных областей мозга (префронтальная, поясная, теменная кора) и подкорковых областей.
- Компонент «центрального исполнителя» рабочей памяти отвечает за контроль и регулирование когнитивных процессов, направление внимания и координацию с долгосрочной памятью.
- Долгосрочная память служит обширным хранилищем знаний о прошлых событиях.
- Долгосрочная память включает эпизодическую (личные события), семантическую (общие знания) и процедурную память.
- Процедурная память позволяет навыкам становиться «автоматическими» благодаря многократной практике.
- Автоматизированные навыки требуют меньше сознательных усилий и энергии.
- Это высвобождает сознательные ресурсы для более сложных задач.
- Рабочая память — это активное рабочее пространство для манипулирования информацией.
- Нейропластичность — это фундаментальная способность нервной системы подвергаться адаптивным структурным и функциональным изменениям в ответ на внутренние или внешние стимулы.
- Нейропластичность включает реорганизацию структуры, функций или связей мозга, даже после травм.
- Ключевые механизмы нейропластичности включают регенерацию нейронов/коллатеральное прорастание (синаптическая пластичность и нейрогенез) и функциональную реорганизацию (эквипотенциальность и викариация).
- Синаптическая пластичность — это способность производить зависящие от опыта, долгосрочные изменения в силе нейронных связей.
- Синаптическая пластичность иллюстрируется долгосрочной потенциацией (LTP) и долгосрочной депрессией (LTD).
- Повторяющиеся нервные импульсы могут увеличивать или уменьшать силу синаптической передачи.
- Нейропластичность означает, что архитектура и функциональные пути мозга динамически адаптируются и реорганизуются на основе опыта, обучения и травм.
- Обучение в мозге фундаментально связано с синаптической пластичностью (LTP/LTD), которая контролирует поток информации на нейронном уровне.
- Нейрогенез добавляет измерение обучения, создавая новые вычислительные единицы.
- Центральной концепцией коннекционизма является распределенное представление.
- Знакомые сущности кодируются паттернами активности, распределенными по многим единицам.
- Каждая единица участвует в представлении нескольких сущностей.
- Распределенные представления объясняют богатство и тонкость отношений между сущностями.
- Они также обеспечивают устойчивость к повреждениям, приводя к «грациозной деградации» информации.
- Мозг также использует разреженное кодирование.
- При разреженном кодировании только небольшое подмножество нейронов активно для кодирования определенного контекста.
- Сильное сходство между мозгом и продвинутыми нейронными сетями — это широкое использование распределенных представлений.
- Разреженная активация естественным образом возникает в обученных архитектурах трансформеров.
- Эта динамическая разреженность является поразительной параллелью.
- Это предлагает преимущества, такие как эффективность и интерпретируемость в ИИ.
- Человеческий мозг является чрезвычайно энергоэффективным устройством.
- Он может выполнять вычисления, эквивалентные экзафлопу, с низким энергопотреблением — всего 20 ватт.
- Мозг работает с присущими ему ограничениями когнитивной нагрузки.
- Последовательная обработка часто накладывает более высокую когнитивную нагрузку.
- Чтобы справиться с этим, мозг использует автоматизм.
- Многократно выполняемые навыки становятся процедурными и требуют меньше сознательных усилий и энергии.


Трансформеры: архитектура и параллельная контекстуальная обработка

Основная архитектура: кодировщик-декодер, самовнимание и позиционное кодирование
- Трансформеры — это тип моделей ИИ с архитектурой кодировщика-декодера.
- Они полностью полагаются на самовнимание для вычисления представлений входных и выходных данных.
- Архитектура состоит из кодировщика, который обрабатывает входную последовательность.
- Декодер итеративно генерирует выходные данные на основе закодированного представления.
- Входные токены преобразуются во входные встраивания.
- Входные встраивания — это плотные векторы фиксированного размера, захватывающие семантическое значение.
- К входным встраиваниям добавляется позиционное кодирование.
- Оно предоставляет явную информацию о позиции каждого токена в последовательности.
- Позиционное кодирование позволяет модели понимать порядок и отношения.
- Оно часто достигается с использованием синусоидальных и косинусоидальных функций с различными частотами.
- Это позволяет модели обрабатывать последовательности произвольной длины.
- Основным механизмом является самовнимание.
- Самовнимание динамически взвешивает важность различных токенов в одной входной последовательности.
- Для каждого токена самовнимание вычисляет «оценки внимания» относительно каждого другого элемента.
- Это включает преобразование входных встраиваний в три изученных вектора: Запрос (Q), Ключ (K) и Значение (V).
- Скалярное произведение между Q и K измеряет релевантность.
- Кодировщик обычно состоит из стека идентичных слоев.
- Каждый слой включает механизм многоголовочного внимания и полносвязную сеть.
- Остаточные связи и нормализация слоев применяются вокруг подслоев.
- Это стабилизирует обучение и облегчает поток градиента.
- Трансформеры явно кодируют позиционную информацию с использованием математических функций.
- Это умное инженерное решение проблемы, которую мозг (и РНС) решают неявно.
- Механизм самовнимания, особенно многоголовочное внимание, позволяет трансформерам динамически взвешивать важность различных частей входной последовательности.
- Это очень похоже на избирательное внимание мозга.
- Мозг динамически распределяет ресурсы обработки.


Сравнительный анализ: мозг против РНС против трансформеров
- Мозг: Мозг функционирует как сложная гибридная система, интегрирующая параллельную и последовательную обработку.
- Он использует параллелизм для быстрого обнаружения низкоуровневых признаков и одновременной обработки сенсорных входных данных.
- Для сложных, логических или пошаговых задач он задействует последовательную обработку.
- Мозг организован иерархически, создавая сложные представления из более простых.
- Внимание действует как динамический распределитель ресурсов, создавая «последовательные узкие места» для сознательной обработки.
- РНС: Фундаментально разработаны для последовательной обработки.
- РНС обрабатывают входные данные по одному элементу за раз.
- Эта присущая последовательность ограничивает их способность использовать параллельные вычисления.
- Это приводит к более медленному обучению и выводу.
- Трансформеры: Представляют собой сдвиг в сторону параллельной обработки.
- Они могут обрабатывать целые последовательности одновременно.
- Это позволяет избежать последовательного узкого места РНС.
- Их механизм позиционного кодирования явно обеспечивает последовательный контекст.

Память и долгосрочные зависимости: глубокое погружение
- Мозг: Обладает сложной, многоступенчатой системой памяти: сенсорной, кратковременной, рабочей и долгосрочной.
- Рабочая память является активным рабочим пространством для манипулирования информацией.
- Долгосрочная память обеспечивает надежное, обширное хранение и извлечение в течение длительных периодов, включая процедурную память.
- РНС: Используют скрытое состояние как форму кратковременной памяти.
- Базовые РНС значительно затрудняются с долгосрочными зависимостями из-за исчезающих/взрывающихся градиентов.
- Это приводит к затуханию информации и предвзятости в отношении недавних данных.
- Расширенные варианты (LSTM и GRU) смягчают это с помощью механизмов вентилей.
- Это позволяет лучшему потоку информации в долгосрочной перспективе и избирательному сохранению памяти.
- Трансформеры: Не имеют рекуррентного механизма «памяти».
- Они обрабатывают долгосрочные зависимости с помощью механизма самовнимания.
- Самовнимание позволяет каждому элементу напрямую взаимодействовать со всеми другими элементами в последовательности.
- Оно динамически взвешивает их релевантность.
- Позиционное кодирование обеспечивает необходимый последовательный контекст.
- Трансформеры превосходно захватывают глобальный контекст и долгосрочные отношения.
- Память мозга представляет собой сложное взаимодействие обоих типов, с различными нейронными субстратами.


Нейронные представления: распределенные, локальные и разреженные
- Мозг: В основном использует распределенные представления.
- Концепции кодируются как паттерны активности по многочисленным нейронам.
- Каждый нейрон участвует в нескольких представлениях.
- Это обеспечивает надежность и позволяет создавать богатые семантические отношения.
- Мозг также использует разреженное кодирование.
- При разреженном кодировании только небольшое подмножество нейронов активно для данного входного сигнала.
- РНС: Используют распределенные представления для своих внутренних состояний и изученных признаков.
- Трансформеры: В значительной степени полагаются на распределенные представления через свои входные и выходные встраивания.
- В них наблюдается возникающая динамическая разреженная активация в скрытых слоях.
- Только небольшой процент нейронов активен для любого данного входного сигнала.
- Это напоминает разреженное кодирование мозга.
- Динамическая разреженность зависит от входных данных.
- Она способствует эффективности и интерпретируемости.
- Естественное появление динамической разреженной активации в трансформерах является поразительной, нетривиальной параллелью разреженному кодированию в мозге.

Обучение и адаптивность: параллели с биологической пластичностью
- Мозг: Обладает высокой пластичностью (структурными и функциональными изменениями, нейропластичностью).
- Синаптическая пластичность (LTP/LTD) используется для обучения.
- Происходит нейрогенез (создание новых нейронов).
- Обучение адаптивно через обратную связь.
- РНС и трансформеры (общие АНС): Обучаются путем корректировки весов и смещений через обратное распространение ошибки и градиентный спуск.
- Это аналогично синаптической пластичности.
- Обучение включает итеративное уменьшение ошибки.
- Пластичность мозга включает как функциональные, так и структурные изменения (нейрогенез, аксональное прорастание).
- Современные АНС в основном демонстрируют _параметрическую_ пластичность (корректировка фиксированных весов в фиксированной архитектуре).
- Мозг может фундаментально реконфигурировать свои вычислительные единицы.
- Способность мозга генерировать _новые нейроны_ или _реорганизовывать кортикальные области_ после травмы представляет собой уровень структурной пластичности, не встречающийся в стандартных АНС.

Эффективность и использование ресурсов
- Мозг: Чрезвычайно энергоэффективен (эквивалент экзафлопа при 20 Вт).
- Управляет когнитивной нагрузкой с помощью автоматизма.
- РНС: Более медленное обучение из-за последовательной обработки.
- Вычислительно интенсивны, требуют значительных ресурсов памяти.
- Имеют более низкие требования к памяти, чем у трансформеров.
- Трансформеры: Более быстрое обучение благодаря параллельной обработке.
- Хорошо распараллеливаются.
- Высокое использование памяти.
- Требуют значительной вычислительной мощности (GPU/TPU).
- Текущие модели ИИ на порядки менее энергоэффективны, чем человеческий мозг.
- РНС эффективны по памяти, но медленны из-за последовательности.
- Трансформеры быстры из-за параллелизма, но интенсивны по памяти.


- Исследования предполагают, что сознание возникает через глобальное рабочее пространство (ГРП), где информация интегрируется и транслируется в мозге.  
- Кажется вероятным, что бессознательные модули обрабатывают информацию параллельно, а внимание выбирает, что станет сознательным.  
- Доказательства указывают на участие префронтальной и теменной коры в ГРП, но роль префронтальной коры остается предметом дискуссий.  
- Память, мышление и знания связаны с ГРП, взаимодействуя через ассоциативные зоны, такие как корковые области.  

Теория глобального рабочего пространства (ТГРП) предлагает объяснение, как сознание возникает из взаимодействия распределенных нейронных процессов. Она описывает мозг как систему, где информация из бессознательных модулей становится сознательной через центральный узел — глобальное рабочее пространство (ГРП). Эта теория интегрирует аспекты памяти, мышления, сознательных и бессознательных процессов, знаний и ассоциативных зон, что делает ее полезной для понимания сложных когнитивных функций.  

Прайминг – это психологическое явление, при котором воздействие одного стимула влияет на реакцию на последующий связанный стимул.

Ниже представлена схема, иллюстрирующая, как мозг работает согласно ТГРП, с учетом всех указанных аспектов:  

Вход сенсорной информации:  
   - Информация от органов чувств (зрение, слух, осязание) поступает в специализированные бессознательные модули, такие как зрительная кора (V1) или слуховая кора (A1).  
   - Эти модули обрабатывают данные параллельно, без сознательного осознания, выполняя задачи, такие как распознавание образов или звуков.  

Параллельная обработка в бессознательных модулях:  
   - Бессознательные модули включают области, ответственные за разные типы памяти:  
     - Процедурная память: Хранится в базальных ганглиях и мозжечке (например, навыки, как езда на велосипеде).  
     - Семантическая память: Хранится в височных и префронтальных ассоциативных зонах (например, факты и концепции).  
     - Эпизодическая память: Зависит от гиппокампа и медиальной височной доли (например, личные события).  
   - Здесь также происходят бессознательные процессы, такие как прайминг и автоматические реакции.  

Механизм внимания:  
   - Префронтальная и теменная кора действуют как механизм внимания, выбирая релевантную информацию из бессознательных модулей на основе целей, контекста и значимости.  
   - Внимание действует как "прожектор", направляя выбранную информацию в ГРП.  

Глобальное рабочее пространство (ГРП):  
   - Это распределенная сеть, включающая префронтальную кору (исполнительные функции, принятие решений), теменную кору (внимание, пространственная обработка), переднюю височную долю (интеграция семантики) и прекунеус (саморефлексия).  
   - ГРП имеет ограниченную емкость, схожую с рабочей памятью, и интегрирует информацию в единое сознательное переживание.  

Нейронное зажигание:
   - Когда информация входит в ГРП, происходит быстрое и когерентное активация сети, называемая нейронным зажиганием.  
   - Это включает рекуррентную обработку и синхронизацию нейронной активности, что делает информацию доступной для сознания.  

Глобальная трансляция:
   - Информация в ГРП транслируется всем бессознательным модулям, обеспечивая координацию действий, обучение и решение задач.  
   - Например, сознательное восприятие красной машины включает трансляцию этой информации в моторные области для действия и в системы памяти для контекста.  

Обратные связи:
   - Существуют непрерывные взаимодействия между ГРП и бессознательными модулями, позволяющие обновлять сознательное содержание и консолидировать память.  
   - Например, распознавание лица включает обратную связь между ГРП и визуальными/памятными системами.  

Взаимодействие с памятью: 
   - Рабочая память: Тесно связана с ГРП, удерживая информацию для сознательной манипуляции.  
   - Долгосрочная память: Извлекается в ГРП при необходимости для сознательного мышления.  
   - Ассоциативные зоны (префронтальная, теменная, височная кора) интегрируют мультимодальную информацию и хранят знания.  

Сознательные и бессознательные процессы:  
   - Сознательные процессы происходят в ГРП (например, осознанное мышление, принятие решений).  
   - Бессознательные процессы остаются в специализированных модулях (например, автоматические навыки, сенсорная обработка).  

Мышление и знания:
    - Мышление включает как сознательные процессы в ГРП (например, размышления), так и бессознательные в модулях (например, автоматические ассоциации).  
    - Знания хранятся в ассоциативных зонах (например, семантические знания в височной коре) и доступны через ГРП.  

- Глобальное рабочее пространство как центральный узел: ГРП — это функциональное место, где информация временно хранится и совместно используется различными областями мозга. Оно имеет ограниченную емкость, что объясняет явления, такие как слепота невнимания, когда не вся сенсорная информация достигает сознания.  
- Роль бессознательных модулей: Мозг состоит из множества специализированных процессоров, работающих параллельно, таких как зрительное восприятие, обработка языка и управление моторикой. Эти модули формируют "коалиции", конкурируя за доступ к ГРП.  
- Внимание как "прожектор": Внимание выбирает наиболее релевантную информацию для ГРП, действуя как фильтр на основе целей и контекста. Это обеспечивает, что только важная информация становится сознательной.  
- Концепция трансляции: После попадания в ГРП информация транслируется всем бессознательным модулям, влияя на когнитивные функции, такие как память, принятие решений и обучение. Процесс "зажигания" обеспечивает, что сигнал достигает порога для глобальной доступности.  
- Ограниченная емкость: ГРП имеет ограниченную емкость, схожую с рабочей памятью, что требует избирательного внимания и объясняет компромисс между объемом и глубиной обработки информации.  

- Фронто-париетальная сеть: Префронтальная кора (ПФК) и теменная кора играют центральную роль в организации трансляции и облегчении сознательного доступа.  
- Кортико-таламическая система: Эта система обеспечивает динамический обмен информацией, необходимый для сознания, с таламусом, выступающим ретрансляционной станцией.  
- Нейронное зажигание: Сознательный доступ связан с активацией в париетальных и префронтальных цепях, медленной волной P3, высокочастотными колебаниями и обменом информацией между отдаленными областями.  
- Временная динамика: Сознание отстает от мира примерно на 1/3 секунды, с первоначальной бессознательной обработкой в течение первых 100–300 мс, прежде чем информация достигает ГРП.  

- Рабочая память: ГРП напоминает рабочую память, выступая "сценой", где сознательное содержание удерживается и обрабатывается.  
- Внимание: Внимание управляет потоком информации в ГРП, обеспечивая, что только релевантная информация становится сознательной.  
- Восприятие и память: Сознательное восприятие интегрируется с памятью через ГРП, обеспечивая доступ к автобиографической памяти и контексту.  

- Декларативная память: Осознаваемая, включает эпизодическую (личные события) и семантическую (факты, концепты), зависит от медиальной височной доли и гиппокампа.  
- Процедурная память: Неосознаваемая, связана с базальными ганглиями и мозжечком, например, навыки.  
- Перцептивная память: Неосознаваемая, проявляется через прайминг, зависит от сенсорных кор.  
- Пространственная память: Частично осознаваемая, связана с гиппокампом и париетальной корой, например, навигация.  

Эмоции усиливают память, действуя как "фиксаторы", с гиппокампом, связывающим эмоции с воспоминаниями, а корой, обеспечивающей долговременное хранение.  

Мышление включает сознательные процессы в ГРП (например, размышления) и бессознательные в модулях (автоматические ассоциации). Знания хранятся в ассоциативных зонах — префронтальной, теменной и височной коре, которые интегрируют мультимодальную информацию. Эти зоны являются частью ГРП, обеспечивая доступ к семантическим и эпизодическим знаниям.  

Процесс обработки информации согласно ТГРП можно представить следующим образом:  
1. Бессознательная параллельная обработка: Специализированные модули обрабатывают информацию.  
2. Конкуренция за внимание: Информация конкурирует за доступ к ГРП на основе значимости.  
3. Избирательное внимание: Внимание выбирает информацию для ГРП.  
4. ГРП: Отобранная информация поступает в ГРП с ограниченной емкостью.  
5. Нейронное зажигание: Запускается процесс зажигания для глобальной доступности.  
6. Глобальная трансляция: Информация транслируется всем модулям.  
7. Влияние на когнитивные функции: Обеспечивает координацию, обучение, принятие решений.  
8. Обратные связи: Взаимодействия между сознательными и бессознательными областями.  

Структура кортикальной колонки:
   - Состоит из 7 слоев, каждый с уникальной функцией:  
     - Слои 2-3: сложная обработка информации, горизонтальная передача между колонками, интеграция сенсорных данных.  
     - Слой 4: получает прямые сигналы от таламуса, распределяет информацию в другие слои.  
     - Слой 5: крупные пирамидные нейроны отправляют сигналы к подкорковым структурам.  
     - Слой 6: передает информацию в глубокие области мозга.  


Роль нейронов:
   - Звездчатые нейроны (зернистый слой): формируют ассоциативные связи и образы, могут соединяться за пределами колонки.  
   - Пирамидальные нейроны: действуют как сумматоры сигналов; их дендриты обеспечивают локальное взаимодействие между колонками.  

Механизмы регуляции активности:  
   - Латеральное торможение: создает четкие границы между активными колонками, предотвращая распространение возбуждения.  
   - Латеральное побуждение: регулирует общий уровень активности мозга.  

Два логических слоя коры:  
   - Внешний слой (мелкие пирамидальные клетки) имеет более высокий порог активации, чем внутренний.  
   - Звездчатые нейроны наружного слоя менее нейропластичны, что приводит к разной активности при одинаковых входах.  

Режимы активности колонки:  
   - Полная активность: активируются все слои (обработка сенсорных данных, воображение).  
   - Частичная активность: задействован только верхний слой (работа с абстрактными ассоциациями).  

Функциональные особенности:  
   - Сенсорная кора: двухуровневый механизм позволяет обрабатывать информацию и формировать образы без прямого сенсорного ввода (воображение).  
   - Ассоциативные области: способны создавать связи между объектами с минимальными общими признаками благодаря высокому порогу активации пирамидальных нейронов.  

Глобальное рабочее пространство:  
   - Связи в слоях 2-3 образуют сеть, предположительно отвечающую за рабочую память и интеграцию информации.


Множественность гамма-процессов:
- В мозге существуют как минимум два различных механизма генерации гамма-активности
- Эти механизмы имеют разную нейронную природу и функциональное назначение
- Мозг использует строгую временную последовательность активации: сначала широкополосные, затем узкополосные события
- Существует временное разделение между разными типами нейронной обработки
- Кодирование памяти включает два параллельных процесса с разными электрофизиологическими характеристиками
- Мозг по-разному обрабатывает информацию, которая будет запомнена vs забыта, уже на этапе кодирования
- Различные типы гамма-активности играют специализированные роли в формировании памяти
- Широкополосная активность связана с первичной обработкой входящих сигналов (коррелирует с вызванными потенциалами)
- Узкополосная активность связана с более высокоуровневой когнитивной обработкой (реакция на предъявление слов)
- Мозг использует частотно-специфические механизмы для разных аспектов когнитивной обработки
- Существует функциональное разделение между процессами восприятия и когнитивной обработки на уровне гамма-ритмов


Процесс восприятия и обработки информации
- Сенсорные зоны коры обрабатывают первичные сигналы от таламуса, создавая нейронные паттерны активности, представляющие базовые характеристики восприятия. 
- Сенсорный паттерн разделяется на несколько паттернов, каждый из которых представляет отдельный признак исходного сигнала. Для каждого сигнала в сенсорной коре есть несколько колонок, выделяющих отдельные признаки. Эти колонки расположены близко друг к другу. Если бы колонка просто копировала сигнал, то паттерн оставался бы таким же, как и исходный, сохраняя топологию. Таким образом, происходит разложение начального паттерна на признаки, и для каждого признака можно определить его значение по близости. Обработка более сложных признаков происходит в других областях, а менее сложные, но не элементарные в верхних слоях колонок.
- Обработка в этих зонах происходит автоматически. Обработка сенсорной информации организована иерархически. Первичные сенсорные зоны служат якорями для интеграции модальностей. 
- Первичная сенсорная кора обладает высокой нейропластичностью, т.е. любые комбинации возбуждённых колонок обрабатываются без учета предыдущего опыта.
- После первичной обработки сенсорные паттерны передаются в более высокие области коры, где происходит выделение более сложных признаков. При этом топографическая информация, то есть пространственные отношения между сигналами, сохраняется, то есть допустим есть паттерн в сенсорном канале он проецируется в виде паттерна в сенсорую кору, причем на несколько типов колонок сразу, то есть там существуют дупликаты или сохраняющие топологические отношения паттерны но они разных признаков и эти паттерны передаются в другие области.
- По мере перехода к ассоциативной коре сложность обработки увеличивается, переходя от специализированных областей к мультисенсорным.
- Ассоциативные зоны содержат представительства различных сенсорных и моторных областей, что влияет на целостное восприятия объектов и событий.
- Ассоциативные области обладают нейропластичностью, позволяющей адаптироваться к новому опыту, но со временем её степень может снижаться по мере стабилизации когнитивных функций.
- Обработка сложных стимулов происходит за счет интеграции сенсорной информации. Синаптическая пластичность обеспечивает обучение и адаптацию, динамически изменяя нейронные связи. Ассоциативные волокна координируют работу областей внутри полушария, а мозолистое тело — между полушариями. Префронтальная кора регулирует внимание.
- В моторных и примоторных областях связи двусторонние, что необходимо для формирования ассоциативных связей и рефлекторных дуг.

Определение области коры:
- Область коры представляет собой совокупность кортикальных колонок, выполняющих схожие функции: хранение, обработка, передача и получение информации.
- Работа кратковременной и долговременной памяти в конкретной области определяется принципами функционирования кортикальных колонок, входящих в её состав.
- Информация передается последовательно от одной области к другой.  
- При переходе между областями происходит снижение:
- Плотности связей (уменьшение количества нейронных соединений).        
- Нейропластичности (снижение способности к структурным изменениям).
- Последовательная обработка приводит к выделению ключевых признаков информации, которые ассоциируются с активностью определённых групп нейронов.
- В областях с высоким уровнем обработки формируется паттерн активности, соответствующий наиболее частым или значимым сигналам. Это отражает "оптимизацию" обработки на основе повторяющегося опыта.
- По мере продвижения информации от первичных к ассоциативным областям происходит её абстрагирование — переход от конкретных сенсорных данных к обобщенным признакам и ассоциациям.


Обобщение
- В процессе обучения нейронные ансамбли могут перекрывать друг друга. Когда различные ансамбли перекрываются, они начинают действовать как расщепители, выделяя повторяющиеся, общие аспекты. Нейроны, ранее участвующие в одном ансамбле, могут изменять свою активность и переходить в новый ансамбль в зависимости от текущего контекста задачи, что способствует динамическому обобщению.
- На основе повторяющихся стимулов синаптические связи между нейронами начинают укрепляться через процесс долгосрочной потенциации (LTP).
- Процесс обучения на основе плато укрепляет только те синаптические связи, которые участвуют в образовании стабильных паттернов, что приводит к созданию более устойчивых латентных представлений, кодирующих обобщения, скрытые закономерности среды между различными сенсорными аспектами.

Механизм внимания
- Сканирование стимулов: Подкорковые структуры, такие как верхние холмики (tectum), базальные ганглии и ретикулярная формация, анализируют сенсорные входы, выделяя сигналы по критериям:
        - Новизна: Ретикулярная формация активируется при обнаружении нового сигнала, передавая его таламусу.
        - Угроза: Амигдала участвует в оценке эмоционального или опасного содержания сигнала.
        - Полезность: Базальные ганглии анализируют сигналы на основе прошлого опыта и значимости для текущей задачи.
- Сканирование стимулов осуществляется на ранних этапах сенсорной обработки, еще до полного осознания. Эти структуры получают первичные сигналы от сенсорных рецепторов через соответствующие ядра таламуса, где уже происходит первичная фильтрация данных.
- Таламус выступает в роли "вратаря внимания", усиливая значимые сигналы и подавляя фоновый шум, она получают информацию от подкорковых структур, префронтальной коры.
    - Сильные сигналы от подкорковых структур (например, от ретикулярной формации или амигдалы) усиливают активность специфических таламических ядер (например, зрительного, слухового).
    - Слабые сигналы подавляются за счет латерального торможения, где соседние нейроны ингибируют друг друга.
    - Подкорковые структуры (ретикулярная формация, амигдала, верхние холмики) направляют сигналы через таламические ядра в первичную сенсорную кору.
    - Обратные связи от сенсорной коры помогают уточнять значимость сигналов.
- Активация коры: 
        - Значимый стимул активирует специализированные сенсорные и ассоциативные зоны , а именно энграммы в задней нижней височной коре (обработка зрительных объектов), задней нижней теменной коре (пространственная ориентация) и ассоциативных областях. Эти области проецируются на префронтальную ассоциативную кору.
- Контроль префронтальной коры: 
    - Сигналы приоритетного стимула передаются обратно в таламус и ассоциативные зоны для их усиления.
    - Системы торможения (ГАМК-эргические нейроны) снижают активность нейронов, связанных с менее значимыми стимулами.
- Тета-ритмы гиппокампа: 
    - Гиппокамп через энторинальную кору модулирует активность нейронных ансамблей, связанные с текущим объектом внимания.
    - Тета-ритмы синхронизируют активность в ассоциативных зонах коры, помогая поддерживать фокус.
    - Гиппокамп передает сигналы в таламус и верхние холмики, влияя на двигательные программы, необходимые для переноса внимания (например, поворот головы или глаз).
    - Тета-ритмы способствуют закреплению восприятия в кратковременной памяти, связывая его с текущим контекстом.

Ввод данных
Сенсорный вход:
    - Информация из органов чувств поступает в первичные сенсорные зоны коры головного мозга.
        - Зрительная информация обрабатывается в затылочной коре, слуховая — в височной коре.
    - Здесь выделяются базовые характеристики стимула:
        - Для зрения — цвет, форма, движение.
        - Для слуха — частота, громкость.
Роль таламуса:
    - Таламус получает сенсорные сигналы и выполняет фильтрацию, отбирая только те, которые релевантны текущей задаче.
    - Отбор осуществляется под контролем префронтальной коры (ПК), которая направляет сигналы о текущих приоритетах.
Эмоциональная окраска:
    - Если стимул имеет эмоциональную значимость (угроза, награда), амигдала усиливает эти сигналы, добавляя к ним эмоциональную модуляцию.
    - Это повышает вероятность сохранения эмоционально значимой информации в КП.
Кодирование информации
Создание нейронного паттерна:
    - В сенсорных и ассоциативных зонах коры создается активный паттерн нейронов, который отражает характеристики воспринимаемого стимула.
    - Гамма-ритмы (30–80 Гц):
        - Обеспечивают синхронизацию работы нейронных групп, объединяя отдельные характеристики стимула в целостное представление.
Координация через осцилляции:
    - Тета-ритмы (4–8 Гц):
        - Управляют взаимодействием между различными областями мозга (таламус, гиппокамп, кора), помогая удерживать отдельные элементы информации в разных временных фазах.
    - Каждый элемент информации кодируется своей фазой в тета-ритме, предотвращая их смешение.
Удержание информации
        Роль префронтальной коры (ПК):
    - ПК выполняет функцию "менеджера", распределяя внимание и определяя, какая информация наиболее важна для текущей задачи.
    - Поддерживает активность соответствующих нейронных сетей через сигналы в ассоциативные зоны коры и таламус.
        Поддержание через осцилляции:
    - Тета-ритмы продолжают координировать работу областей мозга, распределяя временные слоты для хранения нескольких элементов информации одновременно.
    - Гамма-ритмы поддерживают локальную синхронизацию нейронов, усиливая их взаимодействие.
        Поддержание через рекуррентные сети (обратные связи):
    - Нейронные петли обратной связи в коре (например, между слоями L2/3 и L5/6) поддерживают активность паттерна даже после прекращения сенсорного стимула.
    - Это позволяет информации оставаться активной для дальнейшей обработки.
        Механизм приоритизации:
    - Если количество информации превышает лимит кратковременной памяти (обычно 4–7 элементов), менее значимые элементы подавляются.
    - Стриатум (полосатое тело) и базальные ганглии участвуют в этом процессе, управляя механизмами подавления и обновления.
        Обновление и замещение
           Замещение менее значимых элементов:
    - При поступлении новой информации происходит замещение элементов с низким приоритетом.
    - Приоритет определяется эмоциональной значимостью (амигдала), вниманием (ПК) или актуальностью задачи.
        Интеграция новой информации:
    - Новые элементы сравниваются с ранее сохраненными паттернами, чтобы определить, являются ли они уникальными или дополняют существующую информацию.
Взаимодействие с долговременной памятью
        Извлечение воспоминаний:
    - При необходимости ПК инициирует процесс поиска информации в долговременной памяти.
    - Сигнал направляется через энторинальную кору к гиппокампу, который извлекает релевантные воспоминания.
    Передача воспоминаний в кору:
    - Гиппокамп активирует соответствующие нейронные сети в коре, восстанавливая ранее сохраненные паттерны.
        Интеграция воспоминаний:
    - Извлеченные данные интегрируются с текущей информацией, что помогает формировать общее представление или план действий.
Генерация ментальных образов
        Инициация:
    - ПК инициирует процесс воображения или моделирования, направляя сигналы в ассоциативные и сенсорные зоны коры.
    - Пример: Для визуальных образов активируются зрительные области (затылочная кора), для звуковых — слуховые зоны.
        Создание образа:
    - Ассоциативные зоны объединяют информацию из разных сенсорных модальностей, формируя целостный ментальный образ.
    - Это позволяет моделировать будущее или воспроизводить прошлое в воображении.
Роль нейропластичности
- Кратковременные изменения связей:
    - В процессе удержания информации в КП усиливаются синаптические связи между активными нейронами.
    - Эти изменения являются временными и исчезают, если информация не переходит в долговременную память.
- Закрепление в долговременной памяти:
    - Если информация многократно повторяется или эмоционально значима, гиппокамп запускает процесс консолидации, при котором временные связи превращаются в долговременные.


Долговременная память

Когнитивные процессы памяти и консолидации зависят от сложного взаимодействия между гиппокампом и корой, что обеспечивает формирование долговременных воспоминаний и их интеграцию с существующими структурами знаний.
Энграммы -  это группы нейронов, которые активируются в момент события, формируя основы памяти. Эти нейронные сети кодируют и хранят информацию, которая затем передаётся в кору для долговременного хранения.

Формирование энграмм: Когда человек сталкивается с новым стимулом, активируются нейроны, создавая временные нейронные сети, которые представляют информацию об этом опыте — энграммы, они образуются в гиппокампе. Эти сети активируются при повторных восприятиях или мыслях о событии.
Консолидация памяти: Гиппокамп передает энграммы в кору головного мозга, где они становятся частью долговременной памяти через консолидацию. 
Сон и консолидация: Во время медленного сна гиппокамп часто активирует энграммы в правильном порядке это приводит к долговременной потенциации. В фазе REM (быстрого сна) происходит интеграция новых воспоминаний с уже существующими.
Долговременная потенциация: При многократной активации нейронных сетей усиливаются синаптические связи между нейронами, что повышает вероятность их активации в будущем. 
Ритмы мозга: Тета-ритмы регулируют взаимодействие между гиппокампом и корой, открывая окна для повторной активации информации во время сна в коре. Гамма-ритмы участвуют в обработке информации и её интеграции в долговременные нейронные сети, синхронизируя краткосрочную и долговременную память.


Роль осцилляций
Частота осцилляций: Колебания делят время на циклы, в которых нейроны могут «фиксировать» своё состояние.
Фазы осцилляций: Различные фазы колебаний кодируют уровни активности. Например, в одной фазе сигнал усиливает активность, а в другой ослабляет её.
Временной мост: Для сохранения информации сеть должна поддерживать связь между активностью в пиках и впадинах осцилляции. Это достигается за счёт:
    - Внутренних процессов (например, медленных ионов в мембранах).
    - Синаптической пластичности (долговременных изменений в связях между нейронами).
Осцилляции периодически усиливают или подавляют активность нейронов.
Состояния сети «квантуются» на основе этих циклов: активность нейрона фиксируется на определённом уровне (пороговые эффекты).
Если сеть имеет внутреннюю адаптацию, активность нейронов стремится к ближайшему устойчивому состоянию (например, 0, 0.5, 1).
Каждый уровень соответствует конкретной фазе или амплитуде входной осцилляции.
Осцилляции с короткими циклами менее подвержены шуму.
Могут поддерживать только 1-2 уровня активности, так как цикл слишком короток для разделения множества состояний.
Длинные периоды позволяют хранить больше уровней.
Но требуют от сети долгосрочных процессов, способных поддерживать связь между пиками осцилляции.
Для увеличения точности можно использовать несколько осцилляторов с разными фазами и частотами.
Их комбинация создаёт более тонкую дискретизацию, так как уровни активности усредняются.
- Осциллятор подаёт входной сигнал с частотой 10 Гц.
- Нейроны сети активируются в зависимости от амплитуды осцилляции.
- Мембранные процессы фиксируют уровень активности в течение каждого цикла (например, 0, 0.3, 0.6, 1).
- Если частота повышается до 50 Гц, сеть сохраняет только 2 уровня (0 и 1), так как времени на разделение состояний недостаточно.
Осцилляторный вход задаёт временные циклы активности.
Нейроны дискретизируют свою активность, формируя несколько устойчивых уровней.
Частота осцилляций определяет количество возможных уровней.
Сеть сохраняет информацию, поддерживая устойчивость уровней между циклами

В коре она образует большую сеть , так как на 2-3 слое существуют соединения между отдельными областями и скорее всего именно там глобальное рабочее пространство , именно по ней происходит восприятие любой информации как в рабочей памяти. 

Это похоже на -> с лева энкодер а с права декодер, по середине (префронтальная)  bottleneck , процессор - последний этап сжатия, и где происходит глубокая обработка информации , выделению базисных признаков, вообще это похоже на диагонализацию,  если рассматривать процесс как изменение базиса, то можно сказать, что в энкодере происходит своего рода преобразование признаков (или данных) в новый базис, где информация представлена более компактно и абстрактно, аналогично диагонализации или представлению данных в собственных базисах матрицы. Энкодер преобразует данные так, чтобы они были более удобны для обработки на глубоком уровне, устраняя пространственные детали, но сохраняя важную информацию.

Затем, в декодере, происходит обратное преобразование — он как бы восстанавливает исходный базис, то есть восстановливает пространственную структуру и детали данных, чтобы представить их в исходной форме (например, изображение или сегментированное изображение). Это похоже на возвращение данных из нового базиса в старый, восстанавливая их прежнюю форму, но с учётом информации, извлечённой энкодером.

Такое преобразование, как изменение базиса, позволяет оптимизировать представление данных, сокращая размер и повышая эффективность обработки. На практике это приводит к улучшению производительности сети, поскольку декодер использует более компактную и абстрагированную информацию, полученную энкодером, для восстановления точных выходных данных.

Сенсорные зоны обрабатывают простые признаки (линии, цвета, текстуры) и делают это автоматически, иерархически от самых элементарных до более сложных.
Сенсорная обработка начинается в первичных сенсорных областях где нейроны реагируют на простые стимулы. Далее информация передается в высшие корковые области, где происходит обработка более сложных признаков, таких как формы, движения, сложные звуковые паттерны. Эта иерархическая обработка соответствует кантовской идее о том, что чувственный опыт является основой для познания. Кант утверждал, что мы воспринимаем мир через "формы чувственности" – пространство и время, которые можно соотнести с ранними этапами сенсорной обработки.
Восприятие начинается с сенсорного кодирования (феноменальная часть). Это отражает субъективный, осознаваемый аспект восприятия.
Сенсорное кодирование действительно является основой восприятия. Феноменальный аспект (qualia) – это субъективное переживание, "как это – видеть красный цвет" или "слышать звук скрипки". Кант называл это "явлениями" (Erscheinungen), которые являются результатом взаимодействия "вещей в себе" (Ding an sich) с нашими формами чувственности , с органами чувств.
После первичной обработки паттерны передаются в высшие области с сохранением топографической информации. Сохранение пространственно-временных отношений важно для выявления большей информации. То есть для выявления большего количества признаков, категории , и в общем разных аспектов.
Улучшение и дополнение: Топографическая организация (например, ретинотопия в зрительной коре) сохраняется на разных уровнях обработки. Это позволяет мозгу сохранять пространственные отношения между стимулами. Сохранение пространственно-временных отношений критически важно для восприятия движения, глубины, а также для интеграции информации от разных органов чувств. Это соответствует кантовской идее о пространстве и времени как априорных формах чувственности, которые предшествуют опыту и организуют его.
Сложность обработки возрастает по мере перехода к ассоциативной коре, от специализированных к мультисенсорным областям. Так ассоциативная кора объединяет разно-модальную обработанную сенсорную информацию в целостную.
Ассоциативная кора играет ключевую роль в интеграции информации от разных сенсорных модальностей. Эта интеграция позволяет формировать целостное представление о мире.
Различные области мозга выделяют основные аспекты объектов (пространство, время, причинность).
Различные области мозга специализируются на обработке разных аспектов информации. Кант считал причинность априорной категорией рассудка, которая позволяет нам понимать связи между событиями. Современные исследования показывают, что мозг развивает представления о причинности на основе опыта, но сама способность к пониманию причинно-следственных связей может быть врожденной. Часто это так потому что мозг это предсказатель будущего, он хранит информацию которая помогает предсказывать будущее и часто пытается это сделать.
В ассоциативной коре сохранены категории/концепты как фильтры/интерпретаторы сенсорного опыта. "Разум интерпретирует признаки, используя категории." Более точно говорить о том, что _когнитивные процессы_, происходящие в мозге, включая память, внимание и мышление, участвуют в интерпретации сенсорной информации на основе имеющихся категорий и концептов.
Кант утверждал, что мы познаем мир не только через чувственный опыт, но и через априорные категории рассудка, такие как единство, множество, цельность, причинность и другие. Эти категории позволяют нам упорядочивать и интерпретировать чувственные данные. Современные когнитивные науки показывают, что мозг формирует концепты и категории на основе опыта, но сама способность к категоризации может быть врожденной. 
Сенсорная обработка напоминает кодирование информации в более компактное и абстрагированное от незначительной информации представление, а ассоциативная кора – последующую обработку и интерпретацию. Декодирование можно рассматривать как процесс восстановления информации для представления (воображения) в виде сенсорного образа или для прогнозирования. 
Хотя Мозг работает гораздо сложнее, чем простая схема кодирования-декодирования. Обработка информации происходит параллельно и рекурсивно, с множеством обратных связей. "Декодер" не просто восстанавливает исходный сигнал, но и генерирует прогнозы, ожидания, которые затем сравниваются с поступающей сенсорной информацией. Это лежит в основе перцептивного прогнозирования и обучения, а именно: "Декодирование можно рассматривать как процесс восстановления исходной информации... или когда мозг автоматически предсказывает будущее." Предсказательное кодирование – важная концепция в современной нейробиологии. Мозг постоянно генерирует прогнозы о будущем на основе прошлого опыта и текущей ситуации. Сравнение этих прогнозов с поступающей информацией позволяет корректировать модель мира и адаптироваться к изменениям.
Мозг не просто пассивно регистрирует информацию, но и активно генерирует прогнозы о будущем. Сравнение этих прогнозов с поступающей сенсорной информацией позволяет корректировать модель мира , уточняя не априорные категории или создавая их в ассоциативной коре. Это соответствует кантовской идее о том, что познание – это активный процесс, в котором разум упорядочивает и интерпретирует чувственные данные.
Если нет элементарных понятий, ты не сможешь хорошо оперировать более сложными понятиями, но сами понятия формируются через выделение паттернов в сенсорных данных, создание концептов в ассоциативных зонах мозга и их коррекцию через прогнозирование.
Формирование концептов происходит на основе сенсорного опыта, но также зависит от когнитивных процессов, таких как внимание, память и мышление. Прогнозирование играет важную роль в коррекции и уточнении существующих концептов. Этот процесс является непрерывным и динамичным.
Сенсорные зоны мозга обрабатывают простые признаки (линии, цвета, текстуры) после на основе простых признаков выделяются более сложные.
Информация передается от первичных сенсорных областей к высшим корковым, где происходит обработка форм, движений, сложных звуковых паттернов.
Мозг активно генерирует прогнозы о будущем на основе опыта и текущей ситуации.
Прогнозы сравниваются с сенсорной информацией, уточняя модели мира .
Способность кодировать и извлекать наш ежедневный личный опыт, называемая эпизодической памятью, поддерживается схемой медиальной височной доли (MTL), включая гиппокамп, который интенсивно взаимодействует с рядом специфических распределенных
корковых и подкорковых структур. Корковые компоненты этой системы выполняют ключевые функции в ряде аспектов восприятия и познания, в то время как структуры МТЛ опосредуют организацию и сохранение сети воспоминаний, детали которых хранятся в этих областях коры. Структуры МТЛ, и особенно гиппокамп, выполняют различные функции
в объединении информации из нескольких корковых потоков, поддерживая нашу способность кодировать и извлекать детали событий, составляющих эпизодические воспоминания. 
И второй вопрос можно ли считать знанием «чисто» сенсорные данные? Эмпирические исследования указывают, что одних ощущений недостаточно: мозг переводит их в более абстрактные репрезентации. Модель иерархии зрительной и других сенсорных систем предполагает, что от первичной коры к ассоциативным областям информация последовательно абстрагируется. Wei et al. (2024) показали с помощью фМРТ, что сигналы от первичных сенсорных областей (зрение, слух и пр.) постепенно редуцируются по мере продвижения вверх по иерархии, а паттерны активации становятся всё более мультисенсорными и абстрактными. Другими словами, по мере интеграции сенсорной информации образы «конденсируются» в менее детализированную и более общую форму, что согласуется с классической моделью Мезулама (1978) о преобразовании сенсорного входа в когнитивные репрезентации.
Современные данные также подтверждают, что знания не сводятся к необработанным ощущениям. Би (2021) в обзоре отмечает концепцию двойного кодирования знаний: существуют «воплощённые» (sensory embodiment) представления, основанные на сенсомоторном опыте, и параллельно – дисэмболизированные, языково-символьные системы знаний. Эксперименты на незрячих людях, изучавших свойства объектов без прямого сенсорного опыта, показали значимую роль лингвистических/когнитивных представлений (десенсоризованная информация) в сопровождении с сенсорной формой. Таким образом, знание включает и абстрактные понятия, помимо чистых ощущений.
Pinotsis et al. (2019) на модельных животных прямо сравнивали нейронные представления сенсорного распознавания и категоризации. Они обнаружили, что одни зоны мозга при обработке цвета преимущественно «реагируют» на чистые сенсорные признаки, а при решении задач на отнесение к категории («куда относить» объекты по движению) задействуются более абстрактные вычисления. Это говорит о том, что при формировании знаний мозг гибко переключается между сенсорной обработкой и категоризацией.



