{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57d6c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "from gpt4all import GPT4All\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ahocorasick\n",
    "from collections import defaultdict, deque, Counter\n",
    "from itertools import combinations\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f77ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEnhancedTextProcessor:\n",
    "    def __init__(self, gemini_api_keys: List[str], model_name: str = \"gemini-2.0-flash\"):\n",
    "        self.gemini_api_keys = gemini_api_keys\n",
    "        self.current_key_index = 0\n",
    "        self.key_usage_count = {key: 0 for key in gemini_api_keys}\n",
    "        self.key_last_used = {key: 0 for key in gemini_api_keys}\n",
    "        self.model_name = model_name\n",
    "        self._setup_gemini_client()\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "        except OSError:\n",
    "            raise RuntimeError(\"Требуется установка: python -m spacy download ru_core_news_sm\")\n",
    "        self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.formula_patterns = [\n",
    "            r\"[yY]['’ʹ′‵‶‶‷]?\\s*=\\s*f[ₙ₀-₉]*\\s*\\([^()]*(?:\\([^()]*(?:\\([^()]*(?:\\([^()]*\\)[^()]*)*\\)[^()]*)*\\)[^()]*)*\\)(?:[WB][₀-₉ₙₖ]*\\s*[+\\-]\\s*[WB][₀-₉ₖ]*)*\", \n",
    "            r\"[EεΣ]\\s*=\\s*\\([^)]+(?:\\([^)]*\\)[^)]*)*\\)\\s*/\\s*[nmδ]\", \n",
    "            r\"h_[t]\\s*=\\s*f\\([^)]+\\)\", \n",
    "            r\"[εδ²]+\\s*[<≤]\\s*(?:\\[[^\\]]+\\]|[^,\\n.; ]+)\\s*/\\s*[m]\", \n",
    "            r\"\\|[^|]+\\|\\s*[≈=]\\s*2\\^\\{?[^}\\n.,;]+\\}?\", \n",
    "            r\"(?:\\b[a-zA-Zα-ωΑ-Ωεδ]['’ʹ′‵‶‶‷]*(?:[₀-₉ₙₖₜᵢⱼₓ]|['’ʹ′‵‶‶‷]\\s*)*\\b(?:\\[[^\\]]+\\])?|\\b[a-zA-Zα-ωΑ-Ωεδ][₀-₉ₙₖₜᵢⱼₓ]*\\s*\\([^)]+\\)|\\([^)]+\\))\\s*[=≈<>≤≥]\\s*(?:[^,\\n.; ]|FORMULA_\\d+__){1,150}\",\n",
    "            r\"\\b[a-zA-Zα-ωΑ-Ωεδ][₀-₉ₙₖₜᵢⱼₓ]*\\s*\\([^)]+\\)\",\n",
    "            r\"\\b[WB][₀-₉ₙₖ]*\\s*[+\\-*/]\\s*[WBηДЕЛа-яА-Яα-ωΑ-Ωεδ][₀-₉ₙₖ]*\", \n",
    "            r\"\\b[a-zA-Zα-ωΑ-Ωεδ]['’ʹ′‵‶‶‷]*(?:[₀-₉ₙₖₜᵢⱼₓ]|['’ʹ′‵‶‶‷]\\s*)+(?:\\[[^\\]]+\\])?\\b\", \n",
    "            r\"\\([a-zA-Zα-ωΑ-Ωεδ]['’ʹ′‵‶‶‷]*[₀-₉ₙₖₜᵢⱼₓ]*\\)\",\n",
    "            r\"\\b[a-zA-Zα-ωΑ-Ωεδ]\\s*['’ʹ′‵‶‶‷]\\s*-\\s*[a-zA-Zα-ωΑ-Ωεδ]['’ʹ′‵‶‶‷]*\\b\", \n",
    "            r\"\\b\\d+\\^[a-zA-Zα-ωΑ-Ωεδ(][^)\\s]*\\)?\", \n",
    "            r\"\\b[a-zA-Z0-9₀-₉ₙₖₜᵢⱼₓ chronically]+\\s*(?:×|\\*)\\s*[a-zA-Z0-9₀-₉ₙₖₜᵢⱼₓ chronically]+\\b\", \n",
    "            r\"\\b[I]\\([^)]+;[^)]+\\)\\s*/\\s*[I]\\([^)]+;[^)]+\\)\",\n",
    "            r\"\\b[I]\\s*\\([^)]+;\\s*[^)]+\\)\\s*=\\s*(?:[^,\\n.;()]|\\([^)]*\\)|__MATH_FORMULA_\\d+__)+\",\n",
    "        ]\n",
    "        self.symbol_normalize = {'—': '-', '–': '-', '«': '\"', '»': '\"'}\n",
    "        self.protected_terms = {\n",
    "            'weights', 'biases', 'loss function', 'gradient descent',\n",
    "            'neural network', 'activation function', 'backpropagation',\n",
    "            'dropout', 'batch normalization', 'transformer', 'attention',\n",
    "            'deep learning', 'machine learning', 'cross-validation'\n",
    "        }\n",
    "        self.term_synonyms = {}\n",
    "        self.abbreviations = {}\n",
    "        self.context_rules = {}\n",
    "        self.ac_automaton = None\n",
    "        self.automation_pairs = []\n",
    "        self.chunk_size = 2000\n",
    "        self.rate_limit_delay = 1.0\n",
    "        self._gemini_cache = {}\n",
    "        self.processed_segments = set()\n",
    "        self.formula_counter = 0\n",
    "        self.formula_names = {}\n",
    "        \n",
    "    def _setup_gemini_client(self):\n",
    "        current_key = self.gemini_api_keys[self.current_key_index]\n",
    "        genai.configure(api_key=current_key)\n",
    "        self.gemini_model = genai.GenerativeModel(self.model_name)\n",
    "\n",
    "    def _rotate_api_key(self):\n",
    "        self.current_key_index = (self.current_key_index + 1) % len(self.gemini_api_keys)\n",
    "        self._setup_gemini_client()\n",
    "        print(f\"Переключился на API ключ #{self.current_key_index + 1}\")\n",
    "\n",
    "    def smart_text_chunking(self, text: str) -> List[str]:\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()] \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) + 1 < self.chunk_size:\n",
    "                current_chunk += (\" \" if current_chunk else \"\") + sentence\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return chunks\n",
    "    \n",
    "    def _extract_and_protect_formulas(self, text: str) -> Tuple[str, List[str], Dict[str, str]]:\n",
    "        processed_text = re.sub(r'подчеркивание+[A-Za-z]*|FORMU[а-я]*A[₀-₉]*', '', text)\n",
    "        all_matches = []\n",
    "        combined_pattern = '|'.join(f'({pattern})' for pattern in self.formula_patterns)\n",
    "        for match in re.finditer(combined_pattern, processed_text, re.IGNORECASE | re.UNICODE):\n",
    "            formula = match.group().strip()\n",
    "            if self._is_valid_formula(formula):\n",
    "                all_matches.append((match.start(), match.end(), formula))\n",
    "        if not all_matches:\n",
    "            return processed_text, [], {}\n",
    "        all_matches.sort(key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "        final_matches = []\n",
    "        last_end = -1\n",
    "        for start, end, formula in all_matches:\n",
    "            if start >= last_end:  \n",
    "                final_matches.append((start, end, formula))\n",
    "                last_end = end\n",
    "        final_matches.sort(key=lambda x: x[0], reverse=True)\n",
    "        formulas = []\n",
    "        formula_placeholders = {}\n",
    "        for start, end, formula in final_matches:\n",
    "            self.formula_counter += 1\n",
    "            placeholder = f\"__MATH_FORMULA_{self.formula_counter}__\"\n",
    "            formula_dict = {\n",
    "                'placeholder': placeholder,\n",
    "                'formula': formula\n",
    "            }\n",
    "            formulas.append(formula_dict)\n",
    "            formula_placeholders[placeholder] = formula\n",
    "            processed_text = processed_text[:start] + placeholder + processed_text[end:]\n",
    "        return processed_text, formulas[::-1], formula_placeholders\n",
    "    \n",
    "    def _is_valid_formula(self, text: str) -> bool:\n",
    "        \"\"\"Проверка валидности математической формулы.\"\"\"\n",
    "        if len(text) < 2:\n",
    "            return False\n",
    "        math_indicators = [\n",
    "            r'[=≈<>≤≥]',  \n",
    "            r'[₀-₉ₙₖₜ]',  \n",
    "            r'[∑∏∫∂∇]',   \n",
    "            r'f[₀-₉ₙ]*\\s*\\(', \n",
    "            r'[+\\-*/^]',   \n",
    "            r'[α-ωΑ-Ωεδ]', \n",
    "            r'[WBhxy]_[₀-₉ₙₖₜ]', \n",
    "            r'\\^[₀-₉{}\\[\\]]', \n",
    "            r'\\|[^|]+\\|',\n",
    "            r'[₀-₁₀ₙₖₜᵢⱼₓ chronically]', \n",
    "            r\"['’ʹ′‵‶‶‷]\",        \n",
    "            r'[a-zA-Zα-ωΑ-Ωεδ]\\s*\\(', \n",
    "            r'[WBXYHV]\\s*_[^_\\s]+', \n",
    "            r'\\^[^{}\\[\\]\\s]+|\\^\\{[^{}]*\\}|\\^\\[[^\\]]*\\]', \n",
    "            r'[⇒→∈∀∃]',      \n",
    "            r'\\d+\\.\\d+',      \n",
    "            r'log|exp|sin|cos|tan|det|tr'   \n",
    "        ]\n",
    "        matches = sum(1 for pattern in math_indicators if re.search(pattern, text))\n",
    "        return matches >= 2\n",
    "\n",
    "    def _normalize_text_content(self, text: str) -> str:\n",
    "        text = re.sub(r'[₀-₉ₙₖₜ]+(?![A-Za-z])', '', text)  \n",
    "        text = re.sub(r'[α-ωΑ-Ωεδ](?![A-Za-z])', '', text)  \n",
    "        text = re.sub(r'[∑∏∫∂∇≈≤≥]', '', text) \n",
    "        for old, new in self.symbol_normalize.items():\n",
    "            text = text.replace(old, new)\n",
    "        return text\n",
    "\n",
    "    def _post_process_text(self, text: str) -> str:\n",
    "        \"\"\"Постобработка: убираем тире и приводим к нижнему регистру\"\"\"\n",
    "        formula_pattern = r'__MATH_FORMULA_\\d+__'\n",
    "        formulas_found = re.findall(formula_pattern, text)\n",
    "        temp_text = re.sub(formula_pattern, '<<<FORMULA>>>', text)\n",
    "        temp_text = temp_text.replace('-', ' ')\n",
    "        temp_text = temp_text.lower()\n",
    "        for formula in formulas_found:\n",
    "            temp_text = temp_text.replace('<<<formula>>>', formula, 1)\n",
    "        return temp_text\n",
    "\n",
    "    def _fix_word_boundaries(self, text: str) -> str:\n",
    "        text = re.sub(r'([а-яё])([А-ЯЁ])', r'\\1 \\2', text) \n",
    "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)    \n",
    "        text = re.sub(r'([а-яёa-z])([А-ЯЁA-Z][а-яёa-z])', r'\\1 \\2', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
    "        text = re.sub(r'([.!?])\\s*([А-ЯЁA-Z])', r'\\1 \\2', text)\n",
    "        text = re.sub(r'([а-яёa-z])([,.!?;:])([а-яёa-z])', r'\\1\\2 \\3', text)\n",
    "        text = re.sub(r'([а-яёa-z]{2,})([.!?])([А-ЯЁA-Z][а-яёa-z])', r'\\1\\2 \\3', text) \n",
    "        return text.strip()\n",
    "\n",
    "    def process_text_segment(self, text: str) -> str:\n",
    "        if not text or not text.strip():\n",
    "            return text\n",
    "        segment_hash = hash(text.strip())\n",
    "        if segment_hash in self.processed_segments:\n",
    "            return text\n",
    "        text = self._normalize_text_content(text)\n",
    "        if not (self.ac_automaton and self.automation_pairs):\n",
    "            text = self._fix_word_boundaries(text)\n",
    "            self.processed_segments.add(segment_hash)\n",
    "            return text\n",
    "        valid_matches = []\n",
    "        text_len = len(text) \n",
    "        try:\n",
    "            for end_index, pair_index in self.ac_automaton.iter(text):\n",
    "                if not (0 <= pair_index < len(self.automation_pairs)):\n",
    "                    continue\n",
    "                original, replacement = self.automation_pairs[pair_index]\n",
    "                original_len = len(original)\n",
    "                start_index = end_index - original_len + 1\n",
    "                if (0 <= start_index < text_len and \n",
    "                    end_index < text_len and\n",
    "                    text[start_index:end_index + 1] == original):\n",
    "                    if (self._safe_to_replace(text, start_index, end_index + 1, original) and\n",
    "                        self._is_word_boundary(text, start_index, end_index + 1)):\n",
    "                        valid_matches.append((start_index, end_index + 1, replacement))\n",
    "        except Exception:\n",
    "            valid_matches = []\n",
    "        if valid_matches:\n",
    "            valid_matches.sort(key=lambda x: x[0], reverse=True)\n",
    "            for start, end, replacement in valid_matches:\n",
    "                text = text[:start] + replacement + text[end:]\n",
    "        text = self._fix_word_boundaries(text)\n",
    "        self.processed_segments.add(segment_hash)\n",
    "        return text\n",
    "\n",
    "    def _safe_to_replace(self, text: str, start: int, end: int, original: str) -> bool:\n",
    "        context = text[max(0, start-10):min(len(text), end+10)].lower()\n",
    "        return not any(term in context for term in self.protected_terms)\n",
    "\n",
    "    def _is_word_boundary(self, text: str, start: int, end: int) -> bool:\n",
    "        before_ok = start == 0 or not text[start-1].isalnum()\n",
    "        after_ok = end >= len(text) or not text[end].isalnum()\n",
    "        return before_ok and after_ok\n",
    "\n",
    "    def build_automaton(self):\n",
    "        self.ac_automaton = ahocorasick.Automaton()\n",
    "        replacement_dict = {}\n",
    "        for source in [self.abbreviations, self.term_synonyms]:\n",
    "            for key, value in source.items():\n",
    "                if not self._is_protected_term(str(key)):\n",
    "                    if isinstance(value, list):\n",
    "                        replacement_dict[key] = ' '.join(value)\n",
    "                    else:\n",
    "                        replacement_dict[key] = str(value)\n",
    "        self.automation_pairs = []\n",
    "        for idx, (term, replacement) in enumerate(replacement_dict.items()):\n",
    "            term_str = str(term).strip()\n",
    "            if len(term_str) > 1 and term_str != replacement:\n",
    "                try:\n",
    "                    self.ac_automaton.add_word(term_str, idx)\n",
    "                    self.automation_pairs.append((term_str, replacement))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if self.automation_pairs:\n",
    "            self.ac_automaton.make_automaton()\n",
    "\n",
    "    def _is_protected_term(self, term: str) -> bool:\n",
    "        term_lower = term.lower()\n",
    "        return any(protected in term_lower or term_lower in protected \n",
    "                  for protected in self.protected_terms)\n",
    "\n",
    "    async def _call_gemini_with_retry(self, prompt: str, max_retries: int = 3) -> Dict:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                current_key = self.gemini_api_keys[self.current_key_index]\n",
    "                self.key_usage_count[current_key] += 1\n",
    "                self.key_last_used[current_key] = time.time()\n",
    "                response = self.gemini_model.generate_content(prompt)\n",
    "                json_text = response.text.strip()\n",
    "                json_match = re.search(r'\\{.*\\}', json_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    return json.loads(json_match.group())\n",
    "                else:\n",
    "                    return {}         \n",
    "            except Exception as e:\n",
    "                error_str = str(e).lower()\n",
    "                if 'quota' in error_str or 'limit' in error_str or 'rate' in error_str:\n",
    "                    print(f\"Лимит API ключа #{self.current_key_index + 1}, переключаемся...\")\n",
    "                    self._rotate_api_key()\n",
    "                    await asyncio.sleep(2)\n",
    "                elif attempt < max_retries - 1:\n",
    "                    await asyncio.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(f\"Ошибка Gemini после {max_retries} попыток: {e}\")\n",
    "                    return {}\n",
    "        return {}\n",
    "\n",
    "    def create_analysis_prompt(self, text_chunk: str) -> str:\n",
    "        return (\n",
    "            f\"Проанализируй научный текст по ML/нейросетям и верни JSON:\\n\\n\"\n",
    "            f\"ВАЖНО: НЕ разбивай термины 'weights', 'biases', 'loss function'!\\n\\n\"\n",
    "            f\"Найди ВСЕ математические формулы, уравнения, выражения включая:\\n\"\n",
    "            f\"- Функции: f(x), f_n(...), h_t = f(...)\\n\"\n",
    "            f\"- Уравнения: E = ..., I(...) = ..., y' = ...\\n\"\n",
    "            f\"- Неравенства: ε² < [...], |T| ≈ 2^5\\n\"\n",
    "            f\"- Выражения со спецсимволами: Σ, ∏, ∂, ∇, ≈, ≤, ≥\\n\\n\"\n",
    "            f\"Текст: {text_chunk}\\n\\n\"\n",
    "            f\"JSON формат:\\n{{\\n\"\n",
    "            f'  \"formulas\": [\"полная формула 1\", \"полная формула 2\"],\\n'\n",
    "            f'  \"formula_patterns\": [\"паттерн1\", \"паттерн2\"],\\n'\n",
    "            f'  \"abbreviations\": {{\"аббр\": \"расшифровка\"}},\\n'\n",
    "            f'  \"synonyms\": {{\"термин\": [\"синоним1\", \"синоним2\"]}}\\n'\n",
    "            f\"}}\"\n",
    "        )\n",
    "\n",
    "    async def process_with_gemini(self, chunks: List[str]) -> Dict:\n",
    "        results = {\"formulas\": set(), \"formula_patterns\": set(), \"abbreviations\": {}, \"synonyms\": {}}\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            cache_key = f\"unified_{hash(chunk[:100])}\"\n",
    "            if cache_key in self._gemini_cache:\n",
    "                result = self._gemini_cache[cache_key]\n",
    "            else:\n",
    "                prompt = self.create_analysis_prompt(chunk)\n",
    "                result = await self._call_gemini_with_retry(prompt)\n",
    "                self._gemini_cache[cache_key] = result\n",
    "            self._merge_results(results, result)\n",
    "            if i < len(chunks) - 1:\n",
    "                await asyncio.sleep(self.rate_limit_delay)\n",
    "        return results\n",
    "\n",
    "    def _merge_results(self, target: Dict, source: Dict):\n",
    "        if \"formulas\" in source:\n",
    "            if isinstance(source[\"formulas\"], (list, set)):\n",
    "                target[\"formulas\"].update(source[\"formulas\"])\n",
    "        if \"formula_patterns\" in source:\n",
    "            if isinstance(source[\"formula_patterns\"], (list, set)):\n",
    "                target[\"formula_patterns\"].update(source[\"formula_patterns\"])\n",
    "        for key in [\"abbreviations\", \"synonyms\"]:\n",
    "            if key in source and isinstance(source[key], dict):\n",
    "                filtered = {k: v for k, v in source[key].items() \n",
    "                           if not self._is_protected_term(str(k))}\n",
    "                target[key].update(filtered)\n",
    "\n",
    "    def apply_enhancements(self, enhancements: Dict):\n",
    "        if \"formulas\" in enhancements:\n",
    "            for formula in enhancements[\"formulas\"]:\n",
    "                if formula and len(str(formula).strip()) > 3:\n",
    "                    escaped_formula = re.escape(str(formula).strip())\n",
    "                    if escaped_formula not in self.formula_patterns:\n",
    "                        self.formula_patterns.append(escaped_formula)\n",
    "        if \"formula_patterns\" in enhancements:\n",
    "            for pattern in enhancements[\"formula_patterns\"]:\n",
    "                if pattern and pattern not in self.formula_patterns:\n",
    "                    try:\n",
    "                        re.compile(pattern)\n",
    "                        self.formula_patterns.append(pattern)\n",
    "                    except re.error:\n",
    "                        continue\n",
    "        self.abbreviations.update(enhancements.get(\"abbreviations\", {}))\n",
    "        self.term_synonyms.update(enhancements.get(\"synonyms\", {}))\n",
    "\n",
    "    def _thematic_segmentation(self, sentences: List[str]) -> List[List[str]]:\n",
    "        if len(sentences) < 2:\n",
    "            return [sentences]\n",
    "        try:\n",
    "            embeddings = self.sentence_model.encode(sentences)\n",
    "            similarities = cosine_similarity(embeddings[:-1], embeddings[1:])\n",
    "            segments = []\n",
    "            current_segment = [sentences[0]]\n",
    "            threshold = max(0.3, np.percentile(similarities.diagonal(), 40))\n",
    "            for i, sim in enumerate(similarities.diagonal()):\n",
    "                if sim > threshold:\n",
    "                    current_segment.append(sentences[i + 1])\n",
    "                else:\n",
    "                    segments.append(current_segment)\n",
    "                    current_segment = [sentences[i + 1]]\n",
    "            segments.append(current_segment)\n",
    "            return segments\n",
    "        except Exception:\n",
    "            return [sentences[i:i+4] for i in range(0, len(sentences), 4)]\n",
    "\n",
    "    async def process_text_enhanced(self, text: str, use_gemini: bool = True, n_workers: int = 2) -> Dict:\n",
    "        try:\n",
    "            self.processed_segments.clear()\n",
    "            word_freq = Counter(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "            if use_gemini:\n",
    "                chunks = self.smart_text_chunking(text)\n",
    "                gemini_results = await self.process_with_gemini(chunks)\n",
    "                self.apply_enhancements(gemini_results)\n",
    "            text_no_formulas, formulas, formula_placeholders = self._extract_and_protect_formulas(text)\n",
    "            self.build_automaton()\n",
    "            chunks = [text_no_formulas[i:i+1000] for i in range(0, len(text_no_formulas), 1000)]\n",
    "            with ThreadPoolExecutor(max_workers=min(n_workers, len(chunks))) as executor:\n",
    "                processed_chunks = list(executor.map(self.process_text_segment, chunks))\n",
    "            processed_text = ''.join(processed_chunks)\n",
    "            processed_text = self._fix_word_boundaries(processed_text)\n",
    "            processed_text = self._post_process_text(processed_text)\n",
    "            doc = self.nlp(processed_text)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "            unique_sentences = []\n",
    "            seen = set()\n",
    "            for sentence in sentences:\n",
    "                clean = re.sub(r'\\s+', ' ', sentence.lower().strip())\n",
    "                if clean not in seen and len(clean) > 5:\n",
    "                    unique_sentences.append(sentence)\n",
    "                    seen.add(clean)\n",
    "            segments = self._thematic_segmentation(unique_sentences)\n",
    "            return {\n",
    "                'processed_text': ' '.join(unique_sentences),\n",
    "                'formulas': formulas,\n",
    "                'formula_placeholders': formula_placeholders,  \n",
    "                'thematic_segments': segments,\n",
    "                'statistics': {\n",
    "                    'words_top': dict(word_freq.most_common(20)),\n",
    "                    'sentences_original': len(sentences),\n",
    "                    'sentences_unique': len(unique_sentences),\n",
    "                    'formulas_found': len(formulas),\n",
    "                    'gemini_formulas': len(gemini_results.get('formulas', [])) if use_gemini else 0,\n",
    "                    'total_patterns': len(self.formula_patterns),\n",
    "                    'protected_terms_count': len([t for t in self.protected_terms \n",
    "                                                if t in processed_text.lower()])\n",
    "                },\n",
    "                'gemini_analysis': {\n",
    "                    'found_formulas': list(gemini_results.get('formulas', [])) if use_gemini else [],\n",
    "                    'found_patterns': list(gemini_results.get('formula_patterns', [])) if use_gemini else [],\n",
    "                    'abbreviations': gemini_results.get('abbreviations', {}) if use_gemini else {},\n",
    "                    'synonyms': gemini_results.get('synonyms', {}) if use_gemini else {}\n",
    "                } if use_gemini else {},\n",
    "                'api_usage': dict(self.key_usage_count)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ошибка обработки: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e17e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    id: str\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str = \"\"\n",
    "    source_chunks: List[int] = None  \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.source_chunks is None:\n",
    "            self.source_chunks = []\n",
    "\n",
    "@dataclass  \n",
    "class Relation:\n",
    "    source: str\n",
    "    target: str\n",
    "    type: str\n",
    "    description: str = \"\"\n",
    "    source_chunks: List[int] = None  \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.source_chunks is None:\n",
    "            self.source_chunks = []\n",
    "\n",
    "class ImprovedKnowledgeExtractor:\n",
    "    def __init__(self, \n",
    "                 gemini_api_keys: List[str], \n",
    "                 generation_model_path: str, \n",
    "                 model_name: str = \"gemini-2.0-flash\", \n",
    "                 chunk_size: int = 5, \n",
    "                 max_sentences: int = 200,\n",
    "                 fallback_timeout: int = 30):\n",
    "        \"\"\"\n",
    "        Инициализация экстрактора знаний\n",
    "        \n",
    "        Args:\n",
    "            gemini_api_keys: Список API ключей для Gemini\n",
    "            generation_model_path: Путь к локальной модели\n",
    "            model_name: Название модели Gemini\n",
    "            chunk_size: Размер чанка в предложениях\n",
    "            max_sentences: Максимальное количество предложений для обработки\n",
    "            fallback_timeout: Таймаут для переключения на локальную LLM (секунды)\n",
    "        \"\"\"\n",
    "        self.gemini_api_keys = gemini_api_keys\n",
    "        self.generation_model_path = generation_model_path\n",
    "        self.model_name = model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_sentences = max_sentences\n",
    "        self.fallback_timeout = fallback_timeout\n",
    "        self._init_local_llm()\n",
    "        self.current_key_index = 0\n",
    "        self.key_usage_count = {key: 0 for key in gemini_api_keys}\n",
    "        self.key_last_used = {key: 0 for key in gemini_api_keys}\n",
    "        self.gemini_available = True\n",
    "        self._setup_gemini_client()\n",
    "        self.generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.1,  \n",
    "            top_p=0.2,\n",
    "            top_k=3,\n",
    "            max_output_tokens=2000,  \n",
    "            candidate_count=1\n",
    "        )\n",
    "        self.formula_pattern = re.compile(r'__MATH_FORMULA_(\\d+)__')\n",
    "        self._setup_prompts()\n",
    "        self._init_nltk()\n",
    "        \n",
    "    def _init_local_llm(self):\n",
    "        \"\"\"Инициализация локальной LLM\"\"\"\n",
    "        try:\n",
    "            self.local_llm = GPT4All(self.generation_model_path)\n",
    "            logger.info(\"Локальная LLM успешно инициализирована\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка инициализации локальной LLM: {e}\")\n",
    "            self.local_llm = None\n",
    "\n",
    "    def _init_nltk(self):\n",
    "        \"\"\"Инициализация NLTK для разделения предложений\"\"\"\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            logger.info(\"Скачиваем punkt tokenizer для NLTK...\")\n",
    "            nltk.download('punkt')\n",
    "\n",
    "    def _setup_gemini_client(self):\n",
    "        \"\"\"Настройка клиента Gemini\"\"\"\n",
    "        try:\n",
    "            current_key = self.gemini_api_keys[self.current_key_index]\n",
    "            genai.configure(api_key=current_key)\n",
    "            self.gemini_model = genai.GenerativeModel(self.model_name)\n",
    "            logger.info(f\"Gemini клиент настроен с ключом #{self.current_key_index + 1}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка настройки Gemini клиента: {e}\")\n",
    "            self.gemini_available = False\n",
    "\n",
    "    def _rotate_api_key(self):\n",
    "        \"\"\"Ротация API ключей\"\"\"\n",
    "        if len(self.gemini_api_keys) > 1:\n",
    "            self.current_key_index = (self.current_key_index + 1) % len(self.gemini_api_keys)\n",
    "            self._setup_gemini_client()\n",
    "            logger.info(f\"Переключился на API ключ #{self.current_key_index + 1}\")\n",
    "        else:\n",
    "            logger.warning(\"Только один API ключ доступен\")\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Настройка промптов для извлечения\"\"\"\n",
    "        self.entity_prompt = \"\"\"Внимательно проанализируй текст и найди все важные понятия, концепции, методы, алгоритмы, теории и другие объекты.\n",
    "\n",
    "ВНИМАНИЕ: В тексте могут встречаться заменители формул вида __MATH_FORMULA_N__, где N - номер формулы. Учитывай их при анализе как математические выражения.\n",
    "\n",
    "СТРОГО следуй формату ответа:\n",
    "```\n",
    "name: точное название объекта/концепта без скобок, короткое но информативное\n",
    "type: тип (tasks/methods/algorithms/data/theories/concepts/formulas/functions/entities/processes/mechanisms/systems/phenomena/tools/techniques/models/structures)\n",
    "description: краткое но информативное описание\n",
    "---\n",
    "```\n",
    "\n",
    "Обязательно найди минимум 3-5 объектов в тексте. Не оставляй ответ пустым.\n",
    "\n",
    "Текст для анализа:\n",
    "{text}\n",
    "\n",
    "Извлеченные объекты:\"\"\"\n",
    "\n",
    "        self.relation_prompt = \"\"\"Найди ВСЕ связи между данными объектами в тексте. Ищи как прямые, так и косвенные связи.\n",
    "\n",
    "ВНИМАНИЕ: В тексте могут встречаться заменители формул вида __MATH_FORMULA_N__. Учитывай их при поиске связей.\n",
    "\n",
    "Известные объекты: {entities}\n",
    "\n",
    "СТРОГО следуй формату ответа:\n",
    "```\n",
    "source: точное название первого объекта\n",
    "target: точное название второго объекта\n",
    "type: тип связи (defines/subclass-of/instance-of/superclass-of/equals/same-as/connected-to/interacts-with/part-of/used-for/enables/causes/prevents/contains/requires/produces/controls/creates/generates/before/after/during/co-occurs-with/represents/implements/applies/based-on/derived-from/similar-to)\n",
    "description: подробное описание связи\n",
    "---\n",
    "```\n",
    "\n",
    "Найди минимум 2-3 связи, если объекты упоминаются в тексте.\n",
    "\n",
    "Текст для анализа связей:\n",
    "{text}\n",
    "\n",
    "Найденные связи:\"\"\"\n",
    "\n",
    "    # def split_text_into_sentences(self, text: str) -> List[str]:\n",
    "    #     \"\"\"\n",
    "    #     Автоматическое разделение текста на предложения с обработкой формул\n",
    "        \n",
    "    #     Args:\n",
    "    #         text: Входной текст\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Список предложений\n",
    "    #     \"\"\"\n",
    "    #     formula_placeholders = {}\n",
    "    #     formula_matches = list(self.formula_pattern.finditer(text))\n",
    "        \n",
    "    #     for match in formula_matches:\n",
    "    #         placeholder = f\"FORMULA_PLACEHOLDER_{match.group(1)}\"\n",
    "    #         formula_placeholders[placeholder] = match.group(0)\n",
    "    #         text = text.replace(match.group(0), placeholder)\n",
    "    #     try:\n",
    "    #         sentences = sent_tokenize(text, language='russian')\n",
    "    #     except:\n",
    "    #         sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    #     processed_sentences = []\n",
    "    #     for sentence in sentences:\n",
    "    #         for placeholder, original in formula_placeholders.items():\n",
    "    #             sentence = sentence.replace(placeholder, original)\n",
    "    #         if len(sentence.strip()) > 10:\n",
    "    #             processed_sentences.append(sentence.strip())\n",
    "    #     logger.info(f\"Текст разделен на {len(processed_sentences)} предложений\")\n",
    "    #     return processed_sentences\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> List[str]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        formula_placeholders = {}\n",
    "        protected_text = text\n",
    "        formula_matches = list(self.formula_pattern.finditer(text))\n",
    "        if formula_matches:\n",
    "            formula_matches.sort(key=lambda m: m.start(), reverse=True)  \n",
    "            for match in formula_matches:\n",
    "                placeholder = f\"FORMULA_PLACEHOLDER_{match.group(1)}\"\n",
    "                formula_placeholders[placeholder] = match.group(0)\n",
    "                start, end = match.span()\n",
    "                protected_text = protected_text[:start] + placeholder + protected_text[end:]\n",
    "        try:\n",
    "            sentences = sent_tokenize(protected_text, language='russian')\n",
    "        except Exception:\n",
    "            sentences = [s.strip() for s in protected_text.split('.') if s.strip()]\n",
    "        if not formula_placeholders:\n",
    "            processed_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "            logger.info(f\"Текст разделен на {len(processed_sentences)} предложений\")\n",
    "            return processed_sentences\n",
    "        placeholder_pattern = '|'.join(re.escape(placeholder) for placeholder in formula_placeholders.keys())\n",
    "        def replace_placeholder(match):\n",
    "            return formula_placeholders[match.group(0)]\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) <= 10:\n",
    "                continue\n",
    "            if any(placeholder in sentence for placeholder in formula_placeholders):\n",
    "                restored_sentence = re.sub(placeholder_pattern, replace_placeholder, sentence)\n",
    "            else:\n",
    "                restored_sentence = sentence\n",
    "            processed_sentences.append(restored_sentence.strip())\n",
    "        logger.info(f\"Текст разделен на {len(processed_sentences)} предложений\")\n",
    "        return processed_sentences\n",
    "\n",
    "    def _preprocess_text_chunk(self, chunk_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Предобработка текста чанка\n",
    "        \n",
    "        Args:\n",
    "            chunk_text: Текст чанка\n",
    "            \n",
    "        Returns:\n",
    "            Обработанный текст\n",
    "        \"\"\"\n",
    "        def replace_formula(match):\n",
    "            formula_num = match.group(1)\n",
    "            return f\"математическая формула номер {formula_num}\"\n",
    "        processed_text = self.formula_pattern.sub(replace_formula, chunk_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text.strip()\n",
    "        return processed_text\n",
    "\n",
    "    def _create_chunks(self, sentences: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Создает чанки из предложений с оптимизацией для больших текстов\n",
    "        \n",
    "        Args:\n",
    "            sentences: Список предложений\n",
    "            \n",
    "        Returns:\n",
    "            Список чанков\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), self.chunk_size):\n",
    "            chunk_sentences = sentences[i:i + self.chunk_size]\n",
    "            chunk_text = ' '.join(chunk_sentences)\n",
    "            processed_text = self._preprocess_text_chunk(chunk_text)\n",
    "            if len(processed_text) > 2000:\n",
    "                processed_text = processed_text[:2000] + \"...\"\n",
    "            chunks.append({\n",
    "                'id': i // self.chunk_size,\n",
    "                'text': processed_text,\n",
    "                'original_text': chunk_text,\n",
    "                'sentences': chunk_sentences,\n",
    "                'start_idx': i,\n",
    "                'end_idx': min(i + self.chunk_size - 1, len(sentences) - 1)\n",
    "            })\n",
    "        logger.info(f\"Создано {len(chunks)} чанков\")\n",
    "        return chunks\n",
    "\n",
    "    async def _call_gemini_api(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Вызов Gemini API с обработкой ошибок и fallback на локальную LLM\n",
    "        \n",
    "        Args:\n",
    "            prompt: Промпт для модели\n",
    "            max_retries: Максимальное количество попыток\n",
    "            \n",
    "        Returns:\n",
    "            Ответ модели или None\n",
    "        \"\"\"\n",
    "        if not self.gemini_available:\n",
    "            return await self._call_local_llm(prompt)\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                current_key = self.gemini_api_keys[self.current_key_index]\n",
    "                self.key_usage_count[current_key] += 1\n",
    "                self.key_last_used[current_key] = time.time()\n",
    "                response = await asyncio.wait_for(\n",
    "                    asyncio.to_thread(\n",
    "                        self.gemini_model.generate_content,\n",
    "                        prompt,\n",
    "                        generation_config=self.generation_config\n",
    "                    ),\n",
    "                    timeout=self.fallback_timeout\n",
    "                )\n",
    "                if response and response.text:\n",
    "                    logger.info(f\"Gemini ответил за {time.time() - start_time:.2f}s\")\n",
    "                    return response.text.strip()\n",
    "                else:\n",
    "                    logger.warning(\"Gemini вернул пустой ответ\")  \n",
    "            except asyncio.TimeoutError:\n",
    "                logger.warning(f\"Gemini превысил таймаут {self.fallback_timeout}s, переключаемся на локальную LLM\")\n",
    "                return await self._call_local_llm(prompt)\n",
    "            except Exception as e:\n",
    "                error_str = str(e).lower()\n",
    "                if any(keyword in error_str for keyword in ['quota', 'limit', 'rate']):\n",
    "                    logger.warning(f\"Лимит API ключа #{self.current_key_index + 1}, переключаемся...\")\n",
    "                    self._rotate_api_key()\n",
    "                    if attempt == max_retries - 1:\n",
    "                        logger.warning(\"Все API ключи исчерпаны, переключаемся на локальную LLM\")\n",
    "                        return await self._call_local_llm(prompt)\n",
    "                else:\n",
    "                    logger.error(f\"Ошибка Gemini на попытке {attempt + 1}: {e}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        logger.warning(\"Все попытки Gemini исчерпаны, переключаемся на локальную LLM\")\n",
    "                        return await self._call_local_llm(prompt)\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "        return await self._call_local_llm(prompt)\n",
    "\n",
    "    async def _call_local_llm(self, prompt: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Вызов локальной LLM\n",
    "        \n",
    "        Args:\n",
    "            prompt: Промпт для модели\n",
    "            \n",
    "        Returns:\n",
    "            Ответ модели или None\n",
    "        \"\"\"\n",
    "        if not self.local_llm:\n",
    "            logger.error(\"Локальная LLM недоступна\")\n",
    "            return None\n",
    "        try:\n",
    "            logger.info(\"Используем локальную LLM\")\n",
    "            response = await asyncio.to_thread(\n",
    "                self.local_llm.generate,\n",
    "                prompt,\n",
    "                max_tokens=1000,\n",
    "                temp=0.1\n",
    "            )\n",
    "            return response.strip() if response else None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка локальной LLM: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_entity_response(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Парсинг ответа с сущностями\n",
    "        \n",
    "        Args:\n",
    "            response: Ответ модели\n",
    "            \n",
    "        Returns:\n",
    "            Список сущностей\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Парсинг ответа сущностей: {response[:200]}...\")\n",
    "        if not response or len(response.strip()) < 10:\n",
    "            logger.warning(\"Получен слишком короткий ответ\")\n",
    "            return []\n",
    "        entities = []\n",
    "        if '---' in response:\n",
    "            entity_blocks = response.split('---')\n",
    "            for block in entity_blocks:\n",
    "                entity = self._parse_entity_block(block.strip())\n",
    "                if entity:\n",
    "                    entities.append(entity)\n",
    "        else:\n",
    "            entities = self._parse_entities_line_by_line(response)\n",
    "        logger.info(f\"Распарсено {len(entities)} сущностей\")\n",
    "        if len(entities) == 0:\n",
    "            logger.warning(f\"Не удалось распарсить сущности из ответа: {response[:500]}\")\n",
    "        return entities\n",
    "    \n",
    "    def _parse_entity_block(self, block: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Парсинг блока сущности\n",
    "        \n",
    "        Args:\n",
    "            block: Блок текста с сущностью\n",
    "            \n",
    "        Returns:\n",
    "            Словарь сущности или None\n",
    "        \"\"\"\n",
    "        if not block:\n",
    "            return None\n",
    "        entity = {}\n",
    "        lines = [line.strip() for line in block.split('\\n') if line.strip()]\n",
    "        for line in lines:\n",
    "            if ':' not in line:\n",
    "                continue\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.lower().strip()\n",
    "            value = value.strip()\n",
    "            if not value:\n",
    "                continue\n",
    "            if key in ['name', 'название', 'понятие']:\n",
    "                entity['name'] = value\n",
    "            elif key in ['type', 'тип']:\n",
    "                entity['type'] = value\n",
    "            elif key in ['description', 'описание']:\n",
    "                entity['description'] = value\n",
    "        if 'name' not in entity:\n",
    "            return None\n",
    "        if 'type' not in entity:\n",
    "            entity['type'] = 'concepts'\n",
    "        if 'description' not in entity:\n",
    "            entity['description'] = ''\n",
    "        return entity\n",
    "\n",
    "    def _parse_entities_line_by_line(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Построчный парсинг сущностей (fallback метод)\n",
    "        \n",
    "        Args:\n",
    "            response: Ответ модели\n",
    "            \n",
    "        Returns:\n",
    "            Список сущностей\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        current_entity = {}\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#') or line.startswith('*'):\n",
    "                continue\n",
    "            if any(line.lower().startswith(prefix) for prefix in ['name:', 'название:', 'понятие:']):\n",
    "                if current_entity and 'name' in current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                name = line.split(':', 1)[1].strip() if ':' in line else line.strip()\n",
    "                if name:\n",
    "                    current_entity = {'name': name}\n",
    "            elif any(line.lower().startswith(prefix) for prefix in ['type:', 'тип:']):\n",
    "                if current_entity and ':' in line:\n",
    "                    entity_type = line.split(':', 1)[1].strip()\n",
    "                    if entity_type:\n",
    "                        current_entity['type'] = entity_type\n",
    "            elif any(line.lower().startswith(prefix) for prefix in ['description:', 'описание:']):\n",
    "                if current_entity and ':' in line:\n",
    "                    description = line.split(':', 1)[1].strip()\n",
    "                    if description:\n",
    "                        current_entity['description'] = description\n",
    "        if current_entity and 'name' in current_entity:\n",
    "            entities.append(current_entity)\n",
    "        for entity in entities:\n",
    "            if 'type' not in entity:\n",
    "                entity['type'] = 'concepts'\n",
    "            if 'description' not in entity:\n",
    "                entity['description'] = ''\n",
    "        return entities\n",
    "\n",
    "    def _parse_relation_response(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Улучшенный парсинг ответа с отношениями\n",
    "        \n",
    "        Args:\n",
    "            response: Ответ модели\n",
    "            \n",
    "        Returns:\n",
    "            Список отношений\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Парсинг ответа отношений: {response[:200]}...\")\n",
    "        if not response or len(response.strip()) < 10:\n",
    "            logger.warning(\"Получен слишком короткий ответ для отношений\")\n",
    "            return []\n",
    "        relations = []\n",
    "        if '---' in response:\n",
    "            relation_blocks = response.split('---')\n",
    "            for block in relation_blocks:\n",
    "                relation = self._parse_relation_block(block.strip())\n",
    "                if relation:\n",
    "                    relations.append(relation)\n",
    "        else:\n",
    "            relations = self._parse_relations_line_by_line(response)\n",
    "        logger.info(f\"Распарсено {len(relations)} отношений\")\n",
    "        if len(relations) == 0:\n",
    "            logger.warning(f\"Не удалось распарсить отношения из ответа: {response[:500]}\")\n",
    "        return relations\n",
    "\n",
    "    def _parse_relation_block(self, block: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Парсинг блока отношения\n",
    "        \n",
    "        Args:\n",
    "            block: Блок текста с отношением\n",
    "            \n",
    "        Returns:\n",
    "            Словарь отношения или None\n",
    "        \"\"\"\n",
    "        if not block:\n",
    "            return None\n",
    "        relation = {}\n",
    "        lines = [line.strip() for line in block.split('\\n') if line.strip()]\n",
    "        for line in lines:\n",
    "            if ':' not in line:\n",
    "                continue\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.lower().strip()\n",
    "            value = value.strip()\n",
    "            if not value:\n",
    "                continue\n",
    "            if key in ['source', 'источник']:\n",
    "                relation['source'] = value\n",
    "            elif key in ['target', 'цель']:\n",
    "                relation['target'] = value\n",
    "            elif key in ['type', 'тип']:\n",
    "                relation['type'] = value\n",
    "            elif key in ['description', 'описание']:\n",
    "                relation['description'] = value\n",
    "        if 'source' not in relation or 'target' not in relation:\n",
    "            return None\n",
    "        if 'type' not in relation:\n",
    "            relation['type'] = 'connected-to'\n",
    "        if 'description' not in relation:\n",
    "            relation['description'] = ''\n",
    "        return relation\n",
    "\n",
    "    def _parse_relations_line_by_line(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Построчный парсинг отношений (fallback метод)\n",
    "        \n",
    "        Args:\n",
    "            response: Ответ модели\n",
    "            \n",
    "        Returns:\n",
    "            Список отношений\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        current_relation = {}\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#') or line.startswith('*'):\n",
    "                continue  \n",
    "            if any(line.lower().startswith(prefix) for prefix in ['source:', 'источник:']):\n",
    "                if current_relation and 'source' in current_relation:\n",
    "                    relations.append(current_relation)\n",
    "                source = line.split(':', 1)[1].strip() if ':' in line else line.strip()\n",
    "                if source:\n",
    "                    current_relation = {'source': source}\n",
    "            elif any(line.lower().startswith(prefix) for prefix in ['target:', 'цель:']):\n",
    "                if current_relation and ':' in line:\n",
    "                    target = line.split(':', 1)[1].strip()\n",
    "                    if target:\n",
    "                        current_relation['target'] = target\n",
    "            elif any(line.lower().startswith(prefix) for prefix in ['type:', 'тип:']):\n",
    "                if current_relation and ':' in line:\n",
    "                    relation_type = line.split(':', 1)[1].strip()\n",
    "                    if relation_type:\n",
    "                        current_relation['type'] = relation_type\n",
    "            elif any(line.lower().startswith(prefix) for prefix in ['description:', 'описание:']):\n",
    "                if current_relation and ':' in line:\n",
    "                    description = line.split(':', 1)[1].strip()\n",
    "                    if description:\n",
    "                        current_relation['description'] = description\n",
    "        if current_relation and 'source' in current_relation:\n",
    "            relations.append(current_relation)\n",
    "        for relation in relations:\n",
    "            if 'type' not in relation:\n",
    "                relation['type'] = 'connected-to'\n",
    "            if 'description' not in relation:\n",
    "                relation['description'] = ''\n",
    "        return relations\n",
    "\n",
    "    async def _extract_with_llm(self, chunk: Dict, prompt_template: str, entities_list: List[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Извлечение данных через LLM (Gemini или локальную)\n",
    "        \n",
    "        Args:\n",
    "            chunk: Чанк для обработки\n",
    "            prompt_template: Шаблон промпта\n",
    "            entities_list: Список сущностей (для отношений)\n",
    "            \n",
    "        Returns:\n",
    "            Список извлеченных данных\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if entities_list:\n",
    "                entities_str = ', '.join(entities_list)\n",
    "                full_prompt = prompt_template.format(text=chunk['text'], entities=entities_str)\n",
    "            else:\n",
    "                full_prompt = prompt_template.format(text=chunk['text'])\n",
    "            logger.info(f\"Обрабатываю чанк {chunk['id']}: {chunk['text'][:50]}...\")\n",
    "            response = await self._call_gemini_api(full_prompt)\n",
    "            if not response:\n",
    "                logger.warning(f\"Пустой ответ для чанка {chunk['id']}\")\n",
    "                return []\n",
    "            if entities_list:\n",
    "                parsed_data = self._parse_relation_response(response)\n",
    "            else:\n",
    "                parsed_data = self._parse_entity_response(response)\n",
    "            if parsed_data:\n",
    "                logger.info(f\"Успешно извлечено {len(parsed_data)} элементов из чанка {chunk['id']}\")\n",
    "                return parsed_data\n",
    "            else:\n",
    "                logger.warning(f\"Не удалось извлечь данные из чанка {chunk['id']}\")\n",
    "                logger.debug(f\"Ответ от LLM: {response}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка обработки чанка {chunk['id']}: {str(e)}\")\n",
    "            logger.debug(f\"Подробности ошибки:\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    # async def _extract_entities_from_chunks(self, chunks: List[Dict]) -> List[Entity]:\n",
    "    #     \"\"\"\n",
    "    #     Извлечение сущностей из чанков\n",
    "        \n",
    "    #     Args:\n",
    "    #         chunks: Список чанков\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Список сущностей\n",
    "    #     \"\"\"\n",
    "    #     logger.info(\"=== ЭТАП 1: Извлечение сущностей ===\")\n",
    "    #     all_entities = []\n",
    "        \n",
    "    #     for chunk in chunks:\n",
    "    #         logger.info(f\"Обрабатываю чанк {chunk['id'] + 1}/{len(chunks)}\")\n",
    "    #         try:\n",
    "    #             entities_data = await self._extract_with_llm(chunk, self.entity_prompt)\n",
    "    #             chunk_entities = []\n",
    "    #             for i, entity_data in enumerate(entities_data):\n",
    "    #                 if 'name' in entity_data and entity_data['name']:\n",
    "    #                     entity = Entity(\n",
    "    #                         id=f\"e_{chunk['id']}_{i}\",\n",
    "    #                         name=entity_data['name'].strip(),\n",
    "    #                         type=entity_data.get('type', 'concepts').strip(),\n",
    "    #                         description=entity_data.get('description', '').strip(),\n",
    "    #                         source_chunks=[chunk['id']]\n",
    "    #                     )\n",
    "    #                     chunk_entities.append(entity)\n",
    "    #             all_entities.extend(chunk_entities)\n",
    "    #             logger.info(f\"Найдено сущностей в чанке: {len(chunk_entities)}\")\n",
    "    #             await asyncio.sleep(1)\n",
    "    #         except Exception as e:\n",
    "    #             logger.error(f\"Критическая ошибка при обработке энтити в чанке {chunk['id']}: {str(e)}\")\n",
    "    #             continue\n",
    "    #     unique_entities = self._merge_duplicate_entities(all_entities)\n",
    "    #     logger.info(f\"Общее количество уникальных энтити: {len(unique_entities)}\")\n",
    "    #     return unique_entities\n",
    "\n",
    "    # async def _extract_relations_from_chunks(self, chunks: List[Dict], entities: List[Entity]) -> List[Relation]:\n",
    "    #     \"\"\"\n",
    "    #     Извлечение отношений из чанков\n",
    "        \n",
    "    #     Args:\n",
    "    #         chunks: Список чанков\n",
    "    #         entities: Список сущностей\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Список отношений\n",
    "    #     \"\"\"\n",
    "    #     logger.info(\"=== ЭТАП 2: Извлечение отношений ===\")\n",
    "    #     entity_names = [entity.name for entity in entities]\n",
    "    #     entity_name_to_obj = {entity.name: entity for entity in entities}\n",
    "    #     all_relations = []\n",
    "    #     for chunk in chunks:\n",
    "    #         logger.info(f\"Обрабатываю отношения в чанке {chunk['id'] + 1}/{len(chunks)}\")\n",
    "    #         try:\n",
    "    #             relations_data = await self._extract_with_llm(chunk, self.relation_prompt, entity_names)\n",
    "    #             chunk_relations = []\n",
    "                    \n",
    "    #             for i, relation_data in enumerate(relations_data):\n",
    "    #                 if ('source' in relation_data and 'target' in relation_data and \n",
    "    #                     relation_data['source'] and relation_data['target']):\n",
    "                            \n",
    "    #                     source_name = relation_data['source'].strip()\n",
    "    #                     target_name = relation_data['target'].strip()\n",
    "                            \n",
    "    #                     if source_name in entity_name_to_obj and target_name in entity_name_to_obj:\n",
    "    #                         relation = Relation(\n",
    "    #                                 source=source_name,\n",
    "    #                                 target=target_name,\n",
    "    #                                 type=relation_data.get('type', 'connected-to').strip(),\n",
    "    #                                 description=relation_data.get('description', '').strip(),\n",
    "    #                                 source_chunks=[chunk['id']]\n",
    "    #                             )\n",
    "    #                         chunk_relations.append(relation)\n",
    "    #                     else:\n",
    "    #                         logger.debug(f\"Пропускаю отношение {source_name} -> {target_name}: сущности не найдены\")\n",
    "                    \n",
    "    #             all_relations.extend(chunk_relations)\n",
    "    #             logger.info(f\"Найдено отношений в чанке: {len(chunk_relations)}\")\n",
    "    #             await asyncio.sleep(1)\n",
    "    #         except Exception as e:\n",
    "    #             logger.error(f\"Критическая ошибка при обработке отношения в чанке {chunk['id']}: {str(e)}\")\n",
    "    #             continue    \n",
    "    #     unique_relations = self._merge_duplicate_relations(all_relations)\n",
    "    #     logger.info(f\"Общее количество уникальных отношении: {len(unique_relations)}\")\n",
    "    #     return unique_relations \n",
    "\n",
    "    async def _extract_entities_from_chunks(self, chunks: List[Dict]) -> List[Entity]:\n",
    "        logger.info(\"=== ЭТАП 1: Извлечение сущностей ===\")\n",
    "        if not chunks:\n",
    "            logger.warning(\"Пустой список чанков\")\n",
    "            return []\n",
    "        all_entities = []\n",
    "        chunks_count = len(chunks)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = chunk['id']\n",
    "            logger.info(f\"Обрабатываю чанк {chunk_id + 1}/{chunks_count}\")\n",
    "            try:\n",
    "                entities_data = await self._extract_with_llm(chunk, self.entity_prompt)\n",
    "                chunk_entities = [\n",
    "                    Entity(\n",
    "                        id=f\"e_{chunk_id}_{j}\",\n",
    "                        name=entity_data['name'].strip(),\n",
    "                        type=entity_data.get('type', 'concepts').strip(),\n",
    "                        description=entity_data.get('description', '').strip(),\n",
    "                        source_chunks=[chunk_id]\n",
    "                    )\n",
    "                    for j, entity_data in enumerate(entities_data)\n",
    "                    if entity_data.get('name') and entity_data['name'].strip()\n",
    "                ]\n",
    "                all_entities.extend(chunk_entities)\n",
    "                logger.info(f\"Найдено сущностей в чанке: {len(chunk_entities)}\")\n",
    "                if (i + 1) % 5 == 0 or i == chunks_count - 1:\n",
    "                    await asyncio.sleep(1)      \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Критическая ошибка при обработке энтити в чанке {chunk_id}: {str(e)}\")\n",
    "                continue\n",
    "        unique_entities = self._merge_duplicate_entities(all_entities)\n",
    "        logger.info(f\"Общее количество уникальных энтити: {len(unique_entities)}\")\n",
    "        return unique_entities\n",
    "\n",
    "    async def _extract_relations_from_chunks(self, chunks: List[Dict], entities: List[Entity]) -> List[Relation]:\n",
    "        logger.info(\"=== ЭТАП 2: Извлечение отношений ===\")\n",
    "        if not chunks or not entities:\n",
    "            logger.warning(\"Пустой список чанков или сущностей\")\n",
    "            return []\n",
    "        entity_names_set = {entity.name for entity in entities}\n",
    "        all_relations = []\n",
    "        chunks_count = len(chunks)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = chunk['id']\n",
    "            logger.info(f\"Обрабатываю отношения в чанке {chunk_id + 1}/{chunks_count}\")\n",
    "            try:\n",
    "                relations_data = await self._extract_with_llm(\n",
    "                    chunk, self.relation_prompt, list(entity_names_set)\n",
    "                )\n",
    "                chunk_relations = []\n",
    "                for j, relation_data in enumerate(relations_data):\n",
    "                    if not (relation_data.get('source') and relation_data.get('target')):\n",
    "                        continue\n",
    "                    source_name = relation_data['source'].strip()\n",
    "                    target_name = relation_data['target'].strip()\n",
    "                    if source_name in entity_names_set and target_name in entity_names_set:\n",
    "                        relation = Relation(\n",
    "                            source=source_name,\n",
    "                            target=target_name,\n",
    "                            type=relation_data.get('type', 'connected-to').strip(),\n",
    "                            description=relation_data.get('description', '').strip(),\n",
    "                            source_chunks=[chunk_id]\n",
    "                        )\n",
    "                        chunk_relations.append(relation)\n",
    "                    else:\n",
    "                        logger.debug(f\"Пропускаю отношение {source_name} -> {target_name}: сущности не найдены\")\n",
    "                all_relations.extend(chunk_relations)\n",
    "                logger.info(f\"Найдено отношений в чанке: {len(chunk_relations)}\")\n",
    "                if (i + 1) % 5 == 0 or i == chunks_count - 1:\n",
    "                    await asyncio.sleep(1) \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Критическая ошибка при обработке отношения в чанке {chunk_id}: {str(e)}\")\n",
    "                continue\n",
    "        unique_relations = self._merge_duplicate_relations(all_relations)\n",
    "        logger.info(f\"Общее количество уникальных отношений: {len(unique_relations)}\")\n",
    "        return unique_relations\n",
    "    \n",
    "    def _merge_duplicate_entities(self, entities: List[Entity]) -> List[Entity]:\n",
    "        \"\"\"\n",
    "        Объединяет дублирующиеся сущности\n",
    "        \n",
    "        Args:\n",
    "            entities: Список сущностей\n",
    "            \n",
    "        Returns:\n",
    "            Список уникальных сущностей\n",
    "        \"\"\"\n",
    "        entity_groups = defaultdict(list)\n",
    "        for entity in entities:\n",
    "            key = self._normalize_entity_key(entity.name)\n",
    "            entity_groups[key].append(entity)\n",
    "        merged_entities = []\n",
    "        for name_key, group in entity_groups.items():\n",
    "            if len(group) == 1:\n",
    "                merged_entities.append(group[0])\n",
    "            else:\n",
    "                base_entity = group[0]\n",
    "                all_chunks = set()\n",
    "                descriptions = []\n",
    "                for entity in group:\n",
    "                    all_chunks.update(entity.source_chunks)\n",
    "                    if entity.description and entity.description not in descriptions:\n",
    "                        descriptions.append(entity.description)\n",
    "                base_entity.source_chunks = sorted(list(all_chunks))\n",
    "                base_entity.description = '; '.join(descriptions) if descriptions else base_entity.description\n",
    "                merged_entities.append(base_entity)\n",
    "        return merged_entities\n",
    "\n",
    "    def _merge_duplicate_relations(self, relations: List[Relation]) -> List[Relation]:\n",
    "        \"\"\"\n",
    "        Объединяет дублирующиеся отношения\n",
    "        \n",
    "        Args:\n",
    "            relations: Список отношений\n",
    "            \n",
    "        Returns:\n",
    "            Список уникальных отношений\n",
    "        \"\"\"\n",
    "        relation_groups = defaultdict(list)\n",
    "        for relation in relations:\n",
    "            key = f\"{self._normalize_entity_key(relation.source)}|{self._normalize_entity_key(relation.target)}|{relation.type.lower()}\"\n",
    "            relation_groups[key].append(relation)\n",
    "        merged_relations = []\n",
    "        for relation_key, group in relation_groups.items():\n",
    "            if len(group) == 1:\n",
    "                merged_relations.append(group[0])\n",
    "            else:\n",
    "                base_relation = group[0]\n",
    "                all_chunks = set()\n",
    "                descriptions = []\n",
    "                for relation in group:\n",
    "                    all_chunks.update(relation.source_chunks)\n",
    "                    if relation.description and relation.description not in descriptions:\n",
    "                        descriptions.append(relation.description)\n",
    "                base_relation.source_chunks = sorted(list(all_chunks))\n",
    "                base_relation.description = '; '.join(descriptions) if descriptions else base_relation.description\n",
    "                merged_relations.append(base_relation)\n",
    "        return merged_relations\n",
    "\n",
    "    def _normalize_entity_key(self, entity_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Нормализация названия сущности для группировки\n",
    "        \n",
    "        Args:\n",
    "            entity_name: Название сущности\n",
    "            \n",
    "        Returns:\n",
    "            Нормализованное название\n",
    "        \"\"\"\n",
    "        normalized = re.sub(r'\\s+', ' ', entity_name.lower().strip())\n",
    "        normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    async def extract_knowledge_graph(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Основной метод для создания графа знаний из текста\n",
    "        \n",
    "        Args:\n",
    "            text: Входной текст для анализа\n",
    "            \n",
    "        Returns:\n",
    "            Словарь с графом знаний\n",
    "        \"\"\"\n",
    "        logger.info(\"=== НАЧАЛО ОБРАБОТКИ ТЕКСТА ===\")\n",
    "        sentences = self.split_text_into_sentences(text)\n",
    "        logger.info(f\"Текст разделен на {len(sentences)} предложений\")\n",
    "        if len(sentences) > self.max_sentences:\n",
    "            logger.info(f\"Текст слишком большой ({len(sentences)} предложений). Обрезаем до {self.max_sentences}.\")\n",
    "            sentences = sentences[:self.max_sentences]\n",
    "        chunks = self._create_chunks(sentences)\n",
    "        logger.info(f\"Создано чанков: {len(chunks)} (по {self.chunk_size} предложений)\")\n",
    "        entities = await self._extract_entities_from_chunks(chunks)\n",
    "        relations = []\n",
    "        if entities:\n",
    "            relations = await self._extract_relations_from_chunks(chunks, entities)\n",
    "        else:\n",
    "            logger.warning(\"Сущности не найдены, пропускаем поиск отношений\")\n",
    "        entity_types = defaultdict(int)\n",
    "        for entity in entities:\n",
    "            entity_types[entity.type] += 1\n",
    "        relation_types = defaultdict(int)\n",
    "        for relation in relations:\n",
    "            relation_types[relation.type] += 1\n",
    "        result = {\n",
    "            'entities': [asdict(e) for e in entities],\n",
    "            'relations': [asdict(r) for r in relations],\n",
    "            'chunks_info': [\n",
    "                {\n",
    "                    'id': chunk['id'],\n",
    "                    'text_preview': chunk['text'][:100] + '...' if len(chunk['text']) > 100 else chunk['text'],\n",
    "                    'sentences_count': len(chunk['sentences']),\n",
    "                    'has_formulas': bool(self.formula_pattern.search(chunk['original_text']))\n",
    "                } for chunk in chunks\n",
    "            ],\n",
    "            'stats': {\n",
    "                'total_sentences': len(sentences),\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_size': self.chunk_size,\n",
    "                'total_entities': len(entities),\n",
    "                'total_relations': len(relations),\n",
    "                'entity_types': dict(entity_types),\n",
    "                'relation_types': dict(relation_types),\n",
    "                'gemini_usage': self.key_usage_count,\n",
    "                'local_llm_used': not self.gemini_available\n",
    "            }\n",
    "        }\n",
    "        logger.info(\"=== ОБРАБОТКА ЗАВЕРШЕНА ===\")\n",
    "        logger.info(f\"Найдено сущностей: {len(entities)}\")\n",
    "        logger.info(f\"Найдено отношений: {len(relations)}\")\n",
    "        logger.info(f\"Использование Gemini: {self.key_usage_count}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "955f5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class SynonymGroup:\n",
    "    \"\"\"Класс для представления группы синонимов\"\"\"\n",
    "    main_term: str\n",
    "    synonyms: List[str]\n",
    "    confidence: float\n",
    "    source: str \n",
    "\n",
    "class SynonymExtractor:\n",
    "    \"\"\"Класс для извлечения синонимов из энтити с помощью LLM\"\"\"\n",
    "    def __init__(self, graphembedder: ImprovedKnowledgeExtractor,\n",
    "                 max_concurrent: int = 5,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Инициализация экстрактора синонимов\n",
    "        Args:\n",
    "            graphembedder: Экземпляр ImprovedKnowledgeExtractor\n",
    "            max_concurrent: Максимальное количество одновременных запросов\n",
    "        \"\"\"\n",
    "        self.graphembedder = graphembedder\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _create_synonym_prompt(self, entities: List[Dict], context: str = \"\") -> str:\n",
    "        \"\"\"Создание промпта для извлечения синонимов\"\"\"\n",
    "\n",
    "        base_prompt = \"\"\"Ты - эксперт по терминологии в области машинного обучения, нейронных сетей и когнитивных наук.\n",
    "\n",
    "Задача: Для каждого предоставленного термина найди все возможные синонимы, переводы и альтернативные названия.\n",
    "\n",
    "Контекст исследования: {context}\n",
    "\n",
    "Термины для анализа:\n",
    "{entity_list}\n",
    "\n",
    "Требования к ответу:\n",
    "1. Для каждого термина укажи ВСЕ возможные синонимы и однокоренные слова\n",
    "2. Включи переводы на английский/русский\n",
    "3. Включи сокращения и аббревиатуры\n",
    "4. Включи научные и разговорные варианты\n",
    "5. Укажи уровень уверенности (1-10)\n",
    "\n",
    "Формат ответа (строго JSON):\n",
    "{{\n",
    "  \"synonyms\": [\n",
    "    {{\n",
    "      \"main_term\": \"основной термин\",\n",
    "      \"alternatives\": [\"синоним1\", \"синоним2\", \"перевод\", \"сокращение\"],\n",
    "      \"confidence\": 8\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "ВАЖНО! main_term должен быть всегда уникален по смыслу и написанию, то что в alternatives должно иметь разное написание но похожее по смыслу!\n",
    "Примеры качественных синонимов:\n",
    "- \"нейронная сеть\" → [\"neural network\", \"нейросеть\", \"ИНС\", \"искусственная нейронная сеть\"]\n",
    "- \"функция активации\" → [\"activation function\", \"функция активации\", \"активация\"]\n",
    "- \"машинное обучение\" → [\"machine learning\", \"ML\", \"автоматическое обучение\"]\"\"\"\n",
    "        \n",
    "        entity_descriptions = []\n",
    "        for i, entity in enumerate(entities, 1):\n",
    "            desc = f\"{i}. '{entity['name']}' ({entity.get('type', 'unknown')})\"\n",
    "            if entity.get('description'):\n",
    "                desc += f\" - {entity['description'][:100]}...\"\n",
    "            entity_descriptions.append(desc)\n",
    "        entity_list = \"\\n\".join(entity_descriptions)\n",
    "        \n",
    "        return base_prompt.format(\n",
    "            context=context or \"исследование механизмов мозга и нейронных сетей\",\n",
    "            entity_list=entity_list\n",
    "        )\n",
    "    \n",
    "    async def _make_llm_request(self, prompt: str) -> Optional[Dict]:\n",
    "        \"\"\"Выполнение запроса к LLM\"\"\"\n",
    "        try:\n",
    "            if asyncio.iscoroutinefunction(self.graphembedder._call_gemini_api):\n",
    "                return await self.graphembedder._call_gemini_api(prompt)\n",
    "            else:\n",
    "                return self.graphembedder._call_gemini_api(prompt)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка LLM запроса: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _batch_entities(self, entities: List[Dict], batch_size: int = 10) -> List[List[Dict]]:\n",
    "        \"\"\"Разбиение энтити на батчи для обработки\"\"\"\n",
    "        batches = []\n",
    "        for i in range(0, len(entities), batch_size):\n",
    "            batches.append(entities[i:i + batch_size])\n",
    "        return batches\n",
    "    \n",
    "    async def extract_synonyms_batch(self, \n",
    "                                   entities: List[Dict], \n",
    "                                   context: str = \"\",\n",
    "                                   batch_size: int = 10) -> List[SynonymGroup]:\n",
    "        \"\"\"\n",
    "        Извлечение синонимов для батча энтити\n",
    "        \n",
    "        Args:\n",
    "            entities: Список энтити\n",
    "            context: Контекст для лучшего понимания\n",
    "            batch_size: Размер батча для обработки\n",
    "            \n",
    "        Returns:\n",
    "            Список групп синонимов\n",
    "        \"\"\"\n",
    "        logger.info(f\"Начинаю извлечение синонимов для {len(entities)} энтити\")\n",
    "        batches = self._batch_entities(entities, batch_size)\n",
    "        synonym_groups = []\n",
    "        semaphore = asyncio.Semaphore(self.max_concurrent)\n",
    "        \n",
    "        async def process_batch(batch: List[Dict]) -> List[SynonymGroup]:\n",
    "            async with semaphore:\n",
    "                try:\n",
    "                    prompt = self._create_synonym_prompt(batch, context)\n",
    "\n",
    "                    response = await self._make_llm_request(prompt)\n",
    "                    \n",
    "                    if not response or not isinstance(response, dict) or 'synonyms' not in response:\n",
    "                        logger.warning(f\"Некорректный ответ от LLM: {response}\")\n",
    "                        return []\n",
    "                    \n",
    "                    groups = []\n",
    "                    for syn_data in response['synonyms']:\n",
    "                        if 'main_term' in syn_data and 'alternatives' in syn_data:\n",
    "                            group = SynonymGroup(\n",
    "                                main_term=syn_data['main_term'],\n",
    "                                synonyms=syn_data['alternatives'],\n",
    "                                confidence=syn_data.get('confidence', 5) / 10.0,\n",
    "                                source='llm'\n",
    "                            )\n",
    "                            groups.append(group)\n",
    "                    \n",
    "                    return groups\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Ошибка в process_batch: {e}\")\n",
    "                    return []\n",
    "\n",
    "        tasks = [process_batch(batch) for batch in batches]\n",
    "\n",
    "        batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        for result in batch_results: \n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Ошибка обработки батча: {result}\")\n",
    "                continue\n",
    "            synonym_groups.extend(result)\n",
    "        \n",
    "        logger.info(f\"Извлечено {len(synonym_groups)} групп синонимов\")\n",
    "        return synonym_groups\n",
    "    \n",
    "    # def create_synonym_mapping(self, synonym_groups: List[SynonymGroup], \n",
    "    #                          confidence_threshold: float = 0.6) -> Dict[str, List[str]]:\n",
    "    #     \"\"\"\n",
    "    #     Создание маппинга синонимов для использования в дедупликации\n",
    "        \n",
    "    #     Args:\n",
    "    #         synonym_groups: Группы синонимов\n",
    "    #         confidence_threshold: Минимальный уровень уверенности\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Словарь синонимов\n",
    "    #     \"\"\"\n",
    "    #     synonym_mapping = {}\n",
    "        \n",
    "    #     for group in synonym_groups:\n",
    "    #         if group.confidence >= confidence_threshold:\n",
    "    #             main_term = group.main_term.lower().strip()\n",
    "    #             synonyms = [syn.lower().strip() for syn in group.synonyms if syn.strip()] \n",
    "                \n",
    "    #             if main_term and synonyms:\n",
    "    #                 synonym_mapping[main_term] = synonyms\n",
    "\n",
    "    #                 for synonym in synonyms:\n",
    "    #                     if synonym not in synonym_mapping:\n",
    "    #                         synonym_mapping[synonym] = [main_term]\n",
    "    #                     elif main_term not in synonym_mapping[synonym]:\n",
    "    #                         synonym_mapping[synonym].append(main_term)\n",
    "        \n",
    "    #     return synonym_mapping\n",
    "\n",
    "\n",
    "    def create_synonym_mapping(self, synonym_groups: List[SynonymGroup],\n",
    "                                    confidence_threshold: float = 0.6) -> Dict[str, List[str]]:\n",
    "        if not synonym_groups:\n",
    "            return {}\n",
    "        from collections import defaultdict\n",
    "        synonym_sets = defaultdict(set)\n",
    "        for group in synonym_groups:\n",
    "            if group.confidence < confidence_threshold:\n",
    "                continue  \n",
    "            main_term = group.main_term.lower().strip()\n",
    "            if not main_term:\n",
    "                continue\n",
    "            valid_synonyms = {\n",
    "                syn.lower().strip() \n",
    "                for syn in group.synonyms \n",
    "                if syn and syn.strip()\n",
    "            }\n",
    "            if not valid_synonyms:\n",
    "                continue\n",
    "            synonym_sets[main_term].update(valid_synonyms)\n",
    "            for synonym in valid_synonyms:\n",
    "                synonym_sets[synonym].add(main_term)\n",
    "        return {\n",
    "            term: list(synonyms) \n",
    "            for term, synonyms in synonym_sets.items() \n",
    "            if synonyms\n",
    "        }\n",
    "\n",
    "async def extract_synonyms_for_entities(graphembedder: ImprovedKnowledgeExtractor, \n",
    "                                      entities: List[Dict], \n",
    "                                      context: str = \"\") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Основная функция для извлечения синонимов\n",
    "    \n",
    "    Args:\n",
    "        graphembedder: Экземпляр ImprovedKnowledgeExtractor\n",
    "        entities: Список энтити\n",
    "        context: Контекст исследования\n",
    "        \n",
    "    Returns:\n",
    "        Словарь синонимов для использования в дедупликации\n",
    "    \"\"\"\n",
    "    extractor = SynonymExtractor(\n",
    "        graphembedder=graphembedder, \n",
    "        max_concurrent=3\n",
    "    )\n",
    "    \n",
    "    synonym_groups = await extractor.extract_synonyms_batch(\n",
    "        entities=entities,\n",
    "        context=context,\n",
    "        batch_size=8  \n",
    "    )\n",
    "    \n",
    "    synonym_mapping = extractor.create_synonym_mapping(\n",
    "        synonym_groups, \n",
    "        confidence_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    return synonym_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "242b9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EntityMatch:\n",
    "    \"\"\"Класс для представления совпадения энтити\"\"\"\n",
    "    entity1_id: str\n",
    "    entity2_id: str\n",
    "    similarity_score: float\n",
    "    match_type: str  \n",
    "\n",
    "class EntityDeduplicator:\n",
    "    def __init__(self, graphembedder: ImprovedKnowledgeExtractor, use_llm=False):\n",
    "        self.use_llm = use_llm\n",
    "        self.graphembedder = graphembedder\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "        except OSError:\n",
    "            print(\"Модель ru_core_news_sm не найдена. Установите: python -m spacy download ru_core_news_sm\")\n",
    "            self.nlp = None\n",
    "         \n",
    "        if use_llm:\n",
    "            try:\n",
    "                self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "            except:\n",
    "                print(\"Не удалось загрузить модель sentence-transformers\")\n",
    "                self.sentence_model = None\n",
    "\n",
    "        self.synonyms = {\n",
    "            'функция': [\"функцией\",\"функции\",'function','функц',\"mapping\",\"отображение\",\"зависимость\",\"соотношение\",\"преобразование\",\"transformation\",\"relation\",\"mathematical function\",\"математическая функция\",\"ф-ция\"],\n",
    "            'предсказание': [\"предсказании\",\"предсказнием\",'prediction', 'прогноз', 'предикт',\"inference\",\"вывод\",\"прогноз\",\"оценка\",\"estimation\",\"прогнозирование\",\"вычисление\",\"computation\",\"предсказание значения\",\"предсказание результата\",\"результат работы сети\",\"выход\",\"output\"],\n",
    "            'слой': [\"слое\",\"слоем\",'layer', 'уровень',\"ярус\",\"level\",\"пласт\",\"stratum\",\"слой нейронов\",\"neural layer\",\"hidden layer\",\"скрытый слой\",\"input layer\",\"входной слой\",\"output layer\",\"выходной слой\"],\n",
    "            'нейрон': [\"нейроне\",\"нейроном\",'neuron', 'нервная клетка',\"neural cell\",\"нервная клетка\",\"artificial neuron\",\"искусственный нейрон\"],\n",
    "            'вес': [\"весом\",\"весе\",'weight',\"connection weight\",\"synaptic weight\",\"синаптический вес\",\"весовой коэффициент\",\"weight coefficient\",\"параметр связи\",\"connection parameter\",\"сила связи\",\"connection strength\",\"вес\",\"weighting factor\"],\n",
    "            'смещение': [\"смещении\",\"смещением\",'bias', 'сдвиг',\"bias term\",\"offset\",\"bias unit\",\"смещающий фактор\",\"bias factor\"],\n",
    "            'активация': ['activation',\"activation function\",\"transfer function\",\"передаточная функция\",\"nonlinear function\",\"нелинейная функция активации\",\"activation function (af)\",\"функция активации (фа)\",\"activation unit\",\"блок активации\"],\n",
    "            'матрица': ['matrix',\"матрицей\", \"матрице\"],\n",
    "            'вектор': ['vector',\"всетором\",\"векторе\"],\n",
    "            'оптимизация': ['optimization', 'оптимизац', \"оптимизации\"],\n",
    "            \"метод оптимизации параметров\": [\n",
    "                \"parameter optimization method\",\n",
    "                \"optimization algorithm\",\n",
    "                \"parameter tuning\",\n",
    "                \"parameter estimation\",\n",
    "                \"оптимизация параметров\",\n",
    "                \"настройка параметров\",\n",
    "                \"оценка параметров\",\n",
    "                \"алгоритм оптимизации\",\n",
    "                \"метод настройки параметров\",\n",
    "                \"parameter optimization technique\"\n",
    "            ],\n",
    "            'обучение': ['training',\n",
    "                          'learning',\n",
    "                            'машинное обучение',\n",
    "                              'machine learning',\n",
    "                                \"обучении\"],\n",
    "            \"функция ошибки\": [\n",
    "                \"loss function\",\n",
    "                \"cost function\",\n",
    "                \"objective function\",\n",
    "                \"error function\",\n",
    "                \"функция потерь\",\n",
    "                \"целевая функция\",\n",
    "                \"функционал ошибки\",\n",
    "                \"критерий ошибки\",\n",
    "                \"мера ошибки\",\n",
    "                \"функция стоимости\",\n",
    "                \"функция расхождения\",\n",
    "                \"error\",\n",
    "                \"loss\",\n",
    "                \"cost\",\n",
    "                \"objective\",\n",
    "                \"потеря\",\n",
    "                \"ошибка\",\n",
    "                \"стоимость\",\n",
    "                \"объектив\",\n",
    "                \"функция несовпадения\",\n",
    "                \"функция неточности\",\n",
    "                \"функция несоответствия\"\n",
    "            ],\n",
    "            \"средняя функция ошибки\": [\n",
    "                \"mean squared error (mse)\",\n",
    "                \"mean error\",\n",
    "                \"average loss\",\n",
    "                \"average cost\",\n",
    "                \"mse\",\n",
    "                \"среднеквадратичная ошибка\",\n",
    "                \"средняя ошибка\",\n",
    "                \"усредненная функция потерь\",\n",
    "                \"усредненная функция стоимости\",\n",
    "                \"среднее значение функции ошибки\",\n",
    "                \"average error function\",\n",
    "                \"mean loss function\",\n",
    "                \"mean cost function\"\n",
    "            ],          \n",
    "            \"градиентный спуск\": [\n",
    "                \"gradient descent\",\n",
    "                \"gd\",\n",
    "                \"steepest descent\",\n",
    "                \"метод градиентного спуска\",\n",
    "                \"спуск по градиенту\",\n",
    "                \"градиентный метод\",\n",
    "                \"метод наискорейшего спуска\",\n",
    "                \"gradient descent optimization\",\n",
    "                \"gradient optimization\",\n",
    "                \"градиентный спуск\",\n",
    "                \"алгоритм градиентного спуска\",\n",
    "                \"оптимизация градиентом\",\n",
    "                \"метод оптимизации первого порядка\",\n",
    "                \"gradient method\"\n",
    "            ],\n",
    "            \"алгоритмы\": [\n",
    "                \"algorithms\",\n",
    "                \"methods\",\n",
    "                \"approaches\",\n",
    "                \"techniques\",\n",
    "                \"procedures\",\n",
    "                \"методы\",\n",
    "                \"подходы\",\n",
    "                \"техники\",\n",
    "                \"процедуры\",\n",
    "                \"способы\",\n",
    "                \"рецепты\",\n",
    "                \"инструкции\",\n",
    "                \"набор инструкций\",\n",
    "                \"набор методов\"\n",
    "            ],\n",
    "            \"метод обратной связи\": [\n",
    "                \"feedback method\",\n",
    "                \"feedback control\",\n",
    "                \"closed-loop control\",\n",
    "                \"обратная связь\",\n",
    "                \"управление с обратной связью\",\n",
    "                \"регулирование с обратной связью\",\n",
    "                \"замкнутый контур управления\",\n",
    "                \"feedback mechanism\",\n",
    "                \"обратная связь\",\n",
    "                \"обратное распространение\"\n",
    "            ],\n",
    "            \"коэффициент обучения\": [\n",
    "                \"learning rate\",\n",
    "                \"step size\",\n",
    "                \"coefficient of learning\",\n",
    "                \"скорость обучения\",\n",
    "                \"размер шага\",\n",
    "                \"learning coefficient\",\n",
    "                \"step size parameter\"\n",
    "            ],\n",
    "            \"ib-искажение\": [\n",
    "                \"ib distortion\",\n",
    "                \"искажение информационного бутылочного горлышка\",\n",
    "                \"distortion of the information bottleneck\",\n",
    "                \"мера искажения в ib\",\n",
    "                \"потеря информации в представлении\",\n",
    "                \"информационные потери\",\n",
    "                \"information loss\",\n",
    "                \"information bottleneck distortion\",\n",
    "                \"ib-loss\",\n",
    "                \"мера релевантности представления\"\n",
    "            ],\n",
    "            \"ibm\": [\n",
    "                \"information bottleneck method\",\n",
    "                \"метод информационного бутылочного горлышка\",\n",
    "                \"информационный барьер\",\n",
    "                \"information bottleneck\",\n",
    "                \"ib\",\n",
    "                \"метод ib\",\n",
    "                \"подход ib\",\n",
    "                \"алгоритм ib\",\n",
    "                \"information bottleneck principle\"\n",
    "            ], \n",
    "            \"классификация\": [\n",
    "                \"категоризация\",\n",
    "                \"распознавание образов\",\n",
    "                \"отнесение к классу\",\n",
    "                \"кластеризация (в контексте обучения с учителем)\",\n",
    "                \"classification\",\n",
    "                \"categorization\",\n",
    "                \"pattern recognition\",\n",
    "                \"clustering (supervised learning context)\",\n",
    "                \"классификация\",\n",
    "                \"категоризация\",\n",
    "                \"распознавание образов\",\n",
    "                \"кластеризация\"\n",
    "            ],\n",
    "            \"регрессия\": [\n",
    "                \"регрессия\",\n",
    "                \"регрессионный анализ\",\n",
    "                \"восстановление зависимости\",\n",
    "                \"regression\",\n",
    "                \"regression analysis\",\n",
    "                \"function approximation\",\n",
    "                \"регрессия\",\n",
    "                \"регрессионный анализ\",\n",
    "                \"аппроксимация функции\"\n",
    "            ],\n",
    "            \"линейная регрессия\": [\n",
    "                \"linear regression\",\n",
    "                \"lr\",\n",
    "                \"линейная модель\",\n",
    "                \"linear model\",\n",
    "                \"регрессия по методу наименьших квадратов\",\n",
    "                \"least squares regression\",\n",
    "                \"обыкновенная линейная регрессия\",\n",
    "                \"ordinary linear regression\",\n",
    "                \"olr\"\n",
    "            ],\n",
    "            \"knn\": [\n",
    "                \"k-ближайших соседей\",\n",
    "                \"метод k-ближайших соседей\",\n",
    "                \"k-nn\",\n",
    "                \"k-nearest neighbors\",\n",
    "                \"knn\",\n",
    "                \"k-ближайших соседей\",\n",
    "                \"метод ближайших соседей\"\n",
    "            ],\n",
    "            \"качество данных\": [\n",
    "                \"quality of data\",\n",
    "                \"точность данных\",\n",
    "                \"согласованность данных\",\n",
    "                \"чистота данных\",\n",
    "                \"data quality\",\n",
    "                \"data integrity\",\n",
    "                \"data accuracy\",\n",
    "                \"data consistency\",\n",
    "                \"data completeness\",\n",
    "                \"качество данных\",\n",
    "                \"целостность данных\",\n",
    "                \"точность данных\",\n",
    "                \"согласованность данных\",\n",
    "                \"полнота данных\"\n",
    "            ],\n",
    "            \"обобщающая способность\": [\n",
    "                \"generalization ability\",\n",
    "                \"способность к обобщению\",\n",
    "                \"способность к генерализации\",\n",
    "                \"устойчивость модели\",\n",
    "                \"generalization\",\n",
    "                \"generalizability\",\n",
    "                \"out-of-sample performance\",\n",
    "                \"мощность модели\",\n",
    "                \"обобщение\",\n",
    "                \"генерализация\",\n",
    "                \"устойчивость\",\n",
    "                \"способность к обучению\"\n",
    "            ],\n",
    "            \"информативные признаки\": [\n",
    "                \"informative patterns\",\n",
    "                \"полезные закономерности\",\n",
    "                \"значимые зависимости\",\n",
    "                \"сигналы\",\n",
    "                \"informative features\",\n",
    "                \"useful patterns\",\n",
    "                \"meaningful relationships\",\n",
    "                \"signals\",\n",
    "                \"информативные закономерности\",\n",
    "                \"полезные признаки\",\n",
    "                \"значимые закономерности\",\n",
    "                \"сигналы\"\n",
    "            ],\n",
    "            \"зашумленные данные\" : [ \n",
    "                \"шумовые данные\",\n",
    "                \"нерелевантные данные\",\n",
    "                \"лишние данные\",\n",
    "                \"мусорные данные\",\n",
    "                \"noise\",\n",
    "                \"noise data\",\n",
    "                \"noisy data\",\n",
    "                \"irrelevant data\",\n",
    "                \"garbage data\",\n",
    "                \"шум\",\n",
    "                \"нерелевантные данные\",\n",
    "                \"мусор\"\n",
    "            ],\n",
    "            \"k-means\": [\n",
    "                \"k-means algorithm\",\n",
    "                \"k-средних\",\n",
    "                \"k-means clustering\",\n",
    "                \"k-средних кластеризация\",\n",
    "                \"метод k-средних\",\n",
    "                \"k-means\",\n",
    "                \"алгоритм кластеризации k-средних\"\n",
    "            ],\n",
    "            \"кластеризация данных\": [\n",
    "                \"data clustering\",\n",
    "                \"clustering\",\n",
    "                \"кластерный анализ\",\n",
    "                \"группировка данных\",\n",
    "                \"сегментация данных\",\n",
    "                \"data segmentation\",\n",
    "                \"data grouping\",\n",
    "                \"cluster analysis\"\n",
    "            ],\n",
    "            \"кластеры\": [\n",
    "                \"k clusters\",\n",
    "                \"кластеры\",\n",
    "                \"группы\",\n",
    "                \"k groups\",\n",
    "                \"сегменты\",\n",
    "                \"clusters\",\n",
    "                \"groups\",\n",
    "                \"segments\",\n",
    "                \"классы\"\n",
    "            ],\n",
    "            \"метод опорных векторов\": [\n",
    "                \"support vector machine\",\n",
    "                \"svm\",\n",
    "                \"support vector networks\",\n",
    "                \"метод опорных сетей\",\n",
    "                \"машина опорных векторов\",\n",
    "                \"support vector classifier\",\n",
    "                \"svc\",\n",
    "                \"supportvectormachines (svm)\",\n",
    "                \"support vector machine\",\n",
    "                \"метод опорных векторов\",\n",
    "                \"машина опорных векторов\",\n",
    "                \"support vector networks\",\n",
    "                \"support vector classifier\",\n",
    "                \"svc\",\n",
    "                \"supportvectormachines (svm)\",\n",
    "                \"сети опорных векторов\",\n",
    "                \"support vector networks\"\n",
    "            ],\n",
    "             \"алгоритм случайного леса\": [\n",
    "                \"random forest algorithm\",\n",
    "                \"random forest\",\n",
    "                \"случайный лес\",\n",
    "                \"rf\",\n",
    "                \"алгоритм rf\",\n",
    "                \"random decision forest\",\n",
    "                \"ансамбль деревьев решений\"\n",
    "            ],\n",
    "            \"входные данные\": [\n",
    "                \"входные данные\",\n",
    "                \"входной вектор\",\n",
    "                \"признак\",\n",
    "                \"фактор\",\n",
    "                \"переменная\",\n",
    "                \"независимая переменная\",\n",
    "                \"предиктор\",\n",
    "                \"input data\",\n",
    "                \"input vector\",\n",
    "                \"feature\",\n",
    "                \"predictor\",\n",
    "                \"independent variable\",\n",
    "                \"explanatory variable\",\n",
    "                \"input\",\n",
    "                \"вход\",\n",
    "                \"аргумент\",\n",
    "                \"аргумент функции\"\n",
    "            ],\n",
    "            \"теория вероятностей\": [\n",
    "                \"probability theory\",\n",
    "                \"probability\",\n",
    "                \"теория вероятности\",\n",
    "                \"тв\",\n",
    "                \"вероятностная теория\",\n",
    "                \"исчисление вероятностей\",\n",
    "                \"probability calculus\",\n",
    "                \"probability science\"\n",
    "            ],\n",
    "            \"событие\": [\n",
    "                \"event\",\n",
    "                \"случайное событие\",\n",
    "                \"исход\",\n",
    "                \"наступление события\",\n",
    "                \"реализация события\"\n",
    "                ],\n",
    "            \"классы\": [\n",
    "                \"категории\",\n",
    "                \"типы\",\n",
    "                \"разряды\",\n",
    "                \"группы\",\n",
    "                \"множества\",\n",
    "                \"классификации\",\n",
    "                \"entities\",\n",
    "                \"объекты\",\n",
    "                \"categories\",\n",
    "                \"types\",\n",
    "                \"groups\",\n",
    "                \"sets\",\n",
    "                \"classifications\",\n",
    "                \"классы объектов\",\n",
    "                \"классы данных\",\n",
    "                \"разделы\",\n",
    "                \"разновидности\"\n",
    "            ],\n",
    "            \"условная энтропия x при условии t_ε\" : [\n",
    "                \"$h(x|t_ε)$\",\n",
    "                \"conditional entropy of x given t_ε\",\n",
    "                \"энтропия условной вероятности\",\n",
    "                \"мера неопределенности x при известном t_ε\",\n",
    "                \"conditional entropy\",\n",
    "                \"h(x|t_epsilon)\",\n",
    "                \"h(x|tε)\",\n",
    "                \"условная информационная энтропия\",\n",
    "                \"information entropy\"\n",
    "            ],\n",
    "            \"взаимная информация между t_ε и x\": [\n",
    "                \"информация о взаимодействии между t_ε и x\",\n",
    "                \"mutual information between t_ε and x\",\n",
    "                \"information interaction\",\n",
    "                \"i(t_epsilon; x)\",\n",
    "                \"i(tε; x)\",\n",
    "                \"$i(t_ε; x)$\",\n",
    "                \"передаваемая информация\",\n",
    "                \"количество информации\",\n",
    "                \"information gain\"\n",
    "            ],\n",
    "            \"cnn\": [\n",
    "                \"сверточная нейронная сеть\",\n",
    "                \"свёрточная нейросеть\",\n",
    "                \"кнс\",\n",
    "                \"convolutional net\",\n",
    "                \"convnet\",\n",
    "                \"снс\",\n",
    "                \"deep convolutional neural networks\",\n",
    "                \"dcnn\"\n",
    "            ],\n",
    "            \"края\": [\n",
    "                \"границы\",\n",
    "                \"линии\",\n",
    "                \"контуры\",\n",
    "                \"edges\",\n",
    "                \"borders\",\n",
    "                \"lines\",\n",
    "                \"contours\",\n",
    "                \"примитивы\",\n",
    "                \"простые признаки\",\n",
    "                \"характерные точки\",\n",
    "                \"edge features\",\n",
    "                \"edge detection\"\n",
    "            ],\n",
    "            \"долговременная память\": [\n",
    "                \"long-term memory\",\n",
    "                \"ltm\",\n",
    "                \"веса нейронной сети\",\n",
    "                \"параметры модели\",\n",
    "                \"сохраненные знания\",\n",
    "                \"матрицы весов\",\n",
    "                \"вектора смещения\",\n",
    "                \"bias\",\n",
    "                \"weights\",\n",
    "                \"model parameters\",\n",
    "                \"trained parameters\",\n",
    "                \"обученные параметры\"\n",
    "            ],       \n",
    "            \"информационная плоскость\": [\n",
    "                \"information plane\",\n",
    "                \"ib plane\",\n",
    "                \"плоскость информационной бутылки\",\n",
    "                \"information bottleneck plane\",\n",
    "                \"область представления информации\",\n",
    "                \"пространство информации\",\n",
    "                \"information space\",\n",
    "                \"information representation space\",\n",
    "                \"плоскость обучения\",\n",
    "                \"learning plane\"\n",
    "            ],\n",
    "            \"измерение полезной информации\": [\n",
    "                \"ось полезной информации\",\n",
    "                \"axis of relevant information\",\n",
    "                \"relevant information dimension\",\n",
    "                \"ось r\",\n",
    "                \"r-axis\",\n",
    "                \"ось представления информации\",\n",
    "                \"information representation axis\"\n",
    "            ],\n",
    "            \"неопределенность\": [\n",
    "                \"неопределённость\",\n",
    "                \"неизвестность\",\n",
    "                \"сомнение\",\n",
    "                \"двусмысленность\",\n",
    "                \"vagueness\",\n",
    "                \"ambiguity\",\n",
    "                \"indeterminacy\",\n",
    "                \"lack of certainty\",\n",
    "                \"degree of doubt\",\n",
    "                \"uncertainty quantification (uq)\",\n",
    "                \"энтропия (в контексте теории информации)\",\n",
    "                \"риск (в контексте принятия решений)\",\n",
    "                \"вероятностная неопределенность\",\n",
    "                \"эпистемическая неопределенность\",\n",
    "                \"алеаторная неопределенность\"\n",
    "            ],\n",
    "            \"слои нейронной сети\": [\n",
    "                \"neural network layers\",\n",
    "                \"layers of a neural network\",\n",
    "                \"слои инс\",\n",
    "                \"слои нейросети\",\n",
    "                \"уровни нейронной сети\",\n",
    "            ],\n",
    "            \"скрытые слои\": [\n",
    "                \"hidden layers\",\n",
    "                \"скрытый слой\",\n",
    "                \"hidden layer\",\n",
    "                \"промежуточные слои\",\n",
    "                \"intermediate layers\",\n",
    "                \"внутренние слои\",\n",
    "                \"internal layers\",\n",
    "                \"слои между входом и выходом\",\n",
    "                \"layers between input and output\",\n",
    "                \"невидимые слои\",\n",
    "                \"invisible layers\"\n",
    "            ],\n",
    "            \"разделимость данных\": [\n",
    "                \"data separability\",\n",
    "                \"separability of data\",\n",
    "                \"линейная разделимость\",\n",
    "                \"linearly separable data\",\n",
    "                \"разделимость классов\",\n",
    "                \"кластеризуемость\",\n",
    "                \"отделимость данных\",\n",
    "                \"различимость данных\",\n",
    "                \"разделимость выборок\",\n",
    "                \"разделимость признаков\",\n",
    "                \"data distinguishability\",\n",
    "                \"class separability\",\n",
    "                \"feature separability\",\n",
    "                \"linearly separable data\",\n",
    "                \"линейно разделимые данные\",\n",
    "                \"разделимые данные\",\n",
    "                \"separable data\",\n",
    "                \"данные с линейной разделимостью\",\n",
    "                \"data with linear separability\",\n",
    "                \"данные, которые можно разделить прямой линией\",\n",
    "                \"data that can be separated by a straight line\",\n",
    "                \"линейно разделимый набор данных\",\n",
    "                \"linearly separable dataset\"\n",
    "            ],\n",
    "            \"наблюдатель\": [\n",
    "                \"observer\",\n",
    "                \"agent\",\n",
    "                \"агент\",\n",
    "                \"субъект\",\n",
    "                \"пользователь\",\n",
    "                \"система\",\n",
    "                \"процессор\",\n",
    "                \"датчик\",\n",
    "                \"сенсор\",\n",
    "                \"рецептор\",\n",
    "                \"приёмник информации\",\n",
    "                \"получатель информации\",\n",
    "                \"информационный агент\"\n",
    "            ],\n",
    "            \"мера информации\": [\n",
    "                \"information measure\",\n",
    "                \"measure of information\",\n",
    "                \"количество информации\",\n",
    "                \"информационная энтропия\",\n",
    "                \"информационная ценность\",\n",
    "                \"информативность\",\n",
    "                \"информационный критерий\",\n",
    "                \"информационная метрика\"\n",
    "            ],\n",
    "            \"собственная информация\": [\n",
    "                \"information content\",\n",
    "                \"self-information\",\n",
    "                \"собственная информация\",\n",
    "                \"информационное содержание\",\n",
    "                \"информативность события\",\n",
    "                \"information of event x\",\n",
    "                \"i(x)\",\n",
    "                \"мера неожиданности события\",\n",
    "                \"surprisal\",\n",
    "                \"-log(p(x))\",\n",
    "                \"отрицательный логарифм вероятности события\"\n",
    "            ],\n",
    "            \"априорная вероятностная модель\": [\n",
    "                \"prior probability model\",\n",
    "                \"модель априорных вероятностей\",\n",
    "                \"apriori probability model\",\n",
    "                \"начальная вероятностная модель\",\n",
    "                \"initial probability model\",\n",
    "                \"базовая вероятностная модель\",\n",
    "                \"baseline probability model\",\n",
    "                \"модель предварительной вероятности\",\n",
    "                \"preliminary probability model\",\n",
    "                \"априорная модель\",\n",
    "                \"prior model\"\n",
    "            ],\n",
    "            \"выходные данные\": [\n",
    "                \"целевой вывод $y$\",\n",
    "                \"target output $y$\",\n",
    "                \"output data $y$\",\n",
    "                \"предсказанный язык\",\n",
    "                \"predicted language\",\n",
    "                \"определенный язык\",\n",
    "                \"identified language\",\n",
    "                \"метка класса\",\n",
    "                \"class label\",\n",
    "                \"истинный класс\",\n",
    "                \"true class\",\n",
    "                \"y\"\n",
    "            ],\n",
    "            \"извлечение признаков\": [\n",
    "                \"feature extraction\",\n",
    "                \"выделение признаков\",\n",
    "                \"feature selection\",\n",
    "                \"инженерия признаков\",\n",
    "                \"feature engineering\",\n",
    "                \"отбор признаков\",\n",
    "                \"feature subset selection\",\n",
    "                \"преобразование признаков\",\n",
    "                \"feature transformation\",\n",
    "                \"проектирование признаков\",\n",
    "                \"feature design\"\n",
    "            ],\n",
    "            \"дерево решений\": [\n",
    "                \"decision tree\",\n",
    "                \"деревья решений\",\n",
    "                \"решающее дерево\",\n",
    "                \"решающие деревья\",\n",
    "                \"decision tree learning\",\n",
    "                \"обучение с использованием деревьев решений\",\n",
    "                \"дерево классификации\",\n",
    "                \"classification tree\",\n",
    "                \"дерево регрессии\",\n",
    "                \"regression tree\",\n",
    "                \"дерево принятия решений\",\n",
    "                \"decision making tree\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Нормализация текста\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def get_lemma(self, text: str) -> str:\n",
    "        \"\"\"Получение леммы текста\"\"\"\n",
    "        if not self.nlp or not text:\n",
    "            return text\n",
    "        doc = self.nlp(text)\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        return ' '.join(lemmas) if lemmas else text\n",
    "    \n",
    "    # def find_exact_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "    #     \"\"\"Поиск точных совпадений по названию\"\"\"\n",
    "    #     matches = []\n",
    "    #     name_to_entities = defaultdict(list)\n",
    "    #     for entity in entities:\n",
    "    #         normalized_name = self.normalize_text(entity['name'])\n",
    "    #         name_to_entities[normalized_name].append(entity['id'])\n",
    "    #     for name, entity_ids in name_to_entities.items():\n",
    "    #         if len(entity_ids) > 1:\n",
    "    #             for i in range(len(entity_ids)):\n",
    "    #                 for j in range(i + 1, len(entity_ids)):\n",
    "    #                     matches.append(EntityMatch(\n",
    "    #                         entity_ids[i], entity_ids[j], 1.0, 'exact'\n",
    "    #                     ))\n",
    "    #     return matches\n",
    "    \n",
    "    # def find_lemma_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "    #     \"\"\"Поиск совпадений по леммам\"\"\"\n",
    "    #     if not self.nlp:\n",
    "    #         return []\n",
    "    #     matches = []\n",
    "    #     lemma_to_entities = defaultdict(list)\n",
    "    #     for entity in entities:\n",
    "    #         lemma = self.get_lemma(self.normalize_text(entity['name']))\n",
    "    #         if lemma:\n",
    "    #             lemma_to_entities[lemma].append(entity['id'])\n",
    "    #     for lemma, entity_ids in lemma_to_entities.items():\n",
    "    #         if len(entity_ids) > 1:\n",
    "    #             for i in range(len(entity_ids)):\n",
    "    #                 for j in range(i + 1, len(entity_ids)):\n",
    "    #                     matches.append(EntityMatch(\n",
    "    #                         entity_ids[i], entity_ids[j], 0.9, 'lemma'\n",
    "    #                     ))\n",
    "    #     return matches\n",
    "    \n",
    "    # def find_synonym_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "    #     \"\"\"Поиск совпадений среди синонимов\"\"\"\n",
    "    #     matches = []\n",
    "    #     word_to_group = {}\n",
    "    #     for main_word, synonyms in self.synonyms.items():\n",
    "    #         word_to_group[main_word] = main_word\n",
    "    #         for syn in synonyms:\n",
    "    #             word_to_group[syn] = main_word\n",
    "    #     group_to_entities = defaultdict(list)\n",
    "    #     for entity in entities:\n",
    "    #         normalized_name = self.normalize_text(entity['name'])\n",
    "    #         for word, group in word_to_group.items():\n",
    "    #             if word in normalized_name or normalized_name in word:\n",
    "    #                 group_to_entities[group].append(entity['id'])\n",
    "    #                 break\n",
    "    #     for group, entity_ids in group_to_entities.items():\n",
    "    #         if len(entity_ids) > 1:\n",
    "    #             for i in range(len(entity_ids)):\n",
    "    #                 for j in range(i + 1, len(entity_ids)):\n",
    "    #                     matches.append(EntityMatch(\n",
    "    #                         entity_ids[i], entity_ids[j], 0.8, 'synonym'\n",
    "    #                     ))\n",
    "    #     return matches\n",
    "\n",
    "    def _generate_matches_from_groups(self, groups: Dict, score: float, match_type: str) -> List[EntityMatch]:\n",
    "        \"\"\"Общий метод для генерации совпадений из групп сущностей\"\"\"\n",
    "        matches = []\n",
    "        for entity_ids in groups.values():\n",
    "            if len(entity_ids) > 1:\n",
    "                matches.extend(\n",
    "                    EntityMatch(id1, id2, score, match_type)\n",
    "                    for id1, id2 in combinations(entity_ids, 2)\n",
    "                )\n",
    "        return matches\n",
    "\n",
    "    def find_exact_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "        \"\"\"Поиск точных совпадений по названию\"\"\"\n",
    "        name_to_entities = defaultdict(list)\n",
    "        for entity in entities:\n",
    "            normalized_name = self.normalize_text(entity['name'])\n",
    "            name_to_entities[normalized_name].append(entity['id'])\n",
    "        \n",
    "        return self._generate_matches_from_groups(groups = name_to_entities, score = 1.0, match_type = 'exact')\n",
    "\n",
    "    def find_lemma_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "        \"\"\"Поиск совпадений по леммам\"\"\"\n",
    "        if not self.nlp:\n",
    "            return []\n",
    "        \n",
    "        lemma_to_entities = defaultdict(list)\n",
    "        for entity in entities:\n",
    "            lemma = self.get_lemma(self.normalize_text(entity['name']))\n",
    "            if lemma:\n",
    "                lemma_to_entities[lemma].append(entity['id'])\n",
    "        \n",
    "        return self._generate_matches_from_groups(groups = lemma_to_entities, score = 0.9, match_type = 'lemma')\n",
    "\n",
    "    def find_synonym_matches(self, entities: List[Dict]) -> List[EntityMatch]:\n",
    "        \"\"\"Поиск совпадений среди синонимов\"\"\"\n",
    "        word_to_group = {}\n",
    "        for main_word, synonyms in self.synonyms.items():\n",
    "            word_to_group[main_word] = main_word\n",
    "            for syn in synonyms:\n",
    "                word_to_group[syn] = main_word\n",
    "        \n",
    "        group_to_entities = defaultdict(list)\n",
    "        for entity in entities:\n",
    "            normalized_name = self.normalize_text(entity['name'])\n",
    "            matched_group = None\n",
    "            for word, group in word_to_group.items():\n",
    "                if word in normalized_name or normalized_name in word:\n",
    "                    matched_group = group\n",
    "                    break\n",
    "            \n",
    "            if matched_group:\n",
    "                group_to_entities[matched_group].append(entity['id'])\n",
    "        \n",
    "        return self._generate_matches_from_groups(groups = group_to_entities, score = 0.8, match_type= 'synonym')\n",
    "\n",
    "    def find_semantic_matches(self, entities: List[Dict], threshold: float = 0.7) -> List[EntityMatch]:\n",
    "        \"\"\"Поиск семантических совпадений с помощью эмбеддингов\"\"\"\n",
    "        if not self.use_llm or not self.sentence_model:\n",
    "            return []\n",
    "        matches = []\n",
    "        texts = []\n",
    "        entity_ids = []\n",
    "        for entity in entities:\n",
    "            text = f\"{entity['name']} {entity.get('description', '')}\"\n",
    "            texts.append(text)\n",
    "            entity_ids.append(entity['id'])\n",
    "        if len(texts) < 2:\n",
    "            return matches\n",
    "        try:\n",
    "            embeddings = self.sentence_model.encode(texts)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            for i in range(len(entities)):\n",
    "                for j in range(i + 1, len(entities)):\n",
    "                    similarity = similarity_matrix[i][j]\n",
    "                    if similarity >= threshold:\n",
    "                        matches.append(EntityMatch(\n",
    "                            entity_ids[i], entity_ids[j], float(similarity), 'semantic'\n",
    "                        ))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при семантическом анализе: {e}\")\n",
    "        return matches\n",
    "    \n",
    "    def merge_entities(self, entities: List[Dict], relations: List[Dict], \n",
    "                      matches: List[EntityMatch]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Объединение энтити и обновление связей\"\"\"\n",
    "        merge_groups = defaultdict(set)\n",
    "        entity_to_group = {}\n",
    "        for match in matches:\n",
    "            group_id = None\n",
    "            if match.entity1_id in entity_to_group:\n",
    "                group_id = entity_to_group[match.entity1_id]\n",
    "            elif match.entity2_id in entity_to_group:\n",
    "                group_id = entity_to_group[match.entity2_id]\n",
    "            else:\n",
    "                group_id = match.entity1_id \n",
    "            merge_groups[group_id].add(match.entity1_id)\n",
    "            merge_groups[group_id].add(match.entity2_id)\n",
    "            entity_to_group[match.entity1_id] = group_id\n",
    "            entity_to_group[match.entity2_id] = group_id\n",
    "        id_mapping = {}\n",
    "        for group_id, entity_ids in merge_groups.items():\n",
    "            for entity_id in entity_ids:\n",
    "                id_mapping[entity_id] = group_id\n",
    "        merged_entities = []\n",
    "        processed_groups = set()\n",
    "        for entity in entities:\n",
    "            entity_id = entity['id']\n",
    "            if entity_id in entity_to_group:\n",
    "                group_id = entity_to_group[entity_id]\n",
    "                if group_id not in processed_groups:\n",
    "                    group_entities = [e for e in entities if e['id'] in merge_groups[group_id]]\n",
    "                    merged_entity = self.create_merged_entity(group_entities, group_id)\n",
    "                    merged_entities.append(merged_entity)\n",
    "                    processed_groups.add(group_id)\n",
    "            else:\n",
    "                merged_entities.append(entity)\n",
    "        updated_relations = []\n",
    "        for relation in relations:\n",
    "            new_relation = relation.copy()\n",
    "            if relation['source'] in id_mapping:\n",
    "                new_relation['source'] = id_mapping[relation['source']]\n",
    "            if relation['target'] in id_mapping:\n",
    "                new_relation['target'] = id_mapping[relation['target']]\n",
    "            if new_relation['source'] != new_relation['target']:\n",
    "                updated_relations.append(new_relation)\n",
    "        unique_relations = []\n",
    "        seen_relations = set()\n",
    "        for relation in updated_relations:\n",
    "            relation_key = (relation['source'], relation['target'], relation['type'])\n",
    "            if relation_key not in seen_relations:\n",
    "                unique_relations.append(relation)\n",
    "                seen_relations.add(relation_key)\n",
    "        return merged_entities, unique_relations\n",
    "    \n",
    "    def create_merged_entity(self, entities: List[Dict], new_id: str) -> Dict:\n",
    "        \"\"\"Создание объединенной энтити\"\"\"\n",
    "        main_entity = max(entities, key=lambda e: len(e.get('description', '')))\n",
    "        names = list(set(e['name'] for e in entities))\n",
    "        descriptions = [e.get('description', '') for e in entities if e.get('description')]\n",
    "        types = list(set(e.get('type', '') for e in entities if e.get('type')))\n",
    "        source_chunks = list(set(chunk for e in entities for chunk in e.get('source_chunks', [])))\n",
    "        merged_entity = {\n",
    "            'id': new_id,\n",
    "            'name': main_entity['name'],\n",
    "            'alternative_names': names[1:] if len(names) > 1 else [],\n",
    "            'type': types[0] if types else main_entity.get('type', ''),\n",
    "            'description': '; '.join(filter(None, descriptions)),\n",
    "            'source_chunks': sorted(source_chunks)\n",
    "        }\n",
    "        return merged_entity\n",
    "    \n",
    "    async def update_synonims(self, entities : List[Dict]):\n",
    "        syn = await extract_synonyms_for_entities(entities=entities, graphembedder=self.graphembedder)\n",
    "        self.synonyms.update(syn)\n",
    "\n",
    "    async def deduplicate(self, data: Dict, semantic_threshold: float = 0.7) -> Dict:\n",
    "        \"\"\"Основная функция дедупликации\"\"\"\n",
    "        entities = data.get('entities', [])\n",
    "        relations = data.get('relations', [])\n",
    "        await self.update_synonims(entities)\n",
    "        all_matches = []\n",
    "        exact_matches = self.find_exact_matches(entities)\n",
    "        all_matches.extend(exact_matches)\n",
    "        lemma_matches = self.find_lemma_matches(entities)\n",
    "        all_matches.extend(lemma_matches)\n",
    "        synonym_matches = self.find_synonym_matches(entities)\n",
    "        all_matches.extend(synonym_matches)\n",
    "        if self.use_llm:\n",
    "            semantic_matches = self.find_semantic_matches(entities, semantic_threshold)\n",
    "            all_matches.extend(semantic_matches)\n",
    "        unique_matches = []\n",
    "        seen_pairs = set()\n",
    "        for match in all_matches:\n",
    "            pair = tuple(sorted([match.entity1_id, match.entity2_id]))\n",
    "            if pair not in seen_pairs:\n",
    "                unique_matches.append(match)\n",
    "                seen_pairs.add(pair)\n",
    "        merged_entities, updated_relations = self.merge_entities(entities, relations, unique_matches)\n",
    "        result = data.copy()\n",
    "        result['entities'] = merged_entities\n",
    "        result['relations'] = updated_relations\n",
    "        if 'stats' in result:\n",
    "            result['stats']['total_entities'] = len(merged_entities)\n",
    "            result['stats']['total_relations'] = len(updated_relations)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34182cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мало связей между энтити и выделенны не все энтити, некоторые сложные понятия не раскрыты\n",
    "class RDFReasoningEngine:\n",
    "    \"\"\"Движок логических рассуждений для графа знаний\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph: Dict):\n",
    "        self.entities = {}\n",
    "        self.relations = []\n",
    "        self.adjacency_graph = defaultdict(list)\n",
    "        self.reverse_graph = defaultdict(list)\n",
    "        self.type_hierarchy = defaultdict(set)\n",
    "        \n",
    "        self._load_knowledge_graph(knowledge_graph)\n",
    "        self._build_graphs()\n",
    "        self._infer_type_hierarchy()\n",
    "    \n",
    "    def _load_knowledge_graph(self, kg: Dict):\n",
    "        \"\"\"Загружает граф знаний из JSON\"\"\"\n",
    "\n",
    "        for entity_data in kg.get('entities', []):\n",
    "            entity = Entity(\n",
    "                id=entity_data['id'],\n",
    "                name=entity_data['name'],\n",
    "                type=entity_data['type'],\n",
    "                description=entity_data['description'],\n",
    "                source_chunks=entity_data.get('source_chunks', [])\n",
    "            )\n",
    "            self.entities[entity.name] = entity\n",
    "        \n",
    "        for relation_data in kg.get('entities', []):\n",
    "            if 'source' in relation_data and 'target' in relation_data:\n",
    "                relation = Relation(\n",
    "                    source=relation_data['source'],\n",
    "                    target=relation_data['target'],\n",
    "                    type=relation_data['type'],\n",
    "                    description=relation_data['description'],\n",
    "                    source_chunks=relation_data.get('source_chunks', [])\n",
    "                )\n",
    "                self.relations.append(relation)\n",
    "    \n",
    "    def _build_graphs(self):\n",
    "        \"\"\"Строит графы смежности для быстрого поиска\"\"\"\n",
    "        for relation in self.relations:\n",
    "            self.adjacency_graph[relation.source].append(relation)\n",
    "            self.reverse_graph[relation.target].append(relation)\n",
    "    \n",
    "    def _infer_type_hierarchy(self):\n",
    "        \"\"\"Выводит иерархию типов на основе связей\"\"\"\n",
    "        for entity_name, entity in self.entities.items():\n",
    "            self.type_hierarchy[entity.type].add(entity_name)\n",
    "    \n",
    "    def find_path(self, start: str, end: str, max_depth: int = 5) -> List[List[Relation]]:\n",
    "        \"\"\"Находит все пути между двумя сущностями\"\"\"\n",
    "        paths = []\n",
    "        queue = deque([(start, [])])\n",
    "        visited = set()\n",
    "        \n",
    "        while queue:\n",
    "            current, path = queue.popleft() \n",
    "            if len(path) > max_depth:\n",
    "                continue\n",
    "            if current == end and path:\n",
    "                paths.append(path)\n",
    "                continue\n",
    "            state = (current, tuple(r.type for r in path))\n",
    "            if state in visited:\n",
    "                continue\n",
    "            visited.add(state)\n",
    "            for relation in self.adjacency_graph[current]:\n",
    "                if relation.target not in [r.target for r in path]: \n",
    "                    queue.append((relation.target, path + [relation]))\n",
    "        return paths\n",
    "    \n",
    "    def logical_inference(self, premises: List[Tuple[str, str, str]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Логический вывод на основе предпосылок\n",
    "        premises: список троек (субъект, предикат, объект)\n",
    "        \"\"\"\n",
    "        inferences = []\n",
    "        \n",
    "        for subject, predicate, obj in premises:\n",
    "            if predicate in ['used-for', 'produces', 'enables', 'controls']:\n",
    "                transitive_inferences = self._check_transitivity(subject, predicate, obj)\n",
    "                inferences.extend(transitive_inferences)\n",
    "            if predicate in ['similar-to', 'co-occurs-with']:\n",
    "                symmetric_inference = self._check_symmetry(subject, predicate, obj)\n",
    "                if symmetric_inference:\n",
    "                    inferences.append(symmetric_inference)\n",
    "            type_inferences = self._check_type_hierarchy(subject, obj)\n",
    "            inferences.extend(type_inferences)\n",
    "        \n",
    "        return inferences\n",
    "    \n",
    "    def _check_transitivity(self, subject: str, predicate: str, obj: str) -> List[Dict]:\n",
    "        \"\"\"Проверяет транзитивные отношения A->B, B->C => A->C\"\"\"\n",
    "        inferences = []\n",
    "        for relation in self.adjacency_graph[obj]:\n",
    "            if relation.type == predicate:\n",
    "                inference = {\n",
    "                    'type': 'transitive',\n",
    "                    'rule': f'({subject} {predicate} {obj}) ∧ ({obj} {predicate} {relation.target}) → ({subject} {predicate} {relation.target})',\n",
    "                    'conclusion': (subject, predicate, relation.target),\n",
    "                    'confidence': 0.8,\n",
    "                    'evidence': [obj]\n",
    "                }\n",
    "                inferences.append(inference)\n",
    "        \n",
    "        return inferences\n",
    "    \n",
    "    def _check_symmetry(self, subject: str, predicate: str, obj: str) -> Optional[Dict]:\n",
    "        \"\"\"Проверяет симметричные отношения A~B => B~A\"\"\"\n",
    "        for relation in self.adjacency_graph[obj]:\n",
    "            if relation.target == subject and relation.type == predicate:\n",
    "                return None  \n",
    "        return {\n",
    "            'type': 'symmetric',\n",
    "            'rule': f'({subject} {predicate} {obj}) → ({obj} {predicate} {subject})',\n",
    "            'conclusion': (obj, predicate, subject),\n",
    "            'confidence': 0.9,\n",
    "            'evidence': [subject]\n",
    "        }\n",
    "    \n",
    "    def _check_type_hierarchy(self, subject: str, obj: str) -> List[Dict]:\n",
    "        \"\"\"Проверяет отношения на основе иерархии типов\"\"\"\n",
    "        inferences = []\n",
    "        if subject in self.entities and obj in self.entities:\n",
    "            subj_type = self.entities[subject].type\n",
    "            obj_type = self.entities[obj].type\n",
    "            if subj_type == obj_type:\n",
    "                inference = {\n",
    "                    'type': 'type_similarity',\n",
    "                    'rule': f'type({subject}) = type({obj}) → similar_type({subject}, {obj})',\n",
    "                    'conclusion': (subject, 'similar_type', obj),\n",
    "                    'confidence': 0.7,\n",
    "                    'evidence': [subj_type]\n",
    "                }\n",
    "                inferences.append(inference)\n",
    "        return inferences\n",
    "    \n",
    "    def reasoning_chain(self, query: str, context: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Цепочка рассуждений для ответа на вопрос\n",
    "        \"\"\"\n",
    "        key_terms = self._extract_key_terms(query)\n",
    "        relevant_entities = []\n",
    "        for term in key_terms:\n",
    "            matches = self._find_similar_entities(term)\n",
    "            relevant_entities.extend(matches)\n",
    "        reasoning_steps = []\n",
    "        if len(relevant_entities) >= 2:\n",
    "            for i in range(len(relevant_entities)):\n",
    "                for j in range(i+1, len(relevant_entities)):\n",
    "                    paths = self.find_path(relevant_entities[i].name, relevant_entities[j].name)\n",
    "                    if paths:\n",
    "                        step = {\n",
    "                            'type': 'connection',\n",
    "                            'entities': [relevant_entities[i].name, relevant_entities[j].name],\n",
    "                            'paths': [self._path_to_string(path) for path in paths[:3]],\n",
    "                            'reasoning': f\"Найдены связи между {relevant_entities[i].name} и {relevant_entities[j].name}\"\n",
    "                        }\n",
    "                        reasoning_steps.append(step)\n",
    "        premises = []\n",
    "        for relation in self.relations[:10]: \n",
    "            premises.append((relation.source, relation.type, relation.target))\n",
    "        inferences = self.logical_inference(premises)  \n",
    "        return {\n",
    "            'query': query,\n",
    "            'key_terms': key_terms,\n",
    "            'relevant_entities': [e.name for e in relevant_entities],\n",
    "            'reasoning_steps': reasoning_steps,\n",
    "            'inferences': inferences[:5],  \n",
    "            'confidence': self._calculate_overall_confidence(reasoning_steps, inferences)\n",
    "        }\n",
    "    \n",
    "    def _extract_key_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Извлекает ключевые термины из текста\"\"\"\n",
    "        words = re.findall(r'\\b[а-яё]+\\b', text.lower())\n",
    "        return [word for word in words if len(word) > 3]\n",
    "    \n",
    "    def _find_similar_entities(self, term: str) -> List[Entity]:\n",
    "        \"\"\"Находит похожие сущности по названию\"\"\"\n",
    "        matches = []\n",
    "        for entity in self.entities.values():\n",
    "            if term.lower() in entity.name.lower() or term.lower() in entity.description.lower():\n",
    "                matches.append(entity)\n",
    "        return matches\n",
    "    \n",
    "    def _path_to_string(self, path: List[Relation]) -> str:\n",
    "        \"\"\"Преобразует путь в читаемую строку\"\"\"\n",
    "        if not path:\n",
    "            return \"\"\n",
    "        result = path[0].source\n",
    "        for relation in path:\n",
    "            result += f\" --[{relation.type}]--> {relation.target}\"\n",
    "        return result\n",
    "    \n",
    "    def _calculate_overall_confidence(self, steps: List[Dict], inferences: List[Dict]) -> float:\n",
    "        \"\"\"Вычисляет общую уверенность в рассуждении\"\"\"\n",
    "        if not steps and not inferences:\n",
    "            return 0.0\n",
    "        total_confidence = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        for inference in inferences:\n",
    "            total_confidence += inference.get('confidence', 0.5)\n",
    "            count += 1\n",
    "        total_confidence += len(steps) * 0.3\n",
    "        count += len(steps)\n",
    "        return min(total_confidence / max(count, 1), 1.0)\n",
    "    \n",
    "    def validate_knowledge_consistency(self) -> Dict:\n",
    "        \"\"\"Проверяет консистентность базы знаний\"\"\"\n",
    "        issues = []\n",
    "        cycles = self._detect_cycles()\n",
    "        if cycles:\n",
    "            issues.append({\n",
    "                'type': 'circular_dependency',\n",
    "                'description': 'Обнаружены циклические зависимости',\n",
    "                'details': cycles\n",
    "            })\n",
    "        contradictions = self._detect_contradictions()\n",
    "        if contradictions:\n",
    "            issues.append({\n",
    "                'type': 'contradiction',\n",
    "                'description': 'Обнаружены противоречия',\n",
    "                'details': contradictions\n",
    "            })\n",
    "        return {\n",
    "            'is_consistent': len(issues) == 0,\n",
    "            'issues': issues,\n",
    "            'statistics': {\n",
    "                'entities_count': len(self.entities),\n",
    "                'relations_count': len(self.relations),\n",
    "                'types_count': len(self.type_hierarchy)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _detect_cycles(self) -> List[List[str]]:\n",
    "        \"\"\"Обнаруживает циклы в графе\"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        cycles = []\n",
    "        def dfs(node, path):\n",
    "            if node in rec_stack:\n",
    "                cycle_start = path.index(node)\n",
    "                cycles.append(path[cycle_start:] + [node])\n",
    "                return\n",
    "            if node in visited:\n",
    "                return\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            for relation in self.adjacency_graph[node]:\n",
    "                dfs(relation.target, path + [node])\n",
    "            rec_stack.remove(node)\n",
    "        for entity_name in self.entities.keys():\n",
    "            if entity_name not in visited:\n",
    "                dfs(entity_name, [])\n",
    "        return cycles\n",
    "    \n",
    "    def _detect_contradictions(self) -> List[Dict]:\n",
    "        \"\"\"Обнаруживает логические противоречия\"\"\"\n",
    "        contradictions = []\n",
    "        for relation1 in self.relations:\n",
    "            for relation2 in self.relations:\n",
    "                if (relation1.source == relation2.target and \n",
    "                    relation1.target == relation2.source and\n",
    "                    self._are_contradictory(relation1.type, relation2.type)):\n",
    "                    contradictions.append({\n",
    "                        'type': 'bidirectional_contradiction',\n",
    "                        'relation1': f\"{relation1.source} {relation1.type} {relation1.target}\",\n",
    "                        'relation2': f\"{relation2.source} {relation2.type} {relation2.target}\"\n",
    "                    })\n",
    "        return contradictions\n",
    "    \n",
    "    def _are_contradictory(self, type1: str, type2: str) -> bool:\n",
    "        \"\"\"Проверяет, являются ли два типа отношений противоречивыми\"\"\"\n",
    "        contradictory_pairs = {\n",
    "            ('enables', 'prevents'),\n",
    "            ('produces', 'destroys'),\n",
    "            ('supports', 'opposes')\n",
    "        }\n",
    "        return (type1, type2) in contradictory_pairs or (type2, type1) in contradictory_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f3a2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_knowledge_graph_to_gephi(json_data: Dict[str, Any], \n",
    "                                     nodes_output_path: str = \"C:/Users/user/Desktop/nodes.csv\",\n",
    "                                     edges_output_path: str = \"C:/Users/user/Desktop/edges.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Трансформирует данные графа знаний из JSON в CSV формат для Gephi.\n",
    "    \n",
    "    Args:\n",
    "        json_data: Словарь с данными графа знаний\n",
    "        nodes_output_path: Путь для сохранения файла узлов\n",
    "        edges_output_path: Путь для сохранения файла рёбер\n",
    "    \"\"\"\n",
    "\n",
    "    nodes_data = []\n",
    "    \n",
    "    for entity in json_data.get(\"entities\", []):\n",
    "        node = {\n",
    "            \"Id\": entity[\"id\"],\n",
    "            \"Label\": entity[\"name\"],\n",
    "            \"Type\": entity.get(\"type\", \"unknown\"),\n",
    "            \"Description\": entity.get(\"description\", \"\").replace(\"\\n\", \" \").replace(\";\", \",\"),\n",
    "            \"Alternative_Names\": \"|\".join(entity.get(\"alternative_names\", [])),\n",
    "            \"Source_Chunks\": \"|\".join(map(str, entity.get(\"source_chunks\", [])))\n",
    "        }\n",
    "        nodes_data.append(node)\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes_data)\n",
    "    nodes_df.to_csv(nodes_output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    edges_data = []\n",
    "    edge_id = 0\n",
    "    \n",
    "    for relation in json_data.get(\"relations\", []):\n",
    "        edge = {\n",
    "            \"Id\": edge_id,\n",
    "            \"Source\": relation[\"source\"],\n",
    "            \"Target\": relation[\"target\"],\n",
    "            \"Type\": relation.get(\"type\", \"unknown\"),\n",
    "            \"Label\": relation.get(\"type\", \"unknown\"),\n",
    "            \"Description\": relation.get(\"description\", \"\").replace(\"\\n\", \" \").replace(\";\", \",\"),\n",
    "            \"Source_Chunks\": \"|\".join(map(str, relation.get(\"source_chunks\", [])))\n",
    "        }\n",
    "        edges_data.append(edge)\n",
    "        edge_id += 1\n",
    "\n",
    "    edges_df = pd.DataFrame(edges_data)\n",
    "    edges_df.to_csv(edges_output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Создано узлов: {len(nodes_data)}\")\n",
    "    print(f\"Создано рёбер: {len(edges_data)}\")\n",
    "    print(f\"Файлы сохранены: {nodes_output_path}, {edges_output_path}\")\n",
    "\n",
    "\n",
    "def load_and_transform(json_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Загружает JSON файл и трансформирует его в CSV для Gephi.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Путь к JSON файлу с данными графа знаний\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        transform_knowledge_graph_to_gephi(json_data)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл {json_file_path} не найден\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Ошибка при парсинге JSON файла {json_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "\n",
    "def transform_from_string(json_string: str) -> None:\n",
    "    \"\"\"\n",
    "    Трансформирует JSON строку в CSV для Gephi.\n",
    "    \n",
    "    Args:\n",
    "        json_string: JSON строка с данными графа знаний\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "        transform_knowledge_graph_to_gephi(json_data)\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Ошибка при парсинге JSON строки\")\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab21bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_knowledge_graph(text_path, graph_path):\n",
    "    gemini_api_keys=[\n",
    "\n",
    "                    ]\n",
    "    generation_model_path = \"C:/Users/user/AppData/Local/nomic.ai/GPT4All/qwen2.5-coder-7b-instruct-q4_0.gguf\" \n",
    "    processor = GeminiEnhancedTextProcessor(gemini_api_keys)\n",
    "    graphembedder = ImprovedKnowledgeExtractor(gemini_api_keys, generation_model_path)\n",
    "    deduplicator = EntityDeduplicator(graphembedder)\n",
    "    with open(text_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    result = await processor.process_text_enhanced(text)\n",
    "    graph_data = await graphembedder.extract_knowledge_graph(result['processed_text'])\n",
    "    unique_graph_data = await deduplicator.deduplicate(graph_data, semantic_threshold=0.7)\n",
    "    with open(graph_path, \"w\", encoding='utf-8') as fl:\n",
    "        json.dump(unique_graph_data, fl, ensure_ascii=False, indent=4)\n",
    "    return unique_graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b2901af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_system(query: str, graph_path = 'C:/Users/user/Desktop/fullgraphdata.json'):\n",
    "    \n",
    "    with open(graph_path, 'r', encoding='utf-8') as f:\n",
    "        graph = json.load(f)\n",
    "\n",
    "    reasoning_engine = RDFReasoningEngine(graph)\n",
    "\n",
    "    result = reasoning_engine.reasoning_chain(query)\n",
    "    \n",
    "    print(f\"Запрос: {result['query']}\")\n",
    "    print(f\"Ключевые термины: {result['key_terms']}\")\n",
    "    print(f\"Релевантные сущности: {result['relevant_entities']}\")\n",
    "    print(f\"Уверенность: {result['confidence']:.2f}\")\n",
    "    \n",
    "    if result['reasoning_steps']:\n",
    "        print(\"\\nШаги рассуждения:\")\n",
    "        for i, step in enumerate(result['reasoning_steps'], 1):\n",
    "            print(f\"{i}. {step['reasoning']}\")\n",
    "    \n",
    "    if result['inferences']:\n",
    "        print(\"\\nЛогические выводы:\")\n",
    "        for i, inference in enumerate(result['inferences'], 1):\n",
    "            print(f\"{i}. {inference['rule']} (уверенность: {inference['confidence']})\")\n",
    "    \n",
    "    consistency = reasoning_engine.validate_knowledge_consistency()\n",
    "    print(f\"\\nКонсистентность базы знаний: {'✓' if consistency['is_consistent'] else '✗'}\")\n",
    "    print(f\"Статистика: {consistency['statistics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    unique_graph_data = await create_knowledge_graph('C:/Users/user/Desktop/rawmaterial.txt', 'C:/Users/user/Desktop/fullgraphdata.json')\n",
    "    await asyncio.sleep(1)\n",
    "    print(unique_graph_data)\n",
    "    transform_knowledge_graph_to_gephi(unique_graph_data)\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
