Id,Source,Target,Type,Label,Description,Source_Chunks
0,Нейронная сеть,Аппроксимирующая функция,used-for,used-for,"Нейронная сеть используется для аппроксимации неизвестной сложной функции, связывающей входные и выходные данные.",0
1,Weights,Слой нейронной сети,part-of,part-of,"Weights (веса) связей используются при распространении значений по слоям нейронной сети, умножаясь на значения с нейронов предыдущего слоя., Показаны начальные weights слоев сети.",0|21
2,Biases,Слой нейронной сети,part-of,part-of,Biases (смещения) добавляются к сумме значений в нейроне слоя нейронной сети после взвешенного суммирования значений с предыдущего слоя.,0
3,Предсказание,Нейронная сеть,produces,produces,Нейронная сеть производит предсказание соответствующих выходных значений для новых входных данных.,0
4,Функция активации,Нейрон,part-of,part-of,Функция активации является частью нейрона и преобразует сумму в нейроне.,1
5,Обучение нейронной сети,Weights (Веса),requires,requires,Обучение нейронной сети требует поиска оптимальных значений весов.,1
6,Обучение нейронной сети,Biases (Смещения),requires,requires,Обучение нейронной сети требует поиска оптимальных значений смещений.,1
7,Функция ошибки,Обучающие примеры (training examples),used-for,used-for,Функция ошибки используется для минимизации расхождений между предсказанными и реальными значениями на обучающих примерах.,1
8,Аппроксимирующая функция,Истинная базовая функция,connected-to,connected-to,Аппроксимирующая функция демонстрирует повышенную конвергенцию к истинной базовой функции в процессе обучения нейронной сети.,2
9,Функция ошибки,Пространство весов,part-of,part-of,Функция ошибки показана в пространстве весов (для двух параметров weights и biases).,2
10,Обучение нейронной сети,Сжатие данных,enables,enables,"В процессе обучения нейронная сеть учится сжимать данные в информативные признаки, кодируя закономерности и игнорируя шум.",2
11,Матрица весов,Слой нейронной сети,part-of,part-of,Матрица весов является частью слоя нейронной сети.,3
12,Линейная разделимость,Гиперповерхность,creates,creates,Гиперплоскость создает линейное разделение.,3
13,Переобучение,Обобщение,prevents,prevents,Переобучение (overfitting) препятствует обобщению (способности работать с новыми данными).,3
14,Аппроксимация,Теорема универсальной аппроксимации,enables,enables,Теорема универсальной аппроксимации позволяет аппроксимировать практически любую непрерывную функцию.,4
15,Неопределённость,Информация,connected-to,connected-to,"Наблюдение приносит информацию, уменьшая неопределенность. Информация является мерой уменьшения неопределенности.",4
16,Энтропия,Неопределённость,represents,represents,Энтропия - мера неопределенности.,4
17,равные вероятности,Энтропия,causes,causes,Энтропия максимальна при равных вероятностях.,5
18,Условная entropy,Неопределённость,equals,equals,Условная entropy - это неопределённость y при известном x.,5
19,Опыт,Внутренняя модель мира,creates,creates,"Опыт формирует внутреннюю модель мира, которая является вероятностной моделью.",6
20,Энтропия системы h(x),Оптимальная стратегия задавания вопросов,controls,controls,Оптимальная стратегия задавания вопросов направлена на минимизацию энтропии системы h(x).,6
21,Бинарный поиск,Оптимальная стратегия задавания вопросов,enables,enables,Оптимальная стратегия задавания вопросов использует бинарный поиск для быстрого сужения круга поиска.,6
22,Метод информационного бутылочного горлышка,I(X;Y),used-for,used-for,"Метод информационного бутылочного горлышка используется для извлечения полезной информации I(X,Y).",7
23,Метод информационного бутылочного горлышка,Mean Squared Error (MSE),connected-to,connected-to,"Метод информационного бутылочного горлышка анализирует внутренние процессы сети, а не только итоговые потери, такие как Mean Squared Error (MSE).",7
24,Глубокая нейронная сеть прямого действия,Скрытые слои,part-of,part-of,Глубокая нейронная сеть прямого действия состоит из M скрытых слоев.,7
25,Сжатие данных,Обобщение,causes,causes,"Способность сети сжимать данные, выделяя значимые паттерны, влияет на обобщение.",8
26,Кластеры,Квантование данных,part-of,part-of,"Квантование данных включает в себя разбиение входных данных на группы, называемые кластерами.",8
27,Информация и энтропия,Взаимный информационный выигрыш,connected-to,connected-to,Взаимный информационный выигрыш связан с понятиями информации и энтропии.,8
28,Сжатие данных,Сверточные нейронные сети (Convolutional Neural Network),used-for,used-for,Сверточные нейронные сети используются для эффективного сжатия данных в глубоких нейронных сетях.,9
29,Сжатие данных,Внимание,used-for,used-for,Внимание используется для эффективного сжатия данных в глубоких нейронных сетях.,9
30,Информативные паттерны,Обобщение,enables,enables,"Нейронные сети способны к обобщению, если они могут выделить информативные паттерны.",9
31,Dropout,Квантование данных,causes,causes,"Методы регуляризации, такие как Dropout, косвенно помогают ""квантовать"" данные.",9
32,Batchnorm (Batch Normalization),Квантование данных,causes,causes,"Методы регуляризации, такие как Batchnorm, косвенно помогают ""квантовать"" данные.",9
33,Анализ скрытых представлений (встраивания),Интерпретация моделей,used-for,used-for,"Анализ скрытых представлений используется для интерпретации моделей, показывая, как сеть группирует данные.",9
34,Deep Neural Networks (Глубокие нейронные сети),Сжатие данных,enables,enables,Эффективное сжатие данных является причиной успеха глубоких нейронных сетей.,9
35,Кластер,Нейросеть,connected-to,connected-to,"Кластер - это группа похожих данных, которые нейросеть объединяет по каким-то признакам.",10
36,Прототипы категорий,Кластер,part-of,part-of,Прототипы категорий являются центрами кластеров во входном пространстве.,10
37,Граничные условия категорий,Веса и биасы (смещения),defines,defines,Граничные условия категорий определяются весами и биасами (смещениями).,10
38,Энтропия,Неопределённость,similar-to,similar-to,"Энтропия - это мера ""разнообразия"" данных, что связано с понятием неопределенности.",10
39,Энтропия системы h(x),Неопределённость,represents,represents,Энтропия системы h(x) представляет собой меру неопределённости или разнообразия в данных.,11
40,Условная entropy,Кластер,connected-to,connected-to,"Условная энтропия связана с разнообразием данных внутри кластеров, полученных в результате кластеризации данных нейросетью.",11
41,Взаимный информационный выигрыш,Кластеры,connected-to,connected-to,"Взаимный информационный выигрыш показывает, насколько кластеры помогают понять данные.",12
42,Кластеры,Класс,equals,equals,Каждый кластер представляет собой один класс.,13
43,Сверточные нейронные сети (Convolutional Neural Network),Признаки,produces,produces,Первые слои сверточных нейронных сетей находят простые признаки.,13
44,Сжатие данных,Обобщение,enables,enables,"Чем лучше сеть сжимает данные, тем лучше она обобщает на новые данные.",13
45,Долговременная память сети,Матрицы весов,contains,contains,Долговременная память сети содержит матрицы весов.,14
46,Долговременная память сети,Вектора bias,contains,contains,Долговременная память сети содержит вектора bias.,14
47,Информационная плоскость,Information Bottleneck (IB),connected-to,connected-to,Информационная плоскость связана с теоретическими и практическими пределами IB (Information Bottleneck).,14
48,Идеальный теоретический предел сжатия,Сжатие,part-of,part-of,Идеальный теоретический предел сжатия относится к процессу сжатия.,14
49,Deep Neural Networks (Глубокие нейронные сети),Сжатие,causes,causes,Слои Deep Neural Networks сжимают шум в данных.,15
50,Deep Neural Networks (Глубокие нейронные сети),Информация,connected-to,connected-to,Слои Deep Neural Networks движутся к оптимальному компромиссу между сжатием и сохранением полезной информации.,15
51,Overfitting,Информация,causes,causes,Слишком сильное сохранение информации может привести к переобучению.,15
52,y,Выходная метка y,same-as,same-as,y является сокращением для Выходная метка y.,16
53,Deep Neural Networks (Глубокие нейронные сети),Скрытые слои,part-of,part-of,Скрытые слои являются частью Deep Neural Networks.,16
54,Линейная разделимость,Скрытые слои,requires,requires,Нейроны в Deep Neural Networks требуют трансформаций на скрытых слоях для линейной разделимости данных.,16
55,Сжатие,Абстрагирование,enables,enables,"Скрытые слои Deep Neural Networks сжимают входные данные о y, улучшая абстрагирование.",16
56,"Стохастический градиентный спуск (Stochastic Gradient Descent, SGD)",Deep Neural Networks (Глубокие нейронные сети),used-for,used-for,Стохастический градиентный спуск используется для обучения глубоких нейронных сетей.,17
57,Энтропия системы h(x),Entropy входа,equals,equals,Энтропия системы h(x) равна Entropy входа.,17
58,h(x),Энтропия,represents,represents,h(x) представляет энтропию входа.,17
59,Фаза дрейфа (Drift Phase),Инициализация весов (weights initialization),after,after,Фаза дрейфа происходит после случайной инициализации весов.,17
60,последний скрытый слой,Сжатие данных,causes,causes,Последний скрытый слой нейронной сети приводит к сильному сжатию данных.,18
61,двухфазная структура обучения,фаза подгонки (fitting),part-of,part-of,Двухфазная структура обучения включает в себя фазу подгонки (fitting).,18
62,двухфазная структура обучения,фаза сжатия/забывания (сжатие/forgetting),part-of,part-of,Двухфазная структура обучения включает в себя фазу сжатия/забывания (сжатие/forgetting).,18
63,Фаза подгонки,Функция потерь,enables,enables,Фаза подгонки позволяет сети быстро минимизировать функцию потерь.,19
64,Мини-батчи,Сжатие,connected-to,connected-to,Флуктуации в мини-батчах критически важны для процесса сжатия.,20
65,Размер мини-батча,Сжатие,causes,causes,Размер мини-батча линейно связан с точкой начала компрессии.,20
66,Обобщение,фаза диффузии,enables,enables,Улучшение способности к обобщению происходит во второй фазе (диффузии/забывания).,21
67,фаза диффузии,Забывание,same-as,same-as,фаза диффузии также называется фазой забывания.,21
68,Stochastic Gradient Descent (SGD),Оптимизация,used-for,used-for,Stochastic Gradient Descent (SGD) используется во время оптимизации.,21
69,Информация о метке (label information),Обобщение,causes,causes,Прирост информации о метке приводит к улучшению способности к обобщению.,21
70,фаза диффузии,Сжатие,produces,produces,"В фазе диффузии отбрасывается нерелевантная информация, что приводит к компрессии представления.",21
71,Weights,Инициализация весов (weights initialization),part-of,part-of,Инициализация весов подразумевает задание начальных значений для весов (weights) нейронной сети.,22
72,Обучающие примеры (training examples),Сходимость (convergence),causes,causes,"Меньшее количество обучающих примеров приводит к сходимости, далекой от сходимости при большем количестве примеров.",22
73,Информация о метке (label information),Сжатие,affected_by,affected_by,Информация о метке уменьшается в случае меньшего количества обучающих примеров на этапе сжатия.,22
74,"Стохастический градиентный спуск (Stochastic Gradient Descent, SGD)",Мини-батчи,part-of,part-of,Стохастический градиентный спуск использует мини-батчи для обновления параметров.,23
75,Оптимизация,Скрытые слои,connected-to,connected-to,Оптимизация влияет на среднюю траекторию движения скрытых слоев.,23
76,Обобщение,Оптимизация,after,after,Обобщение происходит после этапа подгонки к данным в процессе оптимизации.,23
77,Линейная разделимость,Обучение нейронной сети,enables,enables,"По мере углубления в сеть данные становятся линейно разделимыми, что упрощает обучение.",24
78,Ковариационная матрица шума,Забывание,enables,enables,Ковариационная матрица шума обеспечивает направленное забывание нерелевантных признаков.,24
79,Фаза подгонки,Обобщение,prevents,prevents,"Точность достигается в фазе подгонки, но способность к обобщению формируется в фазе забывания, то есть фаза подгонки препятствует обобщению.",25
80,Нейронные сети,Иерархия признаков,produces,produces,Нейронные сети автоматически выделяют иерархию признаков.,25
81,Сжатие,Взаимопомощь представлений,enables,enables,"Слои помогают друг другу в компрессии, создавая каскадный эффект очистки от нерелевантной информации. Взаимопомощь представлений способствует сжатию.",25
82,Нейрон,Признаки,connected-to,connected-to,"Нейроны не всегда соответствуют конкретным признакам, особенно в больших сетях.",26
83,Распределённое представление,Нейрон,part-of,part-of,"Распределённое представление часто встречается в нейронных сетях, где нейроны не соответствуют конкретным признакам.",26
84,Нейронные сети,Weights,contains,contains,В нейронных сетях weights являются основным способом хранения информации.,26
85,Нейронные сети,Biases,contains,contains,В нейронных сетях biases являются основным способом хранения информации.,26
86,Weights,Нейрон,connected-to,connected-to,Weights - это числовые значения связей между нейронами.,26
87,Biases,Активация нейронных путей,enables,enables,"Смещения (biases) позволяют нейронам активироваться, даже если входных данных недостаточно.",26
88,Biases,Взвешенное суммирование,part-of,part-of,"Смещения (bias) это константы, добавляемые к взвешенной сумме.",26
89,Weights,Weights (Веса),same-as,same-as,Weights и Weights (Веса) - это одно и то же.,27
90,Biases,biases (биасы),same-as,same-as,Biases и biases (биасы) - это одно и то же.,27
91,Attention (внимание),Transformer,part-of,part-of,Attention (внимание) является важной частью архитектуры Transformer.,27
92,Residual connections,Transformer,part-of,part-of,Residual connections являются важной частью архитектуры Transformer.,27
93,Weights (Веса),Нейронная сеть,part-of,part-of,Weights (Веса) являются частью нейронной сети и определяют силу связей.,27
94,Biases (Смещения),Нейронная сеть,part-of,part-of,Biases (Смещения) являются частью нейронной сети.,27
95,Функция активации,паттерны,causes,causes,"Функция активации влияет на то, какие паттерны сеть может выучить.",27
96,архитектура нейронной сети,паттерны,causes,causes,"Архитектура нейронной сети влияет на то, какие паттерны сеть может выучить.",27
97,Искусственная нейронная сеть,Веса и смещения,contains,contains,Знания в искусственной нейронной сети распределены по всему набору весов и смещений.,28
98,Ансамбли нейронов,Распределённое представление,equals,equals,"Признаки кодируются ансамблями нейронов, что является распределённым представлением.",28
99,Забывание,Пространство весов,causes,causes,Забывание описывается как переформирование ландшафта весов и смещений в пространстве весов.,29
100,Воспоминание,Нейрон,requires,requires,Воспоминание происходит через активацию определенной конфигурации нейронов.,29
101,Нейронные сети,Обобщение,enables,enables,Перекрывающиеся паттерны весов в нейронных сетях обеспечивают обобщение.,29
102,Рекуррентные энграммы (рекуррентные нейронные сети),Рекуррентные нейронные сети,same-as,same-as,"Рекуррентные энграммы - это рекуррентные нейронные сети, предназначенные для обработки последовательных данных.",29
103,Рекуррентные нейронные сети,Скрытое состояние (h_t),contains,contains,Рекуррентные нейронные сети сохраняют контекст через скрытое состояние (h_t).,29
104,Рекуррентные нейронные сети,Обратное распространение ошибки во времени,used-for,used-for,Обратное распространение ошибки во времени используется для обучения рекуррентных нейронных сетей.,30
105,Рекуррентные нейронные сети,Разворачивание сети,part-of,part-of,Разворачивание сети является этапом обучения рекуррентных нейронных сетей.,30
106,Рекуррентные нейронные сети,Вычисление градиентов,part-of,part-of,Вычисление градиентов является этапом обучения рекуррентных нейронных сетей.,30
107,Рекуррентные нейронные сети,Обновление весов,part-of,part-of,Обновление весов является этапом обучения рекуррентных нейронных сетей.,30
108,Рекуррентные нейронные сети,Высокие вычислительные затраты,causes,causes,Обучение рекуррентных нейронных сетей приводит к высоким вычислительным затратам.,30
109,Взрывные градиенты,Рекуррентные нейронные сети (RNN),connected-to,connected-to,"Взрывные градиенты являются одной из проблем, возникающих при обучении рекуррентных нейронных сетей.",31
110,Затухающие градиенты,Рекуррентные нейронные сети (RNN),connected-to,connected-to,"Затухающие градиенты являются одной из проблем, возникающих при обучении рекуррентных нейронных сетей.",31
111,Рекуррентные нейронные сети (RNN),Забывание информации на длинных последовательностях,causes,causes,Рекуррентные нейронные сети подвержены проблеме забывания информации на длинных последовательностях.,31
112,Внимание,Нейронные сети,part-of,part-of,Механизм внимания используется в нейронных сетях.,31
113,Взвешенное суммирование,Контекстный вектор,produces,produces,Взвешенное суммирование данных с учетом весов создает контекстный вектор.,32
114,Внимание,Веса внимания,produces,produces,Механизм внимания вычисляет веса внимания для каждого элемента входных данных.,32
115,Веса внимания,Взвешенное суммирование,used-for,used-for,Веса внимания используются при взвешенном суммировании данных.,32
116,Селективность внимания,Внимание,part-of,part-of,"Селективность является одной из особенностей внимания в мозге, позволяющей выбирать, на чем сосредоточиться.",33
117,Гибкость внимания,Внимание,part-of,part-of,"Гибкость является одной из особенностей внимания в мозге, позволяющей переключаться или распределяться между задачами.",33
118,Префронтальная кора,Внимание,connected-to,connected-to,"Префронтальная кора является частью сложной сети мозга, которая регулирует внимание.",33
119,Теменная кора,Внимание,connected-to,connected-to,"Теменная кора является частью сложной сети мозга, которая регулирует внимание.",33
120,Внимание,Мозг,similar-to,similar-to,"Механизм внимания в нейронных сетях вдохновлен работой мозга, и у них есть общие черты.",34
121,Weights внимания,Внимание,part-of,part-of,Weights внимания являются частью механизма внимания в нейронных сетях.,34
122,Активация нейронных путей,Мозг,part-of,part-of,"Активация нейронных путей является частью работы мозга, аналогичной weights внимания в нейронных сетях.",34
123,Переключение внимания,Мозг,part-of,part-of,"Мозг переключается между словами при чтении, что является примером динамической адаптации внимания к контексту.",34
124,Внимание,Скалярное произведение,used-for,used-for,В нейронных сетях внимание использует скалярное произведение для вычисления весов.,35
125,Нейронные сети,Оптимизация,enables,enables,В нейронных сетях внимание настраивается через оптимизацию на данных.,35
126,Мозг,Внимание,similar-to,similar-to,"В мозге внимание - это сложное взаимодействие нейронов, памяти и восприятия, что является аналогом механизма внимания в нейронных сетях.",35
127,Мозг,Опыт,develops-through,develops-through,Развитие мозга происходит через опыт.,36
128,Мозг,Врожденные механизмы,depends-on,depends-on,Развитие мозга зависит от врожденных механизмов.,36
129,Предиктивное кодирование,Мозг,considers,considers,Гипотеза предиктивного кодирования рассматривает мозг как машину предсказаний.,36
130,Прямое распространение,Предсказание,similar-to,similar-to,Прямое распространение аналогично предсказанию.,36
131,Нейрон,Предсказание,produces,produces,"Нейроны каждого слоя формируют ""ожидания"" (предсказания).",36
132,Нейрон,Weights (Веса),connected-to,connected-to,Связи между нейронами называются весами.,37
133,Предсказание,Ошибка предсказания,causes,causes,"Ошибка предсказания возникает, когда фактический результат отличается от ожидаемого.",37
134,Ошибка предсказания,__MATH_FORMULA_2__,represents,represents,"Ошибка предсказания рассчитывается как разница между реальным значением (y) и предсказанным значением (ŷ), что представлено математической формулой номер 2.",37
135,y,__MATH_FORMULA_2__,part-of,part-of,y (реальное значение) является частью формулы для расчета ошибки предсказания (__MATH_FORMULA_2__).,37
136,ŷ,__MATH_FORMULA_2__,part-of,part-of,ŷ (предсказанное значение) является частью формулы для расчета ошибки предсказания (__MATH_FORMULA_2__).,37
137,Градиенты ошибки,Weights (Веса),used-for,used-for,"Градиенты ошибки вычисляются для каждого веса, чтобы понять, насколько он повлиял на итоговую ошибку.",38
138,Искусственная нейронная сеть,Градиенты ошибки,contains,contains,В искусственных нейронной сети вычисляются градиенты ошибки.,38
139,Предиктивное кодирование,Мозг,connected-to,connected-to,"Предиктивное кодирование рассматривается как механизм, который использует мозг для корректировки внутренней модели мира.",39
140,Сигналы ошибки,Внутренняя модель мира,causes,causes,"Сигналы ошибки, согласно гипотезе предиктивного кодирования, передаются назад, чтобы скорректировать внутреннюю модель мира.",39
141,Weights,Ошибка предсказания,causes,causes,Weights корректируются пропорционально их вкладу в ошибку предсказания.,39
142,Пластичность синапсов,Мозг,same-as,same-as,Пластичность синапсов в мозге сравнивается с процессом корректировки весов в нейронной сети.,39
143,Градиент функции потерь,Weights,connected-to,connected-to,"Градиент функции потерь относительно weights показывает, как вес влияет на ошибку.",39
